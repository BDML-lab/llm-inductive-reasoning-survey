  The origin repository is [[Link](https://github.com/141forever/inductive-reasoning-papers)].

# Inductive Reasoning

Inductive reasoning involves drawing general conclusions from specific observations.
The main characteristics of inductive reasoning are its particular-to-general thinking process and the non-uniqueness of its answers. 
Considering how humans perceive the world, they typically make judgments by drawing analogies from past experiences to current situations, rather than always going through a strictly logical process as in deductive reasoning. 
We can assume that the inductive mode is key to knowledge generalization and better aligns with human cognition.
We give two examples of inductive reasoning in the figure below.

<p align="center">
  <img src="https://github.com/141forever/inductive-reasoning-papers/blob/main/Figures/two_examples.jpg" width="50%">
</p>



# About the Survey

We have made every effort to collect all top-conference papers from 2015 to 2025 whose titles contain the words ‘inductive’ or ‘induction’.
These conferences include, but are not limited to, NeurIPS, ICML, ICLR, ACL, EMNLP, NAACL, COLING, and EACL. 
All collected papers are listed below, and their statistics are summarized in the following table. 
We have conducted a rough categorization of these papers, which may contain errors or overlaps, and we kindly ask for the readers’ understanding.
Our survey focuses on synthesizing and analyzing papers from this list that are related to inductive tasks and language models.
This is the link to our survey [[ARXIV](https://arxiv.org/abs/2510.10182)].

| Conference | 2015 | 2016 | 2017 | 2018 | 2019 | 2020 | 2021 | 2022 | 2023 | 2024 | 2025 |
|-------------|------|------|------|------|------|------|------|------|------|------|------|
| **NIPS**        | 0    | 1    | 2    | 2    | 3    | 3    | 9    | 5    | 11   | 11   | x    |
| **ICML**        | 0    | 0    | 0    | 3    | 0    | 3    | 5    | 9    | 4    | 6    | 9    |
| **ICLR**        | 0    | 0    | 0    | 1    | 1    | 9    | 4    | 8    | 7    | 9    | 13   |
| **ACL**         | 6    | 5    | 4    | 8    | 11   | 7    | 10   | 10   | 17   | 13   | 17   |
| **EMNLP**       | 3    | 2    | 7    | 7    | 12   | 12   | 8    | 13   | 14   | 7    | x    |
| **NAACL**       | 3    | 2    | x    | 5    | 6    | x    | 4    | 8    | x    | 7    | 5    |
| **COLING**      | x    | 0    | x    | 2    | x    | 10   | x    | 4    | x    | 13   | 5    |
| **EACL**        | x    | x    | 6    | x    | x    | x    | 0    | x    | 4    | 4    | x    |

If you find any points in our survey worth discussing or notice any mistakes, feel free to open an issue and share your thoughts!


# Citation
If you think this survey helps, welcome to cite our paper.
```
@misc{chen2025surveyinductivereasoninglarge,
      title={A Survey of Inductive Reasoning for Large Language Models}, 
      author={Kedi Chen and Dezhao Ruan and Yuhao Dan and Yaoting Wang and Siyu Yan and Xuecheng Wu and Yinqi Zhang and Qin Chen and Jie Zhou and Liang He and Biqing Qi and Linyang Li and Qipeng Guo and Xiaoming Shi and Wei Zhang},
      year={2025},
      eprint={2510.10182},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.10182}, 
}
```



# The Paper Collections


## Overview

1. **When Is Inductive Inference Possible?** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/a8808b75b299d64a23255bc8d30fb786-Paper-Conference.pdf)]


## Benchmarks, Datasets and Tasks

1. **On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset** [NIPS2019] [[paper link](https://papers.nips.cc/paper_files/paper/2019/file/d97d404b6119214e4a7018391195240a-Paper.pdf)]

2. **Learning abstract structure for drawing by efficient motor program induction** [NIPS2020] [[paper link](https://papers.nips.cc/paper_files/paper/2020/file/1c104b9c0accfca52ef21728eaf01453-Paper.pdf)]

3. **A large-scale benchmark for few-shot program induction and synthesis** [ICML2021] [[paper link](https://proceedings.mlr.press/v139/alet21a/alet21a.pdf)]

4. **What Has a Foundation Model Found? Inductive Bias Reveals World Models** [ICML2025] [[paper link](https://openreview.net/pdf?id=i9npQatSev)]

5. **Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling** [ICLR2022] [[paper link](https://openreview.net/pdf?id=N0n_QyQ5lBF)]

6. **GeoILP: A Synthetic Dataset to Guide Large‑Scale Rule Induction** [ICLR2025] [[paper link](https://openreview.net/pdf?id=cfGpIcOIa5)]

7. **MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models** [ICLR2025] [[paper link](https://arxiv.org/pdf/2410.09542)]

8. **MTR:A Dataset Fusing Inductive, Deductive, and Defeasible Reasoning** [ACL2023] [[paper link](https://aclanthology.org/2023.findings-acl.640.pdf)]

9. **InductionBench: LLMs Fail in the Simplest Complexity Class** [ACL2025] [[paper link](https://aclanthology.org/2025.acl-long.1287.pdf)]

10. **Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting** [NAACL2024] [[paper link](https://aclanthology.org/2024.naacl-long.299.pdf)]

11. **CLUTRR:ADiagnostic Benchmark for Inductive Reasoning from Text** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1458.pdf)]

12. **A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs** [EMNLP2023] [[paper link](https://aclanthology.org/2023.findings-emnlp.713.pdf)]

13. **Language Models as Inductive Reasoners** [EACL 2024] [[paper link](https://aclanthology.org/2024.eacl-long.13.pdf)]

14. **RuDSI: Graph-based Word Sense Induction Dataset for Russian** [TextGraphs-16 (2022)] [[paper link](https://aclanthology.org/2022.textgraphs-1.9/)]

15. **ShadowSense: A Multi-annotated Dataset for Evaluating Word Sense Induction** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.1286/)]

16. **Code-Driven Inductive Synthesis: Enhancing Reasoning Abilities of Large Language Models with Sequences** [Arxiv2025] [[paper link](https://arxiv.org/abs/2503.13109)]


## Explanations and Explorations

1. **A New Neural Kernel Regime: The Inductive Bias of Multi-Task Learning** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/fdff3c4130c24c40c88aa41eb52d2a27-Paper-Conference.pdf)]

2. **The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/75b0edb869e2cd509d64d0e8ff446bc1-Paper-Conference.pdf)]

3. **Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/7aae9e3ec211249e05bd07271a6b1441-Paper-Conference.pdf)]

4. **Inductive biases of multi-task learning and finetuning: multiple regimes of feature reuse** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/d7346ff79699b5bba26f8af89e700a95-Paper-Conference.pdf)]

5. **Demystifying Inductive Biases for (Beta-)VAE Based Architectures** [ICML2021] [[paper link](http://proceedings.mlr.press/v139/zietlow21a/zietlow21a.pdf)]

6. **Fast Rates for Noisy Interpolation Require Rethinking the Effects of Inductive Bias** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/donhauser22a/donhauser22a.pdf)]

7. **Inductive Biases and Variable Creation in Self-Attention Mechanisms** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/edelman22a/edelman22a.pdf)]

8. **The SSL Interplay: Augmentations, Inductive Bias, and Generalization** [ICML2023] [[paper link](https://openreview.net/pdf?id=d2aohFmZoB)]

9. **Position: The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning** [ICML2024] [[paper link](https://openreview.net/pdf?id=EaJ7nqJ2Fa)]

10. **Towards Understanding Inductive Bias in Transformers: A View From Infinity** [ICML2024] [[paper link](https://openreview.net/pdf?id=HOMXUneCTR)]

11. **What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation** [ICML2024] [[paper link](https://openreview.net/pdf?id=O8rrXl71D5)]

12. **When Diffusion Models Memorize: Inductive Biases in Probability Flow of Minimum-Norm Shallow Neural Nets** [ICML2025] [[paper link](https://openreview.net/pdf?id=WD2CKUrxmx)]

13. **Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence** [ICML2025] [[paper link](https://openreview.net/pdf?id=Xw01vF13aV)]

14. **Stability and Generalization Capability of Subgraph Reasoning Models for Inductive Knowledge Graph Completion** [ICML2025] [[paper link](https://openreview.net/pdf?id=NE6Px91RkQ)]

15. **The Inductive Bias of ReLU Networks on Orthogonally Separable Data** [ICLR2021] [[paper link](https://openreview.net/pdf?id=krz7T0xU9Z_)]

16. **What they do when in doubt: a study of inductive biases in seq2seq learners** [ICLR2021] [[paper link](https://arxiv.org/abs/2006.14953)]

17. **Predicting Inductive Biases of Pre-Trained Models** [ICLR2021] [[paper link](https://openreview.net/pdf?id=mNtmhaDkAr)]

18. **Deconstructing the Inductive Biases of Hamiltonian Neural Networks** [ICLR2022] [[paper link](https://arxiv.org/pdf/2202.04836)]

19. **The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design** [ICLR2022] [[paper link](https://arxiv.org/pdf/2110.04541)]

20. **A theoretical study of inductive biases in contrastive learning** [ICLR2023] [[paper link](https://arxiv.org/pdf/2211.14699)]

21. **Strong inductive biases provably prevent harmless interpolation** [ICLR2023] [[paper link](https://arxiv.org/pdf/2301.07605)]

22. **Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture** [ICLR2025] [[paper link](https://arxiv.org/pdf/2410.12025)]

23. **Generalization through variance: how noise shapes inductive biases in diffusion models** [ICLR2025] [[paper link](https://arxiv.org/pdf/2504.12532)]

24. **Combining Induction and Transduction for Abstract Reasoning** [ICLR2025] [[paper link](https://arxiv.org/pdf/2411.02272)]

25. **Language Models Need Inductive Biases to Count Inductively** [ICLR2025] [[paper link](https://arxiv.org/pdf/2405.20131)]

26. **Examining the Inductive Bias of Neural Language Models with Artificial Languages** [ACL2021] [[paper link](https://aclanthology.org/2021.acl-long.38.pdf)]  

27. **Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models**  [ACL2022] [[paper link](https://aclanthology.org/2022.findings-acl.106.pdf)] 

28. **Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations**  [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.632.pdf)] 

29. **How to Plant Trees in LMs: Data and Architectural  Effects on the Emergence of Syntactic Inductive Biases**  [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.629.pdf)] 

30. **Instruction Induction: From Few Examples  to Natural Language Task Descriptions** [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.108.pdf)] 

31. **Too Big to Fail: Larger Language Models are Disproportionately Resilient  to Induction of Dementia-Related Linguistic Anomalies** [ACL2024] [[paper link](https://aclanthology.org/2024.findings-acl.380.pdf)]

32. **Identifying Semantic Induction Heads to Understand In-Context Learning** [ACL2024] [[paper link](https://aclanthology.org/2024.findings-acl.412.pdf)] 

33. **Information Locality as an Inductive Bias for Neural Language Models**  [ACL2025] [[paper link](https://aclanthology.org/2025.acl-long.1357.pdf)] 

34. **Do Robot Snakes Dream like Electric Sheep?  Investigating the Effects of Architectural Inductive Biases on Hallucination**  [ACL2025] [[paper link](https://aclanthology.org/2025.findings-acl.60.pdf)] 

35. **Can Input Attributions Explain Inductive Reasoning  in In-Context Learning?** [ACL2025] [[paper link](https://aclanthology.org/2025.findings-acl.1092.pdf)] 

36. **GCG-Based Artificial Languages  for Evaluating Inductive Biases of Neural Language Models**  [ACL2025] [[paper link](https://aclanthology.org/2025.conll-1.35.pdf)] 

37. **Studying the Inductive Biases of RNNs  with Synthetic Variations of Natural Languages** [NAACL2019] [[paper link](https://aclanthology.org/N19-1356.pdf)]

38. **On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies** [NAACL2021] [[paper link](https://aclanthology.org/2021.naacl-main.404.pdf)]

39. **Unveiling Divergent Inductive Biases of LLMs on Temporal Data** [NAACL2024] [[paper link](https://aclanthology.org/2024.naacl-short.20.pdf)]    

40. **Text Annotation via Inductive Coding: Comparing Human Experts to  LLMs in Qualitative Data Analysis** [NAACL2025] [[paper link](https://aclanthology.org/2025.findings-naacl.361.pdf)]

41. **Induction Heads as an Essential Mechanism for Pattern Matching in  In-context Learning** [NAACL2025] [[paper link](https://aclanthology.org/2025.findings-naacl.283.pdf)]

42. **Profiling neural grammar induction on morphemically tokenised  child-directed speech** [NAACL2025] [[paper link](https://aclanthology.org/2025.cmcl-1.7.pdf)]

43. **Effects of Parameter Norm Growth During Transformer Training:  Inductive Bias from Gradient Descent** [EMNLP2021] [[paper link](https://aclanthology.org/2021.emnlp-main.133.pdf)] 

44. **Injecting structural hints:  Using language models to study inductive biases in language learning** [EMNLP2023] [[paper link](https://aclanthology.org/2023.findings-emnlp.563.pdf)]  

45. **Scaling Laws vs Model Architectures:  How Does Inductive Bias Influence Scaling?** [EMNLP2023] [[paper link](https://aclanthology.org/2023.findings-emnlp.825.pdf)]   

46. **Syntactic Inductive Bias in Transformer Language Models:  Especially Helpful for Low-Resource Languages?** [EMNLP2023] [[paper link](https://aclanthology.org/2023.conll-1.17.pdf)]  

47. **Inductive Bias Is in the Eye of the Beholder** [EMNLP2023] [[paper link](https://aclanthology.org/2023.genbench-1.12.pdf)]   

48. **It is not True that Transformers are Inductive Learners: Probing NLI Models with External Negation** [EACL 2024] [[paper link](https://aclanthology.org/2024.eacl-long.116.pdf)]  

49. **Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis** [arXiv 2023] [[paper link](https://arxiv.org/abs/2308.07942)]

50. **How Well Can a Long Sequence Model Model Long Sequences? Comparing Architectural Inductive Biases on Long-Context Abilities** [COLING 2025] [[paper link](https://aclanthology.org/2025.coling-main.3.pdf)]


## Methods

1. **A Sparse Interactive Model for Matrix Completion with Side Information** [NIPS2016] [[paper link](https://papers.nips.cc/paper_files/paper/2016/file/093b60fd0557804c8ba0cbf1453da22f-Paper.pdf)]

   一个inductive model用作矩阵补全任务。【矩阵补全】【矩阵补全数据集】【RMSE/MAE】【低秩方法与残差模型】

2. **Neural Program Meta-Induction** [NIPS2017] [[paper link](https://arxiv.org/pdf/1710.04157)]

     集中在Neural Program Induction任务当中，对于其他域少量训练样本，使用知识迁移的方法，提升模型泛化能力。【Neural Program Induction】【人工构建Karel程序任务集】【I/O示例预测准确率】【各种设置下的transformer模型】

3. **Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning** [NIPS2018] [[paper link](https://arxiv.org/pdf/1805.08402)]

   有关inductive transfer learning的一种方法。【K-ITL任务】【MNIST（数字识别）, Omniglot（手写字符）, Isolet（语音字母识别）,tinyImageNet（图像分类子集）】【准确率】【CNN模型】

4. **Library Learning for Neurally-Guided Bayesian Program Induction** [NIPS2018] [[paper link](https://papers.nips.cc/paper_files/paper/2018/file/7aa685b3b1dc1d6780bf36f7340078c9-Paper.pdf)]

   有关Program Induction任务，现有方法依赖于手工设计的DSL（领域特定语言），通用性和扩展性受限。解决方法是：构建算法 EC²，从任务中自动探索程序；压缩并归纳出可复用的DSL；学习神经网络来辅助搜索。【Neural Program Induction】【三个域数百个任务】【任务解决率等】【不同设定下的模型】

5. **On the Inductive Bias of Neural Tangent Kernels** [NIPS2019] [[paper link](https://papers.nips.cc/paper_files/paper/2019/file/c4ef9c39b300931b69a36fb3dbb8d60e-Paper.pdf)]

   对于模型优化过程神经切线核（NTK）的inductive bias的探索，本文给出了一种基于核函数的提高方法。【图像分类】【CIFAR-10, MNIST】【准确率】【CNN模型】

6. **Provable Non-linear Inductive Matrix Completion** [NIPS2019] [[paper link](https://papers.nips.cc/paper_files/paper/2019/file/ce5193a069bea027a60e06c57a106eb6-Paper.pdf)]

   是一种indcutive matrix补全的机器学习方法。【矩阵补全】【Movielens‑100K/Movielens‑10M】【RMSE/MAE】【矩阵补全模型】

7. **Discovering Symbolic Models from Deep Learning with Inductive Biases** [NIPS2020] [[paper link](https://arxiv.org/pdf/2006.11287)]

   通过引入强先验偏置和符号回归，从训练好的深度模型中提取出明确的物理规律和符号模型，以GNN模型类比物理反应。【物理建模领域】【粒子系统，暗物质模拟】【模拟准确性】【GNN模型】

8. **Latent Template Induction with Gumbel-CRFs** [NIPS2020] [[paper link](https://arxiv.org/pdf/2011.14244)]

   提出了一种结构化变分自编码器（Structured Variational Autoencoder）模型，利用连续松弛的条件随机场（CRF）推理机制，自动从文本数据中归纳出控制句子生成结构的潜在模板。【文本生成】【数据到文本生成R4R和无监督同义句生成ParaNMT】【BLEU, ROUGE, Success@1】【CRF模型】

9. **Fine-grained Generalization Analysis of Inductive Matrix Completion** [NIPS2021] [[paper link](https://proceedings.neurips.cc/paper_files/paper/2021/file/d6428eecbe0f7dff83fc607c5044b2b9-Paper.pdf)]

   旨在缩小归纳矩阵补全在理论分析方面与标准矩阵补全之间的差距：此前在无分布假设条件下，IMC 的样本复杂度最高达 $O(rd^2)$，作者努力证明更优的边界，并希望引入更高效的“带权迹范数”正则策略。【矩阵补全】【推荐数据】【理论界与实证loss】【IMC/ATR】

10. **Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning** [NIPS2021] [[paper link](https://arxiv.org/abs/2110.02370)]

     预训练的大型语言模型（如T5、GPT-3）是否具备对抽象文本推理任务（如容器操作、导航路径推断）的归纳偏置？微调 T5，设计 container/navigation/composite 任务，对比tabula rasa和英语vs猫语模板。【NLP任务泛化】【每个任务构造成训练集、插值和外推测试集，用于各类泛化测试（基数、符号、组合）】【Exact match,BLEU】【T5】

11. **Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time** [NIPS2021] [[paper link](https://arxiv.org/abs/2009.10623)]

     监督学习中，辅助损失（auxiliary loss，如物理守恒、对比学习约束等）只在训练阶段使用，会带来泛化缺口，并且训练优化的是合并目标而非关注的任务损失。通过在inference时执行unsupervised优化，让inductive bias可对每个输入样本直接生效。【多任务】【多数据】【多指标】【多模型】

12. **Open Rule Induction** [NIPS2021] [[paper link](https://arxiv.org/pdf/2110.13577)]

     传统基于知识库（KB）的规则归纳只能从已有实体和关系中挖掘模式，表达能力有限；而当前基于语言模型（LM）的规则生成，如 Comet，依赖人工标注的规则，缺乏真正“归纳”能力，生成结果受限于训练样本，难以创造真实通用规则。提出 Open Rule Induction（ORI）问题：给定前提句子如 (x, rp, y)，从 LM 中挖掘 top‑k 个有效的结论 (x, rh, y) 规则，自动诱导的规则。【Open Rule Induction, 关系抽取】【自动生成开放规则与公开关系数据】【质量,F1】【BART】

13. **ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias** [NIPS2021] [[paper link](https://arxiv.org/pdf/2106.03348)]

     ViT将图像拆成1D token序列，缺乏CNN的内在归纳偏置，特别是在局部结构建模与多尺度变化上的能力。结果它需要极大规模数据和长训练才能“间接”学会这些偏置。本文将CNN的 locality和scale‑invariance偏置直接引入ViT。【图像分类和下游任务】【ImageNet-1K和下游数据集】【top1等】【ViT系列】

14. **INDIGO: GNN‑Based Inductive Knowledge Graph Completion Using Pair‑Wise Encoding** [NIPS2021] [[paper link](https://papers.nips.cc/paper_files/paper/2021/file/0fd600c953cde8121262e322ef09f70e-Paper.pdf)]

     GNN用类比的思路获取新实体的表示。【KG补全】【GraIL-BM等】【F1等】【GNN系列】

15. **Towards Open‑World Feature Extrapolation: An Inductive Graph Learning Approach** [NIPS2021] [[paper link](https://arxiv.org/abs/2110.04514)]

     为了使得训练阶段的特征外推到测试阶段新的特征，利用GNN每步随机抽取k个特征构建子图以训练外推能力。【分类和CTR任务】【对应数据集】【acc和ctr】【GNN系列】

16. **Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines** [NIPS2022] [[paper link](https://papers.nips.cc/paper_files/paper/2022/file/0113ef4642264adc2e6924a3cbbdf532-Paper-Conference.pdf)]

     人类自然语言任务描述和程序归纳得到的抽象表示作为额外监督，可使训练出的agents更具人类式归纳偏置。【Meta-RL格式的网格游戏】【GSP人类采样网格和对照网络】【human priors分布和control分布对比】【meta‑RL的各种设定】

17. **Inductive Logical Query Answering in Knowledge Graphs** [NIPS2022] [[paper link](https://papers.nips.cc/paper_files/paper/2022/file/6246e04dcf42baf7c71e3a65d3d93b55-Paper-Conference.pdf)]

     同样是现实知识图随着新增节点/边变化，而使这些过去的模型不能泛化。该论文提出归纳式复杂查询回答任务，即在infer时遇到全新实体，仍能正确回答逻辑查询。【一阶逻辑查询】【FB15k‑237派生数据集】【Hits@10】【GNN系列】

18. **Learning Rule-Induced Subgraph Representations for Inductive Relation Prediction** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/0b06c8673ebb453e5e468f7743d8f54e-Paper-Conference.pdf)]

     解决传统方法无法处理未见过的实体或关系的问题（即inductive setting）。利用规则诱导的子图结构（rule-induced subgraphs）捕捉局部语义模式，从而提升模型在归纳场景下的泛化能力。【归纳式关系预测】【标准归纳数据集WN18RR-Ind、FB15k-237-Ind】【MRR（Mean Reciprocal Rank）、Hit@1/3/10】【归纳模型】

19. **Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/0f25eb6e9dc26c933a5d7516abf1eb8c-Paper-Conference.pdf)]

     当前机器学习模型训练通常依赖大量数据，但并非所有数据对模型泛化能力都有同等贡献。直推式（Transductive）方法（如传统监督学习）依赖固定数据集，难以适应新数据分布；归纳式（Inductive）方法（如元学习）虽然能适应新任务，但计算成本高。文章提出一种高效数据子集选择方法，旨在同时优化直推式和归纳式模型的泛化能力，减少训练数据需求，提高计算效率。【数据高效学习, 跨模型泛化】【直推式：CIFAR-10, ImageNet; 归纳式：Mini-ImageNet（元学习基准）, OGB（图神经网络基准）】【ACCh和泛化差距】【直推式：ResNet, ViT。归纳式：MAML（元学习）, GIN】

20. **Learning from Both Structural and Textual Knowledge for Inductive Knowledge Graph Completion** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/544242770e8333875325d013328b2079-Paper-Conference.pdf)]

     传统知识图谱补全泛化性差，仅利用图谱结构信息（如TransE）的方法难以处理新实体的语义关联，仅依赖文本描述（如BERT编码）的方法忽略图谱的拓扑结构，导致推理能力不足。所以提出一种联合结构与文本知识的归纳式KGC框架。【知识图谱补全】【WN18RR-Ind,FB15k-237-Ind】【MRR（平均倒数排名）, Hit@1/3/10】【直推式：TransE,RotatE,ComplEx。归纳式：GraIL,CoMPILE,Meta-KGC】

21. **C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/7ca55c8276acf1f0aa996cd3622d1df4-Paper-Conference.pdf)]

     当前大多数表征学习认为观察数据是由多个生成因子生成的，且这些因子应该是因果可解耦的——即一个因子的变化不应影响另一个。然而，绝大多数工作忽略了生成因子之间可能存在的混杂变量（confounder），而只追求统计独立，却无法识别那些在观测数据中存在相关性的真实因果因素将先验知识作为可观测标签引入inductive bias。这个 bias 是指用领域知识提供的混淆变量信息，用于在训练中分区拟合并强制条件独立。【图像重构,分类任务】【3dshape,Candle,CelebA】【重构误差和因果指标】【VAE系列】

22. **An Inductive Bias for Tabular Deep Learning** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/8671b6dffc08b4fcf5b8ce26799b2bef-Paper-Conference.pdf)]

     引入一种偏向低频率平滑的归纳偏置以解决表格数据的目标函数通常非常不规则的问题。【表格任务】【表格数据集】【分类准确率】【一些MLP】

23. **Scaling MLPs: A Tale of Inductive Bias** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/bf2a5ce85aea9ff40d9bf8b2c2561cae-Paper-Conference.pdf)]

     探索MLP在视觉任务上随着规模增长的极限，评估在无 inductive bias的条件下，性能是否能被计算资源代偿。论文核心反复强调：MLP不具备视觉偏置，即完全依赖平铺像素信息处理，没有空间结构inductive bias；然而结果表明，通过规模（compute）可以弥补这一缺陷，验证了“scale beats bias”这一命题。【图像分类】【ImageNet‑21k】【分类准确率】【一些MLP】

24. **Mars: Situated Inductive Reasoning in an Open‑World Environment** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/1fb6d0b52f5e41b11392841a66dbfe7d-Paper-Datasets_and_Benchmarks_Track.pdf)]

     当前LLM和RL模型依赖大量“预先存储的知识”，但在新环境中难以灵活应对。作者提出“情境归纳推理（situated inductive reasoning）”：模型需要在开放世界中通过交互从零开始归纳出规则，并在决策中灵活应用这些新知识。【开放世界任务】【Mars】【Reward,Success Rate】【LLM】

25. **How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/3107e4bdb658c79053d7ef59cbc804dd-Paper-Conference.pdf)]

     通过类比Scratchpad技术实现LLM的长OOD任务泛化。【token序列合成任务】【Cycle,Parity与Addition】【训练效率和长度泛化性】【GPT2-style】

26. **Amortized Active Causal Induction with DeepReinforcement Learning** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/4e2c6423d724370b36c3a7612f25b78c-Paper-Conference.pdf)]

     当前的因果结构学习（causal structure learning）通常依赖：对每个数据集进行单独图结构推断（通常需慢速似然或复杂后验算法）；基于推断出的图再贪婪选择干预实验，效率低、对分布变更敏感。论文提出CAASL方法，使用更少干预获得更准确的因果图，适配不同图结构分布、干预类型，甚至更高维环境。【干预设计因果学习】【Gaussian SCM】【Returns,结构指标】【Random（随机干预）,Observational（继续采集观测数据）,DiffCBED等】

27. **On the Inductive Bias of Stacking Towards Improving Reasoning** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/837bc5db12f3d394d220815a7687340c-Paper-Conference.pdf)]

     高效训练的stacking策略（如渐进式层级增长）已被用于加速大规模语言模型结构训练，但这类方法除了提高训练效率外，对模型的归纳偏置影响尚未充分研究本文目标除了探索stacking的效率提升外，更聚焦于其是否能内隐地增强模型的推理能力。（本文偏FLOPS和perplexity等底层）【UL2 objective】【开放式阅读理解,数学等】【准确率】【LLM】

28. **Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/9647157086adf5aa2c0217fb7f82bb19-Paper-Conference.pdf)]

     利用扩散模型（Diffusion Models）结合交叉注意力（Cross Attention），作为一种inductive bias，提高模型在特定任务中的解耦能力。【生成任务】【CIFAR-10等】【FID,IS】【扩散模型】

29. **Explicit Inductive Bias for Transfer Learning with Convolutional Networks** [ICML2018] [[paper link](https://arxiv.org/abs/1802.01483)]

     预训练之后finetune也会有所遗忘，通过显式的正则化机制，在 fine‑tuning 过程中保持与预训练模型的相似性，从而增强迁移性能并减少遗忘。【分类任务】【ImageNet、Places‑365等】【准确率】【CV模型】

30. **Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase Procrustes Flow** [ICML2018] [[paper link](https://proceedings.mlr.press/v80/zhang18b/zhang18b.pdf)]

     多阶段非凸梯度优化做矩阵补全。【矩阵补全】【多标签学习，基因-疾病预测】【相对误差】【矩阵补全模型】

31. **Inductive Two-layer Modeling with Parametric Bregman Transfer** [ICML2018] [[paper link](https://proceedings.mlr.press/v80/ganapathiraman18a/ganapathiraman18a.pdf)]

     实现带有理论保证的全局最优或近似最优两层网络学习。 模型学习的是参数𝑊,𝑈，不需要访问训练样本进行推断。【分类任务】【G241N等】【ACC】【两层网络模型】

32. **Inductive Relation Prediction by Subgraph Reasoning** [ICML2020] [[paper link](https://proceedings.mlr.press/v119/teru20a/teru20a.pdf)]

     现有实体嵌入的方法无法处理新的实体，本文能够从局部子图结构中归纳出关系规则，并推广到未见实体和新图结构。【实体预测】【WN18RR,FB15k-237,NELL-995】【AUC,hit】【GNN模型】

33. **ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases** [ICML2021] [[paper link](http://proceedings.mlr.press/v139/d-ascoli21a/d-ascoli21a.pdf)]

    CNN拥有硬编码的局部性和权重共享等归纳偏置，在小数据集上样本效率非常高，却因表达局限而在大数据上表现逐渐成为性能瓶颈。ViT放弃卷积硬偏置，依赖全局自注意力来捕捉视觉特征，在大规模预训练下效果卓越，但在中小规模数据时容易欠拟合或者过拟合。本文使用了Gated Positional Self-Attention（GPSA）方法，提出ConViT架构解决这一类问题。【图像分类】【ImageNet-1k】【分类准确率】【ConViT】

34. **LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning** [ICML2021] [[paper link](http://proceedings.mlr.press/v139/wu21c/wu21c.pdf)]

    提出通过设计专门的合成任务（synthetic tasks），用于给模型注入“数学推理”的 inductive bias，而非通过改变架构，形成一种新型的数学预训练范式——LIME。【数学推理】【IsarStep等】【top1 top10】【Transformers】

35. **Towards Open-World Recommendation: An Inductive Model-based Collaborative Filtering Approach** [ICML2021] [[paper link](http://proceedings.mlr.press/v139/wu21j/wu21j.pdf)]

    传统协同过滤（Collaborative Filtering）通过矩阵分解学习用户和物品的latent embedding，但这是典型的transductive方法，无法处理训练中未见的新用户，也无法即时推荐（on‑the‑fly）新来访用户。现实推荐场景存在大量的few‑shot用户（极少评分）或zero‑shot用户，传统方法难以应对。本文提出Relation‑learning模块归纳计算用户query特征。【推荐系统】【 MovieLens‑1M,MovieLens‑10M等】【RMSE等推荐指标】【推荐模型】

36. **Cycle Representation Learning for Inductive Relation Prediction** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/yan22a/yan22a.pdf)]

    训练集与测试集实体不重叠，模型不能借助训练期间看到的实体embedding，仅能通过关系模式（rules）来泛化至测试图中全新实体；方法采用 entity-agnostic learning，重点学习循环结构（cycles）及其组合，而不是具体实体embedding。【关系预测】【FB15k‑237, NELL‑995, WN18RR等】【AUC‑PR, Hits@10】【GNN】

37. **Inductive Matrix Completion: No Bad Local Minima and a Fast Algorithm** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/zilber22a/zilber22a.pdf)]

    一种基于优化的矩阵补全方法。【矩阵补全】【相关数据集】【rel‑RMSE】【相关基线】

38. **Neuro-Symbolic Hierarchical Rule Induction** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/glanois22a/glanois22a.pdf)]

    该论文聚焦于 Inductive Logic Programming (ILP) 问题——从一组正负样本和背景知识中自动学习一阶逻辑规则。传统ILP方法依赖于符号组合搜索，效率低、难以扩展；而现有神经符号方法虽然可微分，但通常缺乏通用而高效的结构。作者提出HRI模型，结合元规则（meta-rules）结构化假设与可学习嵌入，旨在获得高效、可解释、且具组合泛化能力的规则归纳方法。【ILP】【相关数据集】【成功率】【相关基线】

39. **PAC-Net: A Model Pruning Approach to Inductive Transfer Learning** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/myung22a/myung22a.pdf)]

      fine‑tune容易在目标任务中过拟合，遗忘源任务核心知识（例如物理规律）；当模型高度过参数化时，修剪后仍能保留源任务性能，剪枝提供了inductive bias这为 transfer learning 提供新的思路；以同时保留源知识并适应目标任务。【分类与回归任务】【Friedman和CelebA等】【RMSE和ACC等】【相关基线】

40. **Parametric Visual Program Induction with Function Modularization** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/duan22c/duan22c.pdf)]

      视觉程序归纳（Visual Program Induction）旨在从图像或视觉场景生成可执行程序，但现有方法大多只能应对非参数化的原始函数（无参数或实例极少），难以处理复杂视觉场景中具有大量参数和多变属性的原始函数。作者发现：对于parametric primitive functions，即具有丰富且异构参数的函数，其变体数量极多（一个函数可能超过 10⁴ 种变体），使得动作空间爆炸，模型难以有效学习与归纳。因此论文提出Parametric Visual Program Induction的新任务，并提出Function Modularization 的方法，以应对动作空间庞大与函数间复杂相关性挑战。【VPI】【Pixel‑Grid dataset等】【ACC】【三个模块叠加的模型】

41. **Understanding Contrastive Learning Requires Incorporating Inductive Biases** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/saunshi22a/saunshi22a.pdf)]

      为对比学习引入归纳偏置。【分类任务】【CIFAR-100, ImageNet】【ACC】【图像对比学习模型】

42. **Graph Inductive Biases in Transformers without Message Passing** [ICML2023] [[paper link](https://openreview.net/pdf?id=HjMdlNgybR)]

      文章提出了一种新方法，在Transformer中引入图的归纳偏置，具体来说，利用位置编码（Positional Encoding）和图结构编码来捕捉图中的节点关系，而不依赖于传统的消息传递机制。【图表示学习】【Cora,Citeseer,PubMed等】【分类准确率】【Transformers,GNN】

43. **INGRAM: Inductive Knowledge Graph Embedding via Relation Graphs** [ICML2023] [[paper link](https://openreview.net/pdf?id=OoOpO0u4Xd)]

      文章提出了INGRAM方法，它通过引入关系图来为每种关系构建一个独立的图结构，并通过该图来嵌入知识图谱中的实体和关系。通过提出一种inductive方法，使得模型能够在遇到新关系或实体时，无需重新训练整个模型，而是能够快速适应新信息。【知识图谱嵌入】【FB15k,WN18等】【Hits@K】【INGRAM模型】

44. **Meta-Learning the Inductive Bias of Simple Neural Circuits** [ICML2023] [[paper link](https://openreview.net/pdf?id=757L5dtuah)]

      文章提出了一种基于元学习的框架来自动学习神经网络的归纳偏置。具体来说，通过设计一个简单的神经电路模型，并通过元学习算法，使得网络能够根据不同的任务自动调整其归纳偏置。【元学习】【Omniglot,Mini-ImageNet】【ACC】【神经电路模型】

45. **SLOG: An Inductive Spectral Graph Neural Network Beyond Polynomial Filter** [ICML2024] [[paper link](https://openreview.net/pdf?id=0SrNCSklZx)]

      通过对子图进行采样并仅在子图上应用实值阶滤波器，避免对整图进行谱分解，使模型能够处理图中未见过的新节点。这种设计使 SLOG 具备真正的归纳式推理能力。【图节点分类】【图数据集】【ACC】【SLOG模型】

46. **Tripod: Three Complementary Inductive Biases for Disentangled Representation Learning** [ICML2024] [[paper link](https://openreview.net/pdf?id=0iXp5P77ho)]

      论文关注为什么无监督表示学习难以实现因子化disentanglement。前人已有多种 inductive bias，如 latent quantization、latent independence（total correlation）和decoder的Hessian正则化，单独使用时都有助益，但组合时往往难以训练、性能没有实质提升。动机即是：能否将三种互补的 inductive bias 有效融合在一个autoencoder中，借此精确限定latent空间结构，从而实现更可靠的disentangled表征学习。 inductive bias来引导模型disentanglement。三条腿分别作用于：latent空间压缩（量化）、latent变量集体独立性、decoder function中latent相互影响最小化，分别对应encoder、latent空间、decoder的inductive bias体现。【无监督的disentangled表征学习】【四个图像disentanglement数据集】【InfoMEC】【Tripod】

47. **Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases** [ICML2024] [[paper link](https://openreview.net/pdf?id=v2o9rRJcEv)]

      利用diffusion模型的temporal inductive bias：确保reward优化与模型多步生成过程对齐，避免仅关注最终图像评分的偏差。【文生图】【Stable Diffusion v1.4】【sample efficiency】【TDPO】

48. **Inductive Moment Matching** [ICML2025] [[paper link](https://openreview.net/pdf?id=pwNSUo7yUb)]

      现有的扩散模型（Diffusion）和流匹配模型（Flow Matching）虽然能生成高质量图像，但推理时需要大量采样步骤，导致推理慢。模型通过自身生成的样本来匹配目标分布，类似数学归纳法自我引导学习。【文生图】【ImageNet‑256×256和CIFAR‑10】【FID】【IMM】

49. **Customizing the Inductive Biases of Softmax Attention using Structured Matrices** [ICML2025] [[paper link](https://openreview.net/pdf?id=Roc5O1ECEt)]

      如何定制attention的归纳偏好，既能处理高维输入，又能显式编码邻近依赖信息，从而提升性能。本文实现了BTT与MLR分别提供对输入中高维依赖结构和距离依赖结构的inductive bias。【in‑context回归任务】【高维输入回归数据集】【MSE】【Transformer】

50. **iN2V: Bringing Transductive Node Embeddings to Inductive Graphs** [ICML2025] [[paper link](https://openreview.net/pdf?id=BYakLzKJDz)]

      传统的图节点嵌入方法，如 Node2Vec（N2V），通常依赖于图的整体结构进行训练，因此属于传递性（transductive）方法，即在训练期间需要访问整个图，包括测试节点。然而，在实际应用中，图结构可能会发生变化，出现新的节点和边，这些新的节点在训练期间是不可见的。因此，如何将传递性节点嵌入方法扩展到归纳性（inductive）设置，使其能够为训练期间未见过的节点生成有效的嵌入。【图节点分类】【Cora,Citeseer和PubMed等等】【ACC】【GNN】

51. **GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder** [ICML2025] [[paper link](https://openreview.net/pdf?id=AYxiZfJN9V)]

      生成式零样本学习（Generative ZSL）常借助GAN、VAE 等生成视觉特征，但多数方法 直接从强语义向量生成样本，对新类别泛化能力有限。本文目标是通过从相似已见类“归纳”出新类样本。本文提出GenZSL：一类具有归纳能力的变分自编码器模型。它不从零生成目标类样本，而是通过从相似已见类中“归纳（induct）”合成新类样本，模拟人类概念迁移。【图零样本分类】【AwA2等等】【ACC】【GenZSL模型】

52. **Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking** [ICLR2018] [[paper link](https://openreview.net/pdf?id=r1ZdKJ-0W)]

      传统图嵌入方法把每个节点表示为低维向量，但这忽略了“表示的不确定性” —— 即当节点信息有冲突或多样性时，单一点向量难以表达这一不确定程度。此外，现有大多数方法为transductive（传导式），训练时不能自然泛化到新的、未见过的节点，需要重新训练或依赖其结构信息。作者因此提出一种将节点嵌入为高斯分布的方法，并结合属性信息，实现不依赖已有图结构也能表示新节点。【链接预测，节点分类】【Cora,DBLP等等】【AUC,F1】【G2G模型】

53. **DEEP REINFORCEMENT LEARNING WITH RELATIONAL INDUCTIVE BIASES** [ICLR2019] [[paper link](https://openreview.net/pdf?id=HkxaFoC9KQ)]

      传统的model‑free深度强化学习（如 CNN+A2C/DQN）在样本效率、泛化能力以及可解释性方面存在缺陷，尤其难以在组合性变化的环境中迁移和泛化。本文将图像输入编码为一组实体向量，然后通过多头点积自注意力模块进行迭代推理，学习实体间的关系。这相当于一个图神经网络或Transformer风格的关系推理层。生成关系感知的表示后，再由actor‑critic模型输出策略与价值估计。【Box‑World，星际2】【这俩数据集】【游戏中得分】【自己训练的Relational Agent】

54. **An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality** [ICLR2020] [[paper link](https://arxiv.org/abs/2002.05825)]

      距离度量在机器学习中无处不在——用于相似度、损失函数、学习目标等。三角不等式既是理论收敛与最优性的基础，也是很有用的归纳偏置。然而，大多数深度度量学习方法仅通过欧氏距离在潜在空间中满足三角不等式，但很多实际场景中的距离（如图中最短路径、RL 中的状态距离）是非对称或无法嵌入欧氏空间的，普通方法无法建模。本文因此提出新的神经网络架构，在结构上就保证满足三角不等式，以更好地刻画这些复杂距离。【图距离建模】【图数据集,RL环境】【各种距离函数】【作者自己提出的三个模型】

55. **Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction** [ICLR2020] [[paper link](https://arxiv.org/abs/2002.00737)]

      不修改模型结构、不微调，只使用预训练语言模型的隐表示（hidden states）来进行语法归纳任务。【无监督语法归纳】【WSJ,multiNLI等】【F1】【Bert等】

56. **Inductive Matrix Completion Based on Graph Neural Networks** [ICLR2020] [[paper link](https://arxiv.org/pdf/1904.12058)]

      矩阵补全（Matrix completion）方法（如传统的矩阵分解或 GNN-based 推断）多为 transductive：它们依赖学习特定用户／物品的潜在嵌入，因此无法推广到训练中未见过的新用户、新物品，且常需使用侧信息（例如用户年龄或电影类型）以实现inductive性能。通过纯粹从评分矩阵构建的二部图中提取local enclosing subgraph（以特定(user, item)为中心的局部子图），并用GNN直接学习子图 → rating的映射，从而让模型具备对新用户／新物品或新评分矩阵的直接迁移能力。【矩阵补全】【Flixster,DOUBAN,MovieLens等等】【RMSE】【IGMC】

57. **Inductive representation learning on temporal graphs** [ICLR2020] [[paper link](https://arxiv.org/abs/2002.07962)]

      现实世界中大量图结构数据具有连续时间变化特性（例如社交网络、用户–商品交互、人–物项购买等），同时随着节点与边的不断新增、删除、特征变化，图结构也会随之演化。已有的大多数图嵌入方法仅能处理静态图或离散快照，缺乏处理连续时间演化与节点新增的能力。文章强调，在工业大规模动态图下，模型需要具备 Inductive inference（新节点/新时间点的嵌入推断能力）和 time-feature interactions 排列。故提出新的模型架构来解决：对时间编码与邻域聚合一体化设计，实现平滑连接到 GraphSage／GAT 思路，但扩展到 时序图，并且支持 Inductive 嵌入生成。【Future Link Prediction】【Reddit,维基等等】【AUC】【TGAT各种变体】

58. **Synthesizing Programmatic Policies that Inductively Generalize** [ICLR2020] [[paper link](https://openreview.net/pdf?id=S1l8oANFDH)]

      深度强化学习虽在多个控制任务上取得进展，但其策略通常“过拟合”训练环境，难以应对诸如样本大小变化、目标数量变化、地点变化等在测试时才遇到的情况，尤其是需要反复某种子行为才能达成的任务；作者将此类能力称为 “归纳泛化（inductive generalization）” —— 能够对任意次数的重复行为（例如多次绕行、拉杆上山等）自动扩展策略；他们发现，程序化的有限状态机(policy as state machine) 天然支持这种行为的循环与分支，从而具有更强的泛化能力；但此类策略很难通过传统梯度网络直接学习，因为其兼具离散控制结构与连续参数部分；故论文提出一种学习范式：基于program synthesis + teacher‑student imitation 的“自适应教学”方法，用有限规则描述policy结构，让策略学到可泛化的 “程序”，而不是绑定到特定范围数据的神经网络。【归纳泛化】【六个归纳泛化场景】【成功率】【一些RL模型】

59. **GraphSAINT: Graph Sampling Based Inductive Learning Method** [ICLR2020] [[paper link](https://arxiv.org/pdf/1907.04931)]

      当下的 GCN 结构在大图上训练时面临**“邻域指数增长”（neighbor explosion）**问题：随着网络层数增加，每个节点的多跳邻居数量迅速膨胀。现有的图像层采样（layer sampling）方法（如 GraphSAGE、FastGCN、S‑GCN、AS‑GCN 等）虽有一定减轻邻域扩展的效果，但仍面临准确率下降、训练开销大、架构兼容性差等问题。因此，论文提出：从“采样图”而非“采样层”入手，改为先抽取图子集，再构建完整的 GCN 并进行训练，从根本上解决扩展性与准确性之间的冲突。【节点分类】【六个节点分类数据集】【F1】【GraphSAINT】

60. **Inductive and Unsupervised Representation Learning on Graph Structured Objects** [ICLR2020] [[paper link](https://openreview.net/pdf?id=rkem91rtDB)]

      许多图结构任务（比如图分类、检索、异常检测）缺少标签或难以获取标签，因此迫切需要一种方法能够在没有监督信号（unsupervised）且能处理未见图（inductive） 的情况下生成图的向量表示。【图分类图聚类】【七个数据集】【ACC】【自己的模型】

61. **Learn to Explain Efficiently via Neural Logic Inductive Learning** [ICLR2020] [[paper link](https://arxiv.org/pdf/1910.02481)]

      在接受噪声和大规模 KB 的同时，用可学习且高效的方法生成更长、更丰富、且具有全局一致性的符号 FOL 规则作为解释。【KG补全】【ILP benchmark等】【MRR,Hit@10】【NLIL模型】

62. **Implementing Inductive bias for different navigation tasks through diverse RNN attractors** [ICLR2020] [[paper link](https://arxiv.org/pdf/2002.02496)]

      动物／人工智能在导航任务中需要某种内部表示（cognitive map），通常假设是欧几里得 metric 地图；但这种“地图”形式在不同导航任务中并非最优。PosNet 的 RNN 形成 “plane/continuous attractor”，便于位置积分与 metric 推理；MemNet 则形成 “discrete attractors”，易于编码地标记忆与 topological 推断。【grid路径任务】【15×15离散grid arena】【Trial-level score】【RNN LSTM各种net】

63. **Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks** [ICLR2021] [[paper link](https://arxiv.org/pdf/2101.05974)]

      现实世界中的temporal networks（社交互动、通信日志等）遵循诸如三角闭合、前馈控制环路等动态演化规律，而这些规律通常独立于节点身份（node identity），是系统的结构性laws。要对新网络泛化，仅靠记忆节点编号是不够的。因此，论文的核心动机是设计一种能够捕捉网络动态规律（motifs）、同时对节点身份保持匿名、还能对新网络实现 归纳式泛化的representation learning方法。【Temporal link prediction】【Reddit, Wikipedia等】【AUC】【CAW系列】

64. **Neural Structured Prediction for Inductive Node Classification** [ICLR2022] [[paper link](https://arxiv.org/pdf/2204.07524)]

      结合 GNN 的高容量与效率，以及 CRF 的结构输出建模能力，来改进归纳节点分类任务。【inductive节点分类】【Cora等】【ACC】【GNN+CRF】

65. **Inductive Relation Prediction Using Analogy Subgraph Embeddings** [ICLR2022] [[paper link](https://openreview.net/pdf?id=PTRo58zPt3P)]

      传统基于嵌入的知识图谱链接预测方法依赖于固定实体与关系的训练集，因此无法处理测试时出现的全新关系类型（即只在推理时才出现的关系）。作者提出利用子图结构与类比模式，在归纳设置中支持新关系的预测，同时带来更好的泛化与可解释性。【归纳关系预测任务】【对应数据集】【MRR/Hits@K】【GraphANGEL模型】

66. **On Incorporating Inductive Biases into VAEs** [ICLR2022] [[paper link](https://arxiv.org/pdf/2106.13746)]

      在标准 VAE 中通过替换先验分布（prior）来引入 inductive bias（归纳偏好）往往效果不佳，论文提出了用 Intermediary Latent Space VAEs（InteL‑VAEs）进行改善。【图像生成】【VAE对应数据集】【ELBO】【VAE系列】

67. **Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling** [ICLR2022] [[paper link](https://arxiv.org/pdf/2110.06021)]

      继承领域 inductive bias（如多模态、层级耦合、连续性）而保持流模型训练的灵活性。【结构化推断】【合成数据集】【对数似然, ELBO】【多EMF结构】

68. **Learning Language Representations with Logical Inductive Bias** [ICLR2023] [[paper link](https://arxiv.org/abs/2302.09458)]

      Transformer 在大型预训练语言模型中表现优越，这部分归因于其固有的 relational inductive bias（如 pairwise attention），但模型缺乏 逻辑推理层次的结构偏好。作者希望引入一种新的 “逻辑归纳偏好（logical inductive bias）”，使模型不仅能建模分布式语义表征，还能具备形式化的逻辑推理能力。将一阶逻辑 Horn 子句作为可学习的 神经逻辑操作符，并通过 forward chaining（前向连锁推理） 构建一个全可微的逻辑推理网络。【文本分类, NIL等】【GLUE等】【ACC, F1】【FOLNet模型】

69. **LogicDP: Creating Labels for Graph Data via Inductive Logic Programming** [ICLR2023] [[paper link](https://openreview.net/pdf?id=2b2s9vd7wYv)]

      现实中的图数据（如场景图和知识图）往往不完整，但图推理模型（如知识图补全、场景图补全）都需要大量训练数据才能取得好效果。人工标注成本高。现有的 Data Programming（DP）方法主要面向非结构化数据，不适用于图结构数据，也依赖专家手动编写 labeling functions（标注函数）。论文动机在于：能否自动生成图结构的标注函数，减少专家工作，并更高效地构建训练集？因此提出 LogicDP 框架。【图推理任务】【场景图数据集】【数据效率】【GNN等图推模型】

70. **Graph Signal Sampling for Inductive One‑Bit Matrix Completion: a Closed‑Form Solution** [ICLR2023] [[paper link](https://arxiv.org/pdf/2302.03933)]

      现代推荐系统中，经常在线上出现 新用户，他们在测试阶段可能只有正反馈（“喜欢”即1），而没有负反馈。传统的 one‑bit matrix completion 无法处理这种 inductive 设置（测试时出现新列只有正例）。作者动机是提出一个 可推广的新用户预测框架，能够在仅获取部分正反馈的情形下，准确推断一个新用户对所有物品的兴趣倾向。【Inductive Top‑N推荐】【Netflix】【HR@50等】【GS】

71. **Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement** [ICLR2024] [[paper link](https://arxiv.org/pdf/2310.08559)]

      论文提出了一种模拟人类归纳推理过程的 三步迭代假设精炼方法：Hypothesis Proposing：LM 根据少量示例生成多个候选规则或假设；Selection：使用 task-specific 符号解释器（symbolic interpreter）检测每个假设能覆盖多少已知例子，挑选覆盖度高者；Refinement：LM 基于反馈进一步修改所选假设，迭代数轮直至收敛。最终得到一个符合多数示例并具有泛化潜在能力的规则，并用它预测 unseen instances。【inductive任务】【因果关系归纳（causal induction）,语言式组合指令归纳（如 MiniSCAN）,符号操作归纳（Rule）,视觉概念归纳（ARC mini versions）】【预测准确率】【GPT‑3.5, GPT‑4,Claude-2】

72. **Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers** [ICLR2024] [[paper link](https://arxiv.org/pdf/2304.00195)]

      传统 Transformer 建模物体之间的关系时，关系信息往往与对象的视觉或属性特征混合表示，导致模型无法显式地进行关系推理。作者认为，为支持从少量数据中进行归纳推理，需要一类结构性inductive bias，即把关系信息与对象特征解耦，使模型能够专注于关系信息进行抽象和泛化。 【判别性关系任务】【合成 relational reasoning benchmark】【ACC】【Transformer+Abstractor】

73. **Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks** [ICLR2024] [[paper link](https://arxiv.org/pdf/2310.06369)]

      当前大多数迁移学习方法聚焦于图像或语言分类任务，而 回归任务（特别是分子性质预测）鲜有能有效迁移的方法；回归任务中，即使源任务相关但任务定义不同，目标数据样本稀缺也带来挑战；标准迁移学习方法在处理 latent 表示几何结构差异时往往无视几何对齐，难以实现知识跨任务流动；为此，作者提出了一种基于黎曼几何的迁移方法，针对回归任务设计 inductive transfer 框架。 【分子性质回归任务】【PubChem,Ochem,CCCB】【RMSE】【GATE模型】

74. **A 2‑Dimensional State Space Layer for Spatial Inductive Bias** [ICLR2024] [[paper link](https://openreview.net/pdf?id=BGkqypmGvm)]

      计算机视觉模型通常需要具备合适的 二维空间归纳偏好（2‑D inductive bias），如位置感知、空间局部性、平移与排列不变性。传统 CNN 本身具有强局部空间偏好；但 Transformer 通常作为一维序列处理图像并借助 positional encoding，其归纳偏好较弱。本文旨在设计可嵌入 Transformer 的新层，弥补其在视觉场景中的空间结构弱偏好。 【分类】【ImageNet‑1K等】【ACC】【ViT等】

75. **Conformal Inductive Graph Neural Networks** [ICLR2024] [[paper link](https://arxiv.org/pdf/2407.09173)]

      虽然 Conformal Prediction（CP） 为传统模型提供了无分布假设下的覆盖保证，适用于 transductive 节点分类，但其要求的 exchangeability（可交换性） 在 inductive（测试阶段出现新节点）场景中破坏，主要原因是 message passing 导致 calibration 分数分布 shift。因此，现有 CP 方法无法在 inductive setting（例如 新节点加入图结构后预测）保证覆盖率。作者动机在于构建一种适用于 inductive GNN 设置 的 CP 方法，既可满足 new node/edge 到来时的覆盖保证，又不损失统计效率。 【节点分类】【多个经典图分类数据集】【真实标签覆盖率】【NodeEx CP+GNN】

76. **Hypothesis Search: Inductive Reasoning with Language Models** [ICLR2024] [[paper link](https://arxiv.org/pdf/2309.05660)]

      Abstract Hypothesis Proposal：让 LLM 生成多个自然语言层面的抽象假设；Hypothesis Filtering / Summarization：通过 LLM 或 minimal 人工筛选，缩小假设集；Concrete Program Implementation：将每个假设转化为可执行的 Python 程序，并验证其是否能正确解释已知样本。 【ARC与列表变换】【对应数据集】【ACC】【GPT3.5和4】

77. **Label-Focused Inductive Bias over Latent Object Features in Visual Classification** [ICLR2024] [[paper link](https://openreview.net/pdf?id=cH3oufN8Pl)]

      输入域偏置问题：现有的视觉分类模型通常依赖于输入图像中的视觉相似性来学习特征，这种方法可能引入与人类标注者基于世界知识所定义的隐式输出域之间的冲突。这种冲突可能限制模型在推理阶段的泛化能力。目标：提出一种方法，通过构建仅由输出标签区分的特征，来减少输入域偏置对模型推理的影响，从而提高模型的泛化能力。【分类任务】【CIFAR-10等等】【ACC】【ViT】

78. **Integrating Planning and Deep Reinforcement Learning via Automatic Induction of Task Substructures** [ICLR2024] [[paper link](https://openreview.net/pdf?id=PR6RMsxuW7)]

      尽管深度学习在高维决策任务中取得了显著进展，但在稀疏奖励和目标导向任务中仍面临学习效率低和泛化能力差的问题。经典规划的优势与局限：经典规划方法擅长处理具有层次结构的任务，通过符号知识进行高层次规划，但大多数方法依赖于预定义的子任务假设，限制了其在未知环境中的应用。提出一种框架，将 DRL 与经典规划相结合，通过从少量示范中自动诱导任务结构和子结构，克服上述挑战。【稀疏奖励和目标导向的任务】【对应数据集】【对应性能】【多种深度学习方法】

79. **LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias** [ICLR2025] [[paper link](https://arxiv.org/pdf/2410.17242)]

      传统的视图合成方法（如 NeRF、3DGS）依赖于固定的 3D 表示和渲染方程，限制了模型的泛化能力和可扩展性。挑战：如何在不依赖传统 3D 偏置的情况下，实现高质量的视图合成。【Novel View Synthesis】【多个数据集】【信噪比】【LVSM模型】

80. **Decision Tree Induction Through LLMs via Semantically-Aware Evolution** [ICLR2025] [[paper link](https://arxiv.org/pdf/2503.14217)]

      决策树具有较强的可解释性和适用性，但传统生成方法（如 CART）容易陷入贪心子优化；优秀的全局搜索算法（如 exact methods）计算复杂度高且受限于小问题规模。遗传编程（GP）虽能寻找更全局最优解，却缺乏语义引导，搜索效率低、易陷入无意义变化。因此作者提出将大语言模型（LLM）中蕴含的语义先验知识纳入 GP 操作中，以提升决策树结构搜索的效率和泛化能力。【分类与回归任务】【对应数据集】【MSE】【LLEGO】

81. **Neuron‑based Personality Trait Induction in Large Language Models** [ICLR2025] [[paper link](https://arxiv.org/pdf/2410.12327)]

      目前 LLM 刚性表达 personality traits（如 Big Five），但主流方法依赖 prompt engineering（稳定性差）或 fine-tuning（资源消耗高）。作者希望探索一个更稳定、高效且可解释的方法：直接通过 操控 LLM 中的具体神经元 实现个性特质调整，而无需微调或重训练整个模型。【LLM人格】【PERSONALITYBENCH】【自动打分】【LLaMA】

82. **Fully‑inductive Node Classification on Arbitrary Graphs** [ICLR2025] [[paper link](https://arxiv.org/pdf/2405.20445)]

      传统的图机器学习模型只能在训练所见图上泛化，无法推广到拥有全新结构、节点特征与标签空间的图。现有 Inductive GNN 方法仍假设测试图与训练图共享特征/标签空间，限制其在不同域间迁移能力。作者提出更广泛、更实用的 “fully‑inductive” 设定：模型需在任意图（结构、feature、label 均不依赖训练图）上执行分类，无需重新训练或微调。【节点分类】【30个不同图domain数据集】【ACC】【GraphAny】

83. **Differentiable Rule Induction from Raw Sequence Inputs** [ICLR2025] [[paper link](https://openreview.net/pdf?id=zDjHOsSQxd)]

      传统可微分归纳逻辑编程（Differentiable ILP）方法通常依赖符号化输入，即从预训练网络或手工特征里获得离散符号标签，再学习规则。这会造成 标签泄漏（label leakage）：模型过度依赖输入中特征标签的监督，不能直接从原始连续输入（如时间序列或图像）中归纳规则。因此，作者希望构建一个 端到端可微分 pipeline，实现从 原始序列或图像输入 学习符号规则的能力，同时避免标签泄漏。【时间序列分类, 图像归纳】【UCR, MNIST】【准确率】【NeurRL】

84. **Selective Induction Heads: How Transformers Select Causal Structures in Context** [ICLR2025] [[paper link](https://openreview.net/pdf?id=bnJgzAQjWf)]

      Transformer 中的 induction heads 已被证明是 in‑context learning 的关键机制，它们能根据上下文复制先前出现的 tokens，实现基于因果依赖的 token 预测。以往研究使用的设置假定因果结构（如 Markov 链滞后）是固定的，无法解释自然语言和实际任务中动态变化的因果关系。构建了一种 交错的 Markov 链（Interleaved Markov Chains），在同一序列中混入多个不同滞后（lag）结构，但保持 transition probabilities 不变。训练 attention-only 的 Transformer 在每个 context 中识别当前有效的滞后结构，并且通过复制相应滞后的 token 进行预测。构建了一个简化的 三层 Transformer 架构，第 1 层估计不同滞后结构的 transition probabilities，第 2 层对这些概率进行 aggregated，最后第 3 层引入 Selective Induction Head 来选择正确滞后并复制对应 token。【交错 Markov Chain生成的synthetic序列上进行next‑token prediction】【合成数据集】【对应方法】【Attention‑only Transformer】

85. **Generative Event Schema Induction with Entity Disambiguation** [ACL2015] [[paper link](https://aclanthology.org/P15-1019.pdf)]

      引入实体属性关系（如修饰词）和触发关系（如动词与实体的语法关系），改进事件模式的归纳效果。方法是使用概率主题分布表示实体及其上下文信息，通过Gibbs采样进行参数估计，利用Dirichlet先验生成分布。【无监督的事件模式归纳】【MUC-4语料库】【精确率、召回率、F-score】【提出的生成模型】

86. **Environment-Driven Lexicon Induction for High-Level Instructions** [ACL2015] [[paper link](https://aclanthology.org/P15-1096.pdf)]

      在机器人执行指令任务中，训练时学习的词汇表在测试时无法覆盖新动词，提出利用环境信息生成新的逻辑形式，动态扩展词汇表。联合建模文本、逻辑形式和环境，通过特征函数评分逻辑形式。【将自然语言指令映射为机器人动作序列】【自建的众包数据集】【IED（编辑距离）、END（Jaccard指数）】【提出的混合模型（训练时词汇归纳+测试时环境驱动搜索）】

87. **Aconvex and feature-rich discriminative approach to dependency grammar induction** [ACL2015] [[paper link](https://aclanthology.org/P15-1133.pdf)]

      无监督依存句法分析方法（从无标注语料中归纳出通用的依存语法规则）要基于生成模型（如DMV），涉及非凸优化问题，初始化敏感且易陷入局部最优。提出一种凸优化框架，支持丰富的特征表示和非投影依存结构生成。【无监督依存句法分析】【Universal Treebanks v2.0】【有向依存准确率】【提出的凸判别式模型】

88. **Probing the Linguistic Strengths and Limitations of Unsupervised Grammar Induction** [ACL2015] [[paper link](https://aclanthology.org/P15-1135.pdf)]

      无监督语法归纳生成的依存关系通常是无标记的。通过带标记的依存关系评估（CCGbank标注），分析无监督CCG解析器的能力边界，尤其关注非局部依赖等复杂结构的处理缺陷，为未来引入语义信号或弱监督提供依据。【无监督组合范畴语法（CCG）归纳与解析】【CCGbank】【带标记依存F1（LF1）、无标记依存F1（UF1）、Supertagging准确率】【在HDP-CCG模型基础上改进】

89. **Bilingual Word Embeddings from Non-Parallel Document-Aligned Data Applied to Bilingual Lexicon Induction** [ACL2015] [[paper link](https://aclanthology.org/P15-2118.pdf)]

      从仅文档对齐的数据中学习双语词嵌入，并将其应用于双语词典归纳（BLI）任务。提出了BWESG，将双语文档合并、随机打乱，确保每个词的上下文包含两种语言的词汇，使用Skip-Gram训练双语词嵌入。
    【双语词典归纳（BLI）】【三种语言对的文档对齐维基百科数据】【测试集中源语言词的正确翻译在目标语言中排名第一的比例】【BWESG】

90. **Labeled Grammar Induction with Minimal Supervision** [ACL2015] [[paper link](https://aclanthology.org/P15-2143.pdf)]

      无监督语法归纳依赖黄金词性标注（gold POS tags）。提出通过少量人工监督，从词聚类中归纳出依存结构。为每个聚类标注3个高频词的词性，通过多数投票确定聚类标签，利用HDP-CCG模型生成依存句法结构。【无监督的带标签语法归纳】【英语和汉语的CCGbank、PASCAL语法归纳挑战赛的10种语言依赖树库】【词聚类质量：Many-to-one (M-1)、V-Measure (VM)、名词/动词/其他召回率（NV/O Recall）；句法分析性能：定向带标签F1（LF1）、无向无标签F1（UF1）】【HDP-CCG】

91. **Liberal Event Extraction and Event Schema Induction** [ACL2016] [[paper link](https://aclanthology.org/P16-1025.pdf)]

      传统事件抽取方法依赖预定义事件模式，提出直接从语料中自动发现事件模式并抽取事件。先通过约束聚类将触发词和论元分别聚类为事件类型和角色，基于聚类中心触发词命名事件类型，并利用AMR/FrameNet映射论元角色名称。【事件模式发现与事件抽取】【ERE语料、ACE2005、PubMed摘要及全文】【模式发现：覆盖度（与ACE/ERE人工模式的对比）、类型/角色数量；事件抽取：触发词/论元的识别（Precision/Recall/F1）与分类准确率】【聚类+AMR/FrameNet】

92. **Extracting token-level signals of syntactic processing from fMRI- with an application to PoS induction** [ACL2016] [[paper link](https://aclanthology.org/P16-1071.pdf)]

      本文提出从fMRI数据中归纳词性信号的方法，基于高斯滑动窗口提取单词级fMRI特征，结合类型约束的二阶HMM进行弱监督词性归纳。【弱监督词性标注】【8名受试者阅读《哈利·波特与魔法石》第9章的fMRI数据】【词性标注准确率（Accuracy）、F1值（按词性类别细分）】【融合fMRI向量的HMM】

93. **ALTO: Active Learning with Topic Overviews for Speeding Label Induction and Document Labeling** [ACL2016] [[paper link](https://aclanthology.org/P16-1110.pdf)]

      通过结合主题模型（全局归纳标签）和主动学习（局部优化标注），解决文本分类中标签归纳和高效标注的问题。提出ALTO框架利用主题模型提供全局视角，并借助主动学习指导用户标注关键文档。【文本分类中的标签归纳和文档标注】【US Congressional Bills（美国国会法案）和20 Newsgroups】【Purity（纯度）、Rand Index（兰德指数）、Normalized Mutual Information（标准化互信息）】【逻辑回归分类器（用于主动学习）和LDA主题模型（用于生成主题概览）】

94. **Joint Word Segmentation and Phonetic Category Induction** [ACL2016] [[paper link](https://aclanthology.org/P16-2010.pdf)]

      通过理想化数据下的联合建模，验证无监督方法能否同时归纳词汇和语音类别，揭示了真实语音识别中上下文变异是主要挑战。方法是分层贝叶斯框架的联合模型，通过Dirichlet过程和高斯分布建模词汇与声学特征，利用Gibbs采样进行推断。【联合词汇分割和元音类别归纳】【基于Brent (1999)的儿童导向语音语料库，添加模拟共振峰数据（来自Hillenbrand et al. 1995的实验室元音数据）】【分割性能：词边界检测的精确率（P）、召回率（R）、F1值；元音聚类：配对F1值（衡量同类元音是否被正确聚类）】【分层贝叶斯模型（Dirichlet过程 + 高斯混合），基于Gibbs采样的推断方法】

95. **Multiplicative Representations for Unsupervised Semantic Role Induction** [ACL2016] [[paper link](https://aclanthology.org/P16-2020.pdf)]

      通过显式建模句法关系提升嵌入的语义角色归纳能力，解决了无监督语义角色标注（SRL）中依赖关系与词汇语义解耦的问题。方法是基于神经网络的乘法依赖变换，通过上下文预测和层次聚类实现语义角色归纳。
    【无监督语义角色归纳】【North American News Text Corpus（训练）、CoNLL 2008（评估）】【纯度（PU）、共现率（CO）、F1值；SimLex999的词相似性】【基于CBOW框架】

96. **Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems** [ACL2017] [[paper link](https://aclanthology.org/P17-1015.pdf)]

      解决代数应用题需要生成一系列算术操作来得到最终答案，但直接从问题-答案对中归纳程序极具挑战性。提出通过生成答案解释（由自然语言和数学表达式组成的中间步骤）来间接监督程序学习。【代数应用题的求解与解释生成】【自建的100,000条问题-解释对数据集】【困惑度（Perplexity）、BLEU-4（解释质量）、答案准确率（Accuracy）】【LSTM支持数学操作和指针网络】

97. **Watset: Automatic Induction of Synsets from a Graph of Synonyms** [ACL2017] [[paper link](https://aclanthology.org/P17-1145.pdf)]

      通过归纳方法从同义词词典中自动构建无歧义的词集，解决资源匮乏语言的词汇资源问题。提出Watset通过局部词义消歧和全局图聚类，从同义词图中归纳出词集。【词集归纳】【英语：WordNet、BabelNet；俄语：RuWordNet、YARN】【精确率（Precision）、召回率（Recall）、F-score（Paired F-score）】【Watset（基于局部-全局聚类）】

98. **Adversarial Training for Unsupervised Bilingual Lexicon Induction** [ACL2017] [[paper link](https://aclanthology.org/P17-1179.pdf)]

      通过对抗训练从单语词嵌入中无监督地归纳出跨语言映射关系，解决双语词典构建中的监督依赖问题。通过生成器与判别器的对抗训练学习跨语言线性映射，结合正交约束和重构损失提升性能。【无监督双语词典归纳】【Wikipedia可比语料及Gigaword大规模语料】【Top-1和Top-5准确率】【提出的三种对抗训练模型（单向、双向、对抗自编码器）】

99. **Zara Returns: Improved Personality Induction and Adaptation by an Empathetic Virtual Agent** [ACL2017] [[paper link](https://aclanthology.org/P17-4021.pdf)]

      通过多模态人格识别和动态个性适配，提升虚拟代理的共情能力，其方法从用户数据中归纳出人格特征与交互偏好之间的关系。【人格识别与虚拟代理的个性化适配】【音频：ChaLearn First Impressions数据集、文本：WCPR的Facebook和YouTube数据集】【F-score、准确率、精确率、召回率】【CNN】

100. **A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors**  [ACL2018] [[paper link](https://aclanthology.org/P18-1002.pdf)]

      现有词嵌入方法在小规模语料或稀疏特征（如罕见词、n-gram、同义词集）场景下性能下降，提出"a la carte embedding"，基于预训练词向量和线性回归学习上下文到嵌入的映射矩阵，支持动态生成新特征的嵌入。【罕见词嵌入学习；同义词集嵌入与词义消歧；n-gram嵌入与文档分类】【自建的Contextual Rare Words (CRW) 数据集；非词任务（nonce）和合成词（chimera）数据集；SemCor（用于WSD）和多个文档分类数据集】【罕见词：Spearman相关系数（与人类评分的一致性）；非词/合成词：平均倒数排名（MRR）和Spearman相关系数；WSD：准确率；文档分类：准确率】【a la carte embedding】

101. **On the Limitations of Unsupervised Bilingual Dictionary Induction**  [ACL2018] [[paper link](https://aclanthology.org/P18-1072.pdf)]

      文章通过实验归纳发现无监督双语词典归纳的缺陷（如语言形态和领域的影响），提出用同形词弱监督和图相似性度量来改进。结合对抗训练和弱监督对齐词嵌入，并用拉普拉斯特征值量化跨语言词嵌入的相似性。
     【双语词典归纳】【Polyglot Wikipedia、EuroParl、EMEA医学语料库、Finnish WaC】【Precision@1】【基于fastText的词嵌入，结合对抗训练】

102. **Embedding Learning Through Multilingual Concept Induction**  [ACL2018] [[paper link](https://aclanthology.org/P18-1141.pdf)]

      研究通过多语言概念归纳学习跨语言的词向量表示，以解决低资源语言中缺乏平行语料的问题，并提升跨语言任务的性能。方法是基于词典图提取概念，利用这些概念训练跨语言词嵌入。
     【跨语言词相似度、情感分析、往返翻译】【Parallel Bible Corpus】【往返翻译的准确率、情感分析的F1分数】【基于word2vec的skip-gram模型】

103. **Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation** [ACL2018] [[paper link](https://aclanthology.org/P18-1146.pdf)]

      从无标注文本中归纳高阶关系模式，解决了知识图谱构建中多元关系缺失的问题。提出TFBA，通过分解低阶张量并聚合二元模式，构建高阶关系模式。【高阶关系模式归纳（HRSI）】【Shootings（美国枪击事件文档）、NYT Sports（纽约时报体育新闻）、MUC（拉丁美洲恐怖事件新闻）】【人工评估（三位评审员验证模式的准确性），使用AvgFIT（平均拟合分数）选择超参数】【TFBA】

104. **End-to-End Reinforcement Learning for Automatic Taxonomy Induction** [ACL2018] [[paper link](https://aclanthology.org/P18-1229.pdf)]

      提出一种端到端强化学习方法（TaxoRL），通过联合学习术语关系表示和分类法构建，解决传统两阶段方法在自动归纳层次分类法时的错误传播和局部优化问题。用强化学习策略网络逐步构建分类法，通过全局奖励（如祖先F1）优化整体结构。【自动分类法归纳】【WordNet、SemEval-2016 Task 13的TExEval-2数据集】【祖先F1、边F1】【TaxoRL】

105. **Unsupervised Semantic Frame Induction using Triclustering**  [ACL2018] [[paper link](https://aclanthology.org/P18-2010.pdf)]

      提出无监督方法Triframes，通过三聚类从大规模三元组数据中自动归纳语义框架。将SVO三元组嵌入后聚类，利用图算法（如Watset）同时发现框架、动词及其角色。【无监督语义框架归纳】【FrameNet 1.7、DepCC、多义动词聚类数据集】【归一化修正纯度、归一化逆纯度、综合F1值】【Triframes】

106. **Orthographic Features for Bilingual Lexicon Induction** [ACL2018] [[paper link](https://aclanthology.org/P18-2062.pdf)]

      通过融合拼写特征与词嵌入，提升低资源相关语言对的双语词典归纳性能。方法是，扩展词嵌入的拼写信息，或直接调整相似性得分以结合编辑距离与嵌入相似性。【无监督双语词典归纳】【英语-意大利语、英语-德语、英语-芬兰语】【翻译准确率】【提出的方法】

107. **Connecting Distant Entities with Induction through Conditional Random Fields for Named Entity Recognition: Precursor-Induced  CRF**  [ACL2018] [[paper link](https://aclanthology.org/W18-2402.pdf)] 

      CRF在命名实体识别中，当实体被多个非实体词分隔时，局部转移信息会丢失，无法捕捉远距离依赖关系。提出了前导诱导CRF，将非实体标签作为传递媒介，利用归纳方法将远距离依赖关系编码到一阶CRF中。
     【命名实体识别】【i2b2 2012临床文本、SNUH风湿病患者出院摘要、JNLPBA 2004生物医学文献】【精确率（P）、召回率（R）、F1值（F）】【Precursor-induced CRF】

108. **Symbolic inductive bias for visually grounded learning of spoken language** [ACL2019] [[paper link](https://aclanthology.org/P19-1647.pdf)] 

      通过多任务学习结合转录语音数据，可以在端到端的视觉-语音学习框架中引入符号化的归纳偏置（symbolic inductive bias），从而提升模型性能。【语音/图像匹配、语音/文本匹配、文本/图像匹配】【Flickr8K Audio Caption Corpus：每张图像配5条语音描述、LibriSpeech用于Speech/Text任务】【Recall@10、Median Rank、说话人识别准确率、表征相似性分析（RSA）、音素解码准确率】【GRU、VGG-16】

109. **Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces**  [ACL2019] [[paper link](https://aclanthology.org/P19-1018.pdf)] 

      文章提出了一种半监督方法BLISS，用于从少量对齐词典和大量未对齐词嵌入中学习两种语言的词嵌入映射，解决了传统双语词典归纳方法对等距假设的依赖问题。【双语词典归纳】【MUSE数据集和VecMap数据集】【词对齐的准确率】【BLISS】

110. **Compound Probabilistic Context-Free Grammars for Grammar Induction**  [ACL2019] [[paper link](https://aclanthology.org/P19-1228.pdf)] 

      提出了一种增强版的概率上下文无关文法（PCFG），通过神经网络和隐变量提升模型的表达能力，从而更有效地从无标注文本中归纳语法结构。【无监督语法归纳】【英文Penn Treebank（PTB）和中文Penn Treebank（CTB）】【无标记F1分数】【PCFG】

111. **Variance of average surprisal: a better predictor for quality of grammar from unsupervised PCFG induction**  [ACL2019] [[paper link](https://aclanthology.org/P19-1235.pdf)]

      提出了指标VAS，用于评估和选择无监督语法归纳模型。通过计算句子平均惊讶度的方差（VAS），捕捉语法模型对功能词和内容词的区分能力，替代传统的数据似然作为模型选择标准。【无监督语法归纳】【多语言数据集（包括Penn Treebank、Universal Dependencies等）】【VAS、数据似然（LL）、右分支得分（RBS）、规则复杂度等】【基于贝叶斯PCFG的语法归纳模型】

112. **Domain Adaptation of Neural Machine Translation by Lexicon Induction**  [ACL2019] [[paper link](https://aclanthology.org/P19-1286.pdf)]  

      为解决神经机器翻译（NMT）跨领域时词汇缺失问题，提出无监督方法DALI，从单语数据中归纳领域词汇表并生成伪数据来微调模型。方法是用词嵌入映射和最近邻搜索提取领域词汇，构造伪平行语料训练NMT模型。【德语到英语的领域适应机器翻译】【Medical、IT、Law、Subtitles、Koran五个领域的数据集】【BLEU分数】【基于LSTM和Transformer的NMT模型】

113. **MAAM: A Morphology-Aware Alignment Model for Unsupervised Bilingual Lexicon Induction**  [ACL2019] [[paper link](https://aclanthology.org/P19-1308.pdf)] 

      为解决无监督双语词典归纳中形态学差异导致的错误对齐，提出了一种结合语法规则的方法。方法是用线性变换做词对齐，并通过去噪自编码器和语言模型引入语法信息来优化对齐结果。【无监督双语词典归纳】【基于Wikipedia训练的300维fastText词向量】【最近邻检索的准确率】【线性变换（SGD优化）+ 去噪评估器】

114. **A Multilingual BPE Embedding Space for Universal Sentiment Lexicon Induction**  [ACL2019] [[paper link](https://aclanthology.org/P19-1341.pdf)] 

      为了解决低资源语言缺乏情感词典的问题，提出了一种通用方法，通过BPE分词和多语言嵌入空间，从英语归纳出1593种语言的情感词典。方法是用BPE分词构建多语言嵌入空间，通过零样本迁移和领域适应生成情感词典。【跨语言情感词典归纳】【Parallel Bible Corpus+、Twitter数据集】【F1分数、Kendall’s 、分类准确率】【word2vec-skipgram】

115. **Hubless Nearest Neighbor Search for Bilingual Lexicon Induction**  [ACL2019] [[paper link](https://aclanthology.org/P19-1399.pdf)] 

      为了解决BLI任务中hubness（某些目标词被过度检索为“中心点”）导致的检索偏差，文章提出HNN方法，通过均衡偏好假设优化词对齐。方法是用均衡偏好约束构建优化问题，并通过高效对偶求解器减少目标词的过度检索。【双语词典归纳】【MUSE库中的6种语言的词嵌入和词典】【Top-1准确率、k-occurrence】【HNN】

116. **Duality of Link Prediction and Entailment Graph Induction**  [ACL2019] [[paper link](https://aclanthology.org/P19-1468.pdf)] 

      通过结合链接预测和蕴含图归纳，利用已知事实归纳出更全面的关系和规则，从而提升两个任务的性能。方法是使用链接预测模型预测缺失关系，构建马尔可夫链计算蕴含分数，再用这些分数优化链接预测结果。【链接预测和蕴含关系检测】【NewsSpike文本语料库（用于链接预测），Levy/Holt数据集（用于蕴含关系评估）】【蕴含任务：AUC；链接预测任务：Hits@1、Hits@10、MR、MRR】【ConvE（链接预测模型）】

117. **Bilingual Lexicon Induction through Unsupervised Machine Translation**  [ACL2019] [[paper link](https://aclanthology.org/P19-1494.pdf)] 

      双语词典归纳（BLI）依赖于跨语言词嵌入的直接检索，但这些存在“中心性”问题。提出通过无监督机器翻译从单语数据中归纳出双语词典，避免了直接检索。【双语词典归纳】【MUSE数据集】【Precision at 1】【词嵌入模型：fastText、基于短语的统计机器翻译】

118. **Unsupervised Induction of Ukrainian Morphological Paradigms for the  NewLexicon: Extending Coverage for Named Entities and Neologisms  Using Inflection Tables and Unannotated Corpora**  [ACL2019] [[paper link](https://aclanthology.org/W19-3701.pdf)] 

      通过无监督方法从语料中归纳出新词的形态变化规则，动态扩展词典，解决了静态资源覆盖不足的问题。方法是利用屈折表拆分词干和后缀，通过语料验证生成完整词形变化范式。【无监督生成乌克兰语新词的形态学范式】【乌克兰语未标注语料及静态词典】【词汇覆盖率】【基于规则的范式生成算法，依赖屈折表和语料验证】

119. **Every Document Owns Its Structure: Inductive Text Classification via  Graph Neural Networks** [ACL2020] [[paper link](https://aclanthology.org/2020.acl-main.31.pdf)] 

      提出文本分类方法TextING，为每个文档构建局部图，节点为词，通过门控机制聚合邻居信息。实现归纳式文本分类，模型可泛化到未见过的新词和新文档。同时可以通过局部图建模词与词的细粒度关系。【文本分类（情感分析、新闻分类、医学摘要分类）】【MR（电影评论情感分析）、MR（电影评论情感分析）、Ohsumed（医学摘要分类，23类）】【准确率】【TextING】

120. **Dynamic Memory Induction Networks for Few-Shot Text Classification** [ACL2020] [[paper link](https://aclanthology.org/2020.acl-main.102.pdf)] 

      通过动态记忆和查询增强，让模型从少量样本中归纳出鲁棒的类别表示，显著提升小样本分类性能。具体方法是结合动态路由调整记忆权重，利用查询信息筛选支持集样本，生成判别性类别向量。【小样本文本分类】【miniRCV1（新闻分类）和ODIC（开放域意图分类）】【分类准确率】【BERT-base、动态记忆模块（DMM）、查询增强归纳模块（QIM）】

121. **A Graph-based Coarse-to-fine Method for Unsupervised  Bilingual Lexicon Induction** [ACL2020] [[paper link](https://aclanthology.org/2020.acl-main.318.pdf)] 

      提出了一种基于图的粗到细方法，通过从单词中归纳出词团级别的语义信息，生成更准确的初始词典，从而提升无监督双语词典归纳的性能。【无监督双语词典归纳】【MUSE数据集】【Precision@1】【提出方法】

122. **Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction** [ACL2020] [[paper link](https://aclanthology.org/2020.acl-main.618.pdf)] 

      提出ClassyMap，通过分类器从少量种子词对中归纳翻译规律，结合多特征优化，提升双语词典归纳的准确性。方法是分类器整合词形、语义等特征筛选高质量翻译对，迭代优化跨语言词向量并重排序候选词。
     【弱监督双语词典归纳】【MUSE数据集】【Precision@1】【ClassyMap】

123. **The Importance of Category Labels in Grammar Induction with Child-directed Utterances** [ACL2020] [[paper link](https://aclanthology.org/2020.iwpt-1.15.pdf)] 

      无监督语法归纳通常忽略短语标签的评估，仅依赖无标签评估导致模型仅学习到表面的分支倾向，无法捕捉人类语法的稀疏性和类别分布特性。提出带标签评估RH，从儿童语料中归纳更真实的语法结构。方法是用贝叶斯PCFG模型和RH指标，结合深度限制，从无监督数据中学习符合语言学特性的语法。【无监督语法归纳】【CHILDES多语言语料、WSJ20Dev】【无标签F1、带标签RH和RVM】【贝叶斯PCFG模型】

124. **Script Induction as Association Rule Mining** [ACL2020] [[paper link](https://aclanthology.org/2020.nuse-1.7.pdf)] 

      将脚本归纳问题转化为关联规则挖掘，从叙事链中归纳高阶事件模式，提升缺失事件预测的准确性和可解释性。方法是用FP-growth挖掘频繁事件集，结合加权集合覆盖和后验概率优化预测缺失事件。【脚本归纳】【Annotated Gigaword的纽约时报部分】【Recall@50和MRR】【FP-growth】

125. **Improving Bilingual Lexicon Induction with Unsupervised Post-Processing of Monolingual Word Vector Spaces** [ACL2020] [[paper link](https://aclanthology.org/2020.repl4nlp-1.7.pdf)]

      跨语言词嵌入主要关注改进投影机制，而忽略了输入单语词向量空间的质量对跨语言对齐的影响。提出对单语词向量空间进行简单的后处理，可以显著提升双语词典归纳的性能。【双语词典归纳】【PanLex数据集】【MRR】【基于投影的跨语言词嵌入方法（如VecMap和RCSLS）】

126. **Inductively Representing Out-of-Knowledge-Graph Entities by Optimal Estimation Under Translational Assumptions** [ACL2021] [[paper link](https://aclanthology.org/2021.repl4nlp-1.10.pdf)] 

      文章解决知识图谱中未见过实体(OOKG）的表示问题，提出一种高效且无需额外训练的归纳方法，利用翻译模型的假设直接估计OOKG实体的嵌入。【链接预测、三元组分类】【FB15k、WN11】【链接预测：MRR、Hits@1/10；三元组分类：准确率】【本文方法InvTransE、InvRotatE】

127. **Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment** [ACL2021] [[paper link](https://aclanthology.org/2021.acl-long.67.pdf)]

      提出结合无监督平行语料挖掘和词对齐，通过归纳数据中的统计规律生成高质量双语词典。方法是使用CRISS挖掘平行语料，SimAlign对齐词，再通过统计特征或MLP过滤噪声，生成最终词典。【双语词典归纳】【BUCC 2020共享任务数据集、MUSE数据集、WikiMatrix平行语料】【F1分数、Precision@1】【CRISS、SimAlign、MLP】

128. **Neural Bi-Lexicalized PCFG Induction** [ACL2021] [[paper link](https://aclanthology.org/2021.acl-long.209.pdf)]

      提出神经双词汇化PCFG模型，通过潜在变量和分解技术从数据中归纳出句法规则，提升无监督句法分析性能。方法是使用规范多线性分解和“展开-折叠”技术优化词汇化PCFG的参数化和计算效率。【无监督句法分析】【华尔街日报（WSJ）语料库（Penn Treebank）】【句子级F1分数、无标记定向附着分数（UDAS）、无标记无向附着分数（UUAS）】【神经双词汇化PCFG】

129. **Knowledge-Enriched Event Causality Identification via Latent Structure  Induction Networks** [ACL2021] [[paper link](https://aclanthology.org/2021.acl-long.376v2.pdf)]

      提出了归纳学习潜在推理结构方法（LSIN），通过结合外部知识，解决事件因果关系识别任务中数据稀缺和隐式因果线索的问题。LSIN利用图神经网络和潜在结构归纳模块，从外部知识中学习并优化推理路径。
     【事件因果关系识别】【EventStoryLine和Causal-TimeBank】【精确率、召回率和F1分数】【LSIN】

130. **StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling** [ACL2021] [[paper link](https://aclanthology.org/2021.acl-long.559.pdf)] 

      提出StructFormer，通过无监督学习从文本中同时归纳依存和成分结构，并利用这些结构提升语言模型的性能。方法是通过句法距离和高度预测模块，结合依存约束的自注意力机制，实现了语言结构的联合归纳和语言建模。【无监督依存解析、无监督成分解析、掩码语言建模】【Penn TreeBank (PTB)、BLLIP】【成分解析：未标记F1分数（UF1）；依存解析：未标记附着分数（UAS）和无向未标记附着分数（UUAS）；MLM：困惑度】【StructFormer】

131. **Semantic Frame Induction using  Masked Word Embeddings and Two-Step Clustering** [ACL2021] [[paper link](https://aclanthology.org/2021.acl-short.102.pdf)]

      提出一种无监督方法，通过掩码词嵌入和两步聚类从文本中归纳语义框架，解决现有方法过度依赖动词表面信息和聚类过度分割的问题。方法总结：结合BERT掩码嵌入减少表面信息干扰，通过两步聚类（先动词内聚类，再跨动词合并）实现更准确的语义框架归纳。【无监督语义框架归纳】【英文FrameNet（Berkeley FrameNet 1.7）】【B-cubed Precision/Recall/F1（BCP/BCR/BCF）、Purity/Inverse Purity/F1】【BERT、聚类】

132. **Combining Static Word Embeddings and Contextual Representations for Bilingual Lexicon Induction** [ACL2021] [[paper link](https://aclanthology.org/2021.findings-acl.260.pdf)]

      提出结合静态词嵌入和上下文表示的机制，通过弹簧网络和相似性插值，从单语数据中归纳更准确的双语词典。通过弹簧网络调整静态词嵌入位置，结合上下文的语义信息，最终加权插值两种相似性以提升翻译匹配精度。【双语词典归纳】【Wikipedia多语言语料、WaCKy语料】【Precision@1】【静态词嵌入：fastText、上下文表示：XLM/mBART】

133. **Verb Sense Clustering using Contextualized Word Representations for Semantic Frame Induction** [ACL2021] [[paper link](https://aclanthology.org/2021.findings-acl.381.pdf)]

      研究如何用BERT等模型自动归纳动词的语义框架，减少人工标注工作。方法是用上下文词表示对动词聚类，区分不同语义框架，并用调整的BIC估计框架数量。【语义框架归纳】【FrameNet和PropBank】【聚类匹配率、Spearman相关系数、准确率、均方根误差】【Transformer】

134. **Simple induction of (deterministic) probabilistic finite-state automata for phonotactics by stochastic gradient descent** [ACL2021] [[paper link](https://aclanthology.org/2021.sigmorphon-1.19.pdf)]

      提出用梯度下降从数据中归纳PFA（概率有限状态自动机），自动学习音系规则。方法是通过优化可微分的PFA参数，支持无限制和受限子正则语言类的归纳。【音系规则建模与PFA归纳】【Quechua和Navajo的词典词形数据】【保留数据的负对数似然（NLL）、合法与非合法非词的似然差异】【PFA】

135. **Probing as Quantifying Inductive Bias** [ACL2022] [[paper link](https://aclanthology.org/2022.acl-long.129.pdf)]  

      提出通过量化预训练表示对特定任务的归纳偏置（inductive bias）来改进探测方法。将表示和探测模型视为一个整体，用贝叶斯证据量化表示-探测模型对在特定任务中的归纳偏置，通过最大化证据，自动选择探测模型架构。【词级别任务、依存弧标注任务、句子级别任务】【词级别和依存弧任务：Universal Dependencies (UD) v2.5树库、UniMorph模式；句子级别任务：MultiNLI、BoolQ、Commitment Bank、RTE】【对数证据，用于衡量表示-探测模型对的归纳偏置】【表示模型：m-BERT、fastText、随机表示（Rand.）、ALBERT、RoBERTa、XLNet、T5；探测模型：线性模型、MLP】

136. **Speaker Information Can Guide Models to Better Inductive Biases: A Case Study On Predicting Code-Switching** [ACL2022] [[paper link](https://aclanthology.org/2022.acl-long.267.pdf)] 

      NLP模型在处理用户生成数据时可能不可靠，提出为模型提供说话者信息，可以引导模型学习更有用的归纳偏置（inductive biases）。在预测英语-西班牙语双语对话中的code-switching任务中进行了验证。
     【预测英语-西班牙语双语对话中的语码转换点】【Bangor Miami数据集，包含56个英语-西班牙语对话，附带说话者的社会语言学特征】【准确率、F1分数、精确率、召回率】【在XLM-RoBERTa基础上预置说话者提示】

137. **Deep Inductive Logic Reasoning for Multi-Hop Reading Comprehension**  [ACL2022] [[paper link](https://aclanthology.org/2022.acl-long.343.pdf)] 

      针对多跳阅读理解任务，提出深度归纳逻辑推理模型DILR，通过逻辑规则生成和评估，实现可解释且高效的多跳推理。先从文档中筛选与查询相关的信息，递归生成逻辑子句，再通过逻辑运算符评估子句的真值概率。【多跳阅读理解】【WikiHop、MedHop】【准确率】【DILR】

138. **Fire Burns, Sword Cuts: Commonsense Inductive Bias for  Exploration in Text-based Games**  [ACL2022] [[paper link](https://aclanthology.org/2022.acl-short.56.pdf)] 

      强化学习代理在探索大规模动作空间时效率低下，引入常识知识作为归纳偏置（Inductive Bias），帮助代理在训练中更好地泛化。使用预训练的语言模型（如COMET）基于当前观察生成常识知识图。【文本游戏中的强化学习探索】【来自Jericho游戏套件的9个游戏】【游戏分数、生成动作的语言流畅性】【基于知识图的强化学习代理KG-A2C + CoMMEXPL(提出的结合常识的探索方法)】

139. **Hierarchical Inductive Transfer for Continual Dialogue Learning**  [ACL2022] [[paper link](https://aclanthology.org/2022.findings-acl.57.pdf)] 

      对话系统需要持续学习新的对话技能，提出分层归纳迁移框架AdaHIT。分层：基础适配器：从多任务中归纳通用对话模式（如语法、流畅性）；任务适配器：在通用知识基础上，通过微调归纳任务特定模式（如共情表达）。【持续对话学习】【ConvAI2（个性化）、WoW（知识）、ED（共情）、BST（混合技能）】【Hits@1/K】【Reddit数据预训练的Poly-encoder，进行微调】

140. **Skill Induction and Planning with Latent Language** [ACL2022] [[paper link](https://aclanthology.org/2022.acl-long.120.pdf)]

      提出一种用少量语言标注从演示数据中自动归纳分层策略的方法，通过语言描述实现可解释的任务规划。方法是结合分段、标注和参数更新的迭代优化，利用预训练语言模型从稀疏标注中学习技能库和分层策略。【分层策略学习与任务规划】【ALFRED（家庭机器人模拟环境）】【子任务完成率、端到端任务成功率】【基于T5-small的控制器和执行器】

141. **Discrete Opinion Tree Induction for Aspect-based Sentiment Analysis** [ACL2022] [[paper link](https://aclanthology.org/2022.acl-long.145.pdf)]

      提出一种无需依赖外部解析器的方法dotGCN，通过归纳学习自动生成针对特定方面的意见树结构，用于方面级情感分析，解决了依赖树模型的局限性。基于注意力分数和强化学习诱导树结构，结合GCN进行分类
     【方面级情感分析】【六个英文基准数据集（如MAMS、SemEval）、一个中文酒店评论数据集和一个韩文汽车评论数据集】【 准确率和宏平均F1分数】【提出的dotGCN模型】

142. **Hierarchical Sketch Induction for Paraphrase Generation** [ACL2022] [[paper link](https://aclanthology.org/2022.acl-long.178.pdf)]

      提出一种通过归纳学习分层句法草图的方法（HRQ-VAE），用于生成高质量且多样化的复述，解决了传统方法需要完全指定句法结构的局限性。方法: 使用分层离散潜在变量逐步细化句法结构，结合编码-解码框架生成复述。【复述生成】【Paralex（问题复述数据集）、Quora Question Pairs（QQP）、MSCOCO 2017（图像描述数据集）】【 iBLEU（平衡复述质量和多样性）、BLEU（复述质量）、Self-BLEU（复述与输入的差异）、P-BLEU（生成复述之间的多样性）】【HRQ-VAE】

143. **Large Scale Substitution-based Word Sense Induction** [ACL2022] [[paper link](https://aclanthology.org/2022.acl-long.325.pdf)]

      提出一种从大规模语料中自动归纳词义的方法，解决了传统词义消歧依赖人工清单的问题，并生成可解释的词义标注和嵌入。方法：用MLM生成替代词，通过图聚类动态归纳词义，再标注语料并训练静态嵌入。
     【词义归纳（WSI）、词义标注、词义感知静态嵌入生成】【English Wikipedia、PubMed Abstracts、SemEval 2010/2013、自建大规模人工标注数据集】【F-Score、V-Measure、FNMI、FBC、分类F1、OPP】【BERT-large、SciBERT、word2vec（生成静态嵌入）】

144. **Query Structure Modeling for Inductive Logical Reasoning  Over Knowledge Graphs**  [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.259.pdf)]  

      提出“结构建模的文本编码框架”（SILR），解决知识图谱上复杂逻辑查询的归纳推理问题。将逻辑查询结构转换为文本序列，设计逐步指令，指示PLMs在查询结构中执行几何操作的顺序。通过显式建模查询的逻辑结构，使其能够泛化到训练时未见过的新实体、关系甚至新的知识图谱。【复杂逻辑查询的归纳推理】【FB15k-237-V2、NELL-V3】【Hits@K（K=3,10）】【BERT-large-cased】

145. **Contrastive Learning with Generated Representations for Inductive  Knowledge Graph Embedding**  [ACL2023] [[paper link](https://aclanthology.org/2023.findings-acl.900.pdf)] 

      提出一种归纳式方法，能够从已有的知识图谱中捕获结构模式，并将其迁移到新的知识图谱中。设计了两种对比学习目标，分别在实体内部和元知识图谱之间进行对比，以显式模拟迁移模式。【知识图谱的归纳式表示学习和链接预测】【F1-F4、N1-N4】【MRR、Hits@1、Hits@10】【VMCL及其变体（基于TransE和RotatE的KGE模型）】

146. **Learning Query Adaptive Anchor Representation for Inductive Relation Prediction** [ACL2023] [[paper link](https://aclanthology.org/2023.findings-acl.882.pdf)] 

      知识图谱中现有的归纳式关系预测方法（如GraIL）需要为每个候选三元组提取一个封闭子图并进行多次推理，效率低下。提出QAAR模型为查询实体提取一个开放子图，覆盖所有候选实体，避免重复提取。同时引入实体无关的锚点和多类特征，提升模型对未见实体的泛化能力。【归纳式关系预测】【WN18RR、FB15k-237、NELL-995】【Hits@10】【QAAR】

147. **I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation**  [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.535.pdf)] 

      提出蒸馏算法，通过约束解码和自模仿学习显著提升GPT-2的常识生成质量。自模仿学习使模型从自身生成的样本中学习，逐步归纳（inductive）出更高质量的常识知识。【生成日常概念的常识性陈述】【种子概念来自GenericsKB、ConceptNet、ProScript和ATOMIC，扩展概念使用GPT-3生成】【准确率、PR曲线】【GPT-2】

148. **Non-Sequential Graph Script Induction via Multimedia Grounding** [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.303.pdf)]

      通过视频数据自动归纳出更灵活的任务步骤图脚本（像流程图一样），而不仅仅是固定的线性步骤。方法是用视频和文本对齐训练模型，生成包含可选和可互换步骤的图脚本。【非顺序图脚本归纳】【CrossTask、Howto100M、wikiHow】【Next Step Prediction（Acc@1, Acc@3, Rec@3, F1@3）、Partial Sequence Completion（Acc@1, Edit Distance）、人工评估（正确性和表达性）】【BART-base，结合对比学习】

149. **Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification** [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.312.pdf)]

      利用大语言模型的常识，自动归纳出更灵活、层次化的事件流程图（比如“疫情爆发”包含“病例激增”“医院超负荷”等子事件）。方法：通过分阶段提问LLM（先列主干事件，再扩展细节，最后验证关系）构建事件模式图。【开放领域层次化事件模式归纳】【ODIN（自建新闻场景库）、RESIN-11（11类新闻场景）、ProScript（日常任务）】【事件匹配F1、时序关系F1、层次关系F1；人工评估】【基于GPT-3的增量提示框架】

150. **Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic** [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.422.pdf)]

      文章提出了一种结合符号知识和神经网络的对话结构归纳方法，通过注入领域规则提升模型在数据稀缺时的表现。方法是用概率软逻辑规则指导神经网络的训练，联合优化符号约束和生成模型。【对话结构归纳】【MultiWoZ 2.1 synthetic、SGD-synthetic、SGD-real】【调整互信息（AMI）、纯度（Purity）、隐藏表示学习的分类准确率】【神经概率软逻辑对话结构归纳】

151. **Limitations of Language Models in Arithmetic and Symbolic Induction** [ACL2023] [[paper link](https://aclanthology.org/2023.acl-long.516.pdf)]

      揭示了LLMs在简单符号操作和长数字运算的归纳局限性，提出通过逐步教学和显式定位提升模型性能。方法：用动作序列模拟人类教学，将复杂任务分解为原子操作（如移动、复制），确保模型逐步正确执行。【复制、反转、加法】【自定义生成的数字序列和符号列表】【准确率，分内分布和OOD测试】【LLMs】

152. **Human-in-the-Loop Schema Induction** [ACL2023] [[paper link](https://aclanthology.org/2023.acl-demo.1.pdf)]

      提出一种人机协同的事件模式归纳方法，利用GPT-3生成候选模式并通过人工干预提升质量，解决了纯自动化方法的领域迁移和语义一致性问题。方法：四阶段流水线（生成→抽取→建图→grounding），结合大模型生成与人工编辑。【事件模式归纳、图构建、节点 grounding】【自建五个场景（如“网络攻击”“医疗救治”）】【评估基于人工标注的步骤/节点准确率、图编辑距离、grounding成功率】【GPT-3（生成步骤/节点/关系）、AllenNLP SRL模型（节点抽取）、BART（entailment评分）、GloVe（相似度计算）】

153. **Knowledge Base Question Answering for Space Debris Queries** [ACL2023] [[paper link](https://aclanthology.org/2023.acl-industry.47.pdf)]

      开发了一个透明、可验证的空间碎片问答系统，通过程序归纳和迁移学习，让模型从少量标注数据中归纳出复杂查询的逻辑，解决了领域数据稀缺问题。方法：分阶段生成查询程序（草图→参数填充），结合领域预训练和GPT-3数据增强提升泛化能力。【复杂知识库问答，支持多跳查询】【KQA Pro、DISCOS-Questions-Programs (DQP)】【各组件（函数、实体、关系等）预测准确率，整体程序生成准确率】【LLMs】

154. **Categorial Grammar Induction from Raw Data** [ACL2023] [[paper link](https://aclanthology.org/2023.findings-acl.149.pdf)]

      研究如何从儿童语言数据中无监督地归纳出基本范畴语法规则，验证了语法结构的可学习性，并通过引入分支偏置提升了性能。方法：扩展了现有的PCFG神经网络模型，通过约束语法范畴的组合方式学习更高效的语法规则。【无监督的语法归纳】【CHILDES中的英语儿童导向语料库】【召回-同质性和未标记F1分数】【基于神经网络的范畴语法归纳模型，扩展自PCFG模型】

155. **Improving Diachronic Word Sense Induction with a Nonparametric Bayesian method** [ACL2023] [[paper link](https://aclanthology.org/2023.findings-acl.567.pdf)]

      通过非参数贝叶斯和动态嵌入模型，从生物医学文本中自动归纳词义的动态演变，解决了参数化模型依赖预设词义数量的局限性。方法：结合HDP的无限聚类能力和DETM的语义嵌入，实现了更准确的词义推断和演变建模。【历时词义归纳（DWSI）,即从时间标注的文本中推断词义的动态变化】【生物医学领域的PubMed和MeSH时间标注数据集】【全局匹配（Global Matching）、V-measure（同质性和完整性）、词义涌现年份误差（MAE）】【分层狄利克雷过程HDP】

156. **Acquiring Frame Element Knowledge with Deep Metric Learning for Semantic Frame Induction** [ACL2023] [[paper link](https://aclanthology.org/2023.findings-acl.596.pdf)]

      通过深度度量学习优化预训练模型的嵌入空间，解决了语义框架归纳中论元角色区分的问题，显著提升了聚类性能。方法：结合Triplet和ArcFace损失微调BERT，并提出了框架内聚类策略以更好地捕捉角色特异性。【论元聚类（argument clustering），即根据语义角色对论元进行分组】【FrameNet 1.7】【纯度（PU）、逆纯度（IPU）、F-SCORE（PIF）、B-CUBED精确度（BCP）、召回率（BCR）和F-SCORE（BCF）】【BERT】

157. **Few-shot Dialogue Strategy Learning  for Motivational Interviewing via Inductive Reasoning**  [ACL2024] [[paper link](https://aclanthology.org/2024.findings-acl.782.pdf)]

      提出了一种归纳推理框架DIIR，从专家对话中归纳出有效的自然语言策略，在数据稀缺时提升对话系统的表现。DIIR通过LLM从有限数据生成潜在策略，并通过生成-验证循环优化这些策略。【动机性访谈对话生成】【AnnoMI】【MI不一致行为比例（%MI-i）、复杂反思比例（C/S）、主动倾听行为比例（%AL）】【GPT-3.5和GPT-4】

158. **SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation**  [ACL2024] [[paper link](https://aclanthology.org/2024.acl-long.355.pdf)]

      让seq2seq模型模拟有限状态转换器FSTs的行为，可以为模型注入结构化的归纳偏置，提升模型的泛化和小样本学习能力。【合成FST任务、自然语言任务】【合成数据：自动生成的FST输入/输出对；自然数据：Wikipron（音素转换）、SyGuS竞赛数据（文本编辑）】【准确率、编辑距离、音素错误率】【seq2seq模型】

159. **LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs** [ACL2024] [[paper link](https://aclanthology.org/2024.findings-acl.224.pdf)]

      通过LLMs生成提示图增强GNN推理器，提升模型在零样本、少样本场景下的归纳推理能力，保持对未见知识图谱的泛化性。prompt graph：利用LLM通过简洁的关系描述推断实体类型，构建关系间的交互边。【知识图谱归纳推理、零样本、单样本和三样本场景】【基于FB15k237、Wikidata68K、NELL-995构建】【MRR和Hits@N】【Llama2-7B/13B、Mistral-7B、GPT-3.5、GPT-4】

160. **DM-BLI: Dynamic Multiple Subspaces Alignment for Unsupervised Bilingual Lexicon Induction** [ACL2024] [[paper link](https://aclanthology.org/2024.acl-long.112.pdf )]

      提出动态多子空间对齐方法（DM-BLI），通过无监督聚类和对比学习，从词嵌入中归纳出子空间结构并优化映射，显著提升了远距离和低资源语言对的双语词典归纳性能。方法总结：DM-BLI通过聚类发现子空间，结合对比学习和动态更新，为每个子空间对学习定制化的映射矩阵。【无监督双语词典归纳】【MUSE双语词典，使用fastText在Wikipedia上训练的词向量】【Precision@1和Precision@5】【DM-BLI】

161. **ItD: Large Language Models Can Teach Themselves Induction through Deduction** [ACL2024] [[paper link](https://aclanthology.org/2024.acl-long.150.pdf)]

      提出ItD框架，利用大语言模型的演绎能力生成数据并优化归纳推理，显著提升了模型从少量样本中学习通用规则的能力。方法总结：通过演绎生成数据，再用朴素贝叶斯方法优化模型对样本的利用，实现更高效的归纳推理。【归纳推理任务，包括语义归纳和符号归纳】【Instruction Induction和List Function】【执行分数，即模型生成的变换规则在测试样本上的正确率】【LLMs】

162. **Temporal Knowledge Question Answering via Abstract Reasoning Induction** [ACL2024] [[paper link](https://aclanthology.org/2024.acl-long.267.pdf)]

      提出 ARI 框架，让 LLM 从历史推理链中“归纳”出抽象方法，再把事实查询与策略决策解耦，提升了时间知识问答的准确率。方法：把问答拆成“归纳式策略选择 + 事实执行”两阶段循环，直到得出答案。【Temporal Knowledge Graph Question Answering（TKGQA）】【MULTITQ、CRONQUESTIONS】【Overall Accuracy】【ARI】

163. **Domain Adaptation for Subjective Induction Questions Answering on Products by Adversarial Disentangled Learning** [ACL2024] [[paper link](https://aclanthology.org/2024.acl-long.491.pdf)]

      提出领域自适应模型，通过解耦领域无关和特定知识，解决低资源领域主观问答的数据不均衡问题，生成融合事实与多视角观点的归纳性答案。方法总结：基于对抗解耦学习分离跨领域共享模式，结合对比学习和强化学习优化答案生成。【主观归纳问答（SunPQA）】【SupQA】【BLEU、ROUGE（评估生成质量），人工评估】【BART】

164. **Structural Optimization Ambiguity and Simplicity Bias in Unsupervised Neural Grammar Induction** [ACL2024] [[paper link](https://aclanthology.org/2024.findings-acl.898.pdf)]

      研究无监督神经语法归纳（UNGI）中的两个问题：结构优化模糊性（SOA）和结构简单性偏差（SSB）。这些导致模型随机选择语法结构（高方差）或生成过于简单的解析树（低表达能力），影响性能。通过预训练解析器筛选高质量解析树作为训练偏置，减少语法归纳中的模糊性和简单性偏差。【无监督语法归纳】【Penn Treebank (PTB)、Chinese Penn Treebank (CTB)、SPMRL多语言数据集】【未标记句子F1分数、负对数似然、规则利用率】【PCFG】

165. **Improving Word Usage Graphs with Edge Induction** [ACL2024] [[paper link](https://aclanthology.org/2024.lchange-1.9.pdf)]

      通过预测缺失的边来增强稀疏的词用法图（WUGs），从而更高效地发现词义，减少人工标注需求。其方法利用了图中已有的结构规律和上下文语义信息，通过归纳学习补全缺失的边。方法：结合图结构特征和预训练语言模型预测缺失的边，再用聚类算法分析增强后的图以发现词义。【词义聚类和词义变化检测】【DWUG_DE、resampled dataset】【加权平均Spearman相关系数、调整兰德指数ARI】【基于逻辑回归的边预测模型、聚类算法】

166. **Towards more complete solutions for Lexical Semantic Change: an extension to multiple time periods and diachronic word sense induction** [ACL2024] [[paper link](https://aclanthology.org/2024.lchange-1.10.pdf)]  
      扩展词汇语义变化研究，从简单的两时间段建模升级到多时间段的动态分析，通过归纳已有数据中的词义规律来预测和追踪语义演变。方法：提出多种动态聚类和评估策略，结合时间序列分析，从已有词义中归纳规律以建模多时间段的语义变化。【多时间段的词汇语义变化检测（LSC）和历时词义归纳】【LSC基准】【聚类质量（如对齐一致性）和语义变化显著性】【上下文嵌入模型、聚类】

167. **Towards an Onomasiological Study of Lexical Semantic Change through the Induction of Concepts** [ACL2024] [[paper link](https://aclanthology.org/2024.lchange-1.15.pdf)]

      词汇语义变化（LSC）研究主要从词到概念的视角出发，而忽略了概念到词的视角。后者能揭示词汇命名方式的演变。提出通过自动归纳概念来填补这一空白，方法是用XLM-R嵌入和层次聚类从词例中归纳概念簇，支持词义和命名演变的双重分析。【历时概念归纳与语义变化分析】【法语历时语料库Presto Corpus】【人工评估概念簇质量，JSD量化词义变化】【XLM-R（提取词例嵌入） + 层次聚类】

168. **The Role of Deductive and Inductive Reasoning in Large Language Models**  [ACL2025] [[paper link](https://aclanthology.org/2025.acl-long.820.pdf)]

      针对LLMs静态推理的局限性，提出了De-In-Ductive（DID）方法，通过动态结合归纳和演绎推理，利用Littlestone维度和信息熵评估任务复杂度，并逐步分解问题，引导模型从简单到复杂逐步解决任务。【逻辑推理、数学推理和时序推理】【AIW、MR-GSM8K、Holiday Puzzle】【准确率、计算成本】【GPT-3.5 Turbo、GPT-4o、Claude 3.5 Sonnet】

169. **Measuring What Matters: Evaluating Ensemble LLMs with Label Refinement in Inductive Coding**  [ACL2025] [[paper link](https://aclanthology.org/2025.findings-acl.563.pdf)]

      针对单一LLM在归纳编码中的不一致性，文章通过集成学习模拟“多专家共识”，结合标签精炼优化归纳结果。方法：用多个小型LLM生成候选编码，通过moderator机制和相似性合并实现精炼，最终以复合指标评估。【自动化归纳编码】【1,000个“文本-编码”对（600条社会科学研究数据 + 400条SemEval-2014评论数据），另用100条ChatGPT用户评论作为独立测试集】【复合评分（核心指标）、BERTScore、ROUGE、覆盖度、新颖性】【Llama3-8B、Falcon-7B、Mistral-7B、Vicuna-7B等】

170. **Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations**  [ACL2025] [[paper link](https://aclanthology.org/2025.findings-acl.1006.pdf)]

      针对LLM在噪声数据中归纳规则时的脆弱性，通过多样化假设和迭代修正来增强推理稳定性。提出Sample-steered Rule Refinement (SRR)，通过生成多样规则候选并基于执行反馈迭代优化。【Robust Rule Induction（从含噪声的输入-输出对中推断潜在规则）】【Arithmetic（非十进制加法）；Cryptography（替换密码）；List Functions（列表操作）】【整体正确率、噪声与无噪声条件下推理一致性】【SRR】

171. **Inductive Linguistic Reasoning with Large Language Models**  [ACL2025] [[paper link](https://aclanthology.org/2025.findings-acl.1171.pdf)]

      针对低资源语言缺乏标注，通过类比提示让模型从少量示例中归纳语言规则，并利用同语系语言的相似性生成辅助示例，增强跨语言推理能力。方法：首先生成同语系类比示例，再结合原始示例进行上下文学习。【语言学谜题】【model_ing和LINGOLY】【精确匹配、ChrF2和BLEU分数】【LLMs】

172. **Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models**  [ACL2025] [[paper link](https://aclanthology.org/2025.knowllm-1.10.pdf)]

      针对LLMs推理缺乏逻辑严谨性的问题，通过模拟人类多模态推理（溯因、演绎、归纳），结合贝叶斯验证，提升复杂任务的可靠性和可解释性。方法：三代理（溯因/演绎/归纳）生成结构化推理图，经NLI验证后选择最优解。【符号推理和数值推理】【WebOfLies、MultiArith】【答案准确率】【本文提出的Theorem-of-Thought (ToTh)框架】

173. **Inductive Learning on Heterogeneous Graphs Enhanced by LLMs for Software Mention Detection** [ACL2025] [[paper link](https://aclanthology.org/2025.sdp-1.16.pdf)]

      针对多语言NER/RE任务中结构化知识融合的难题，通过异构图谱归纳学习与LLM验证协同，提升模型泛化性。方法：异构图谱编码+归纳式GraphSAGE分类+LLM逻辑验证。【多语言命名实体识别（NER）和关系抽取（RE），聚焦软件提及检测】【SOMD 2025竞赛数据集】【Macro F1、Precision、Recall】【基础模型：GraphSAGE；LLM验证层：DeepSeek v3】

174. **Learn to Memorize: Scalable Continual Learning in Semiparametric  Models with Mixture-of-Neighbors Induction Memory**  [ACL2025] [[paper link](https://aclanthology.org/2025.acl-long.1385.pdf)] 

      针对半参数化模型记忆效率低的问题，提出MoNIM，通过结合注意力头的归纳能力和前馈网络的记忆机制，实现动态、可扩展的持续学习。方法：将非参数化记忆改造为可学习的类前馈网络旁路层。【语言建模和下游任务】【News Crawl-20H1、WikiEvent-20H1、ACL论文数据集】【困惑度、记忆率、准确率】【MoNIM（基于kNN-LM改进的可学习记忆模块】

175. **Programming by Examples Meets Historical Linguistics: A Large Language Model Based Approach to Sound Law Induction** [ACL2025] [[paper link](https://aclanthology.org/2025.acl-long.1432.pdf)] 

      提出利用LLMs的“示例编程”（Programming by Examples, PBE）能力来自动生成音变规则，并通过研究“结构”与“实质”在数据分布中的平衡，优化模型的归纳性能。【音变规律归纳】【包括Proto-Polynesian（Pol）和Proto-Tangkhulic（Ptk）等低资源语言的真实数据，以及生成的合成数据】【编辑距离奖励、通过率】【LLMs】

176. **Semantic Frame Induction from a Real-World Corpus** [ACL2025] [[paper link](https://aclanthology.org/2025.acl-srw.72.pdf)] 

      语义框架归纳依赖FrameNet等人工标注资源，在真实语言覆盖上存在局限，通过深度度量学习和真实语料（C4）的结合，验证了语义框架归纳方法的泛化能力。方法：基于BERT嵌入和深度度量学习的聚类方法，通过掩码技术和多阶段聚类优化框架归纳效果。【语义框架归纳】【Colossal Clean Crawled Corpus（C4）作为真实语料，FrameNet 1.7作为评估基准】【B-cubed F1（BCF）、Purity与Inverse Purity的调和平均】【基于BERT的上下文嵌入模型】

177. **Improve Rule Retrieval and Reasoning with Self-Induction and Relevance  ReEstimate** [ACL2025] [[paper link](https://aclanthology.org/2025.findings-acl.286.pdf)] 

      针对规则检索中查询与规则的语义不对齐问题，利用LLM的归纳能力从查询中抽象出潜在规则（Self-Induction Augmented Retrieval ，SIAR），并通过相关性重估优化检索结果，从而提升推理性能。【规则检索与推理】【Clutrr、ULogic、CAIL2018】【检索性能（Recall@1/5/10）、推理性能（Match）】【SIAR】

178. **IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction** [ACL2025] [[paper link](https://aclanthology.org/2025.findings-acl.698.pdf)]

      针对LLM在交互式规则学习中的不足，文章提出IDEA框架，通过模拟人类“假设生成（溯因）-验证（演绎）-修正（归纳）”的循环，提升模型动态学习能力。【在交互式环境中学习隐藏规则并解决问题】【RULEARN基准，包含三种手动设计的谜题类型】【任务成功率、重复动作次数、有效归纳率】【IDEA】

179. **In the LLMera, Word Sense Induction remains unsolved** [ACL2025] [[paper link](https://aclanthology.org/2025.findings-acl.882.pdf)] 

      针对词义归纳（WSI）评估中多义词分布不自然的问题，提出基于自然分布的评估框架，并通过数据增强（如LLM生成、语料库和词典）和半监督学习（如Wiktionary）提升词义归纳的性能。【词义归纳】【SemCor-WSI（基于SemCor构建）、SemEval 2010 Task 14、SemEval 2013 Task 13】【F-B³、NMI、V-measure、Paired F-score等】【基于BERT和Wiktionary的半监督模型】

180. **Unsupervised Induction of Semantic Roles within a Reconstruction-Error Minimization Framework** [NAACL2015] [[paper link](https://aclanthology.org/N15-1001.pdf)] 

      解决无监督语义角色标注（SRL）中强独立性假设和特征过于简单的问题。通过结合丰富的特征和重建误差最小化框架，无需依赖人工标注数据或语言特定知识，实现更准确的语义角色归纳。方法是结合一个预测语义角色的编码模型和一个基于角色预测论元的重建模型，通过联合优化两者的重建误差来学习。【无监督语义角色归纳】【PropBank、SALSA】【纯度、共现性及其调和平均数】【编码模型、重建模型】

181. **Unsupervised POS Induction with Word Embeddings** [NAACL2015] [[paper link](https://aclanthology.org/N15-1144.pdf)] 

      旨在通过词嵌入提升无监督词性标注的准确性，验证词嵌入如何帮助模型从归纳出语法类别（名词、动词）。方法是用高斯分布替换传统多类别分布，生成词嵌入而非词类型，结合HMM或CRF自编码器框架进行无监督学习。【无监督词性标注】【CoNLL-X、CoNLL 2007、Ukwabelana语料库】【V-measure（基于条件熵的外部聚类评估指标）】【高斯HMM、高斯CRF自编码器】

182. **Unsupervised Morphology Induction Using Word Embeddings** [NAACL2015] [[paper link](https://aclanthology.org/N15-1186.pdf)] 

      通过词嵌入自动归纳词形变化规则（如“加-ed变过去式”），无需人工干预，直接从数据中总结形态规律。方法是利用词嵌入空间的向量方向表示形态变换，构建词汇化规则图，并通过语义一致性筛选有效规则。
     【无监督形态归纳】【Wikipedia、WMT-2013、Arabic GigaWor、新闻语料】【Spearman（词相似度任务中的相关性分数）】【SkipGram】

183. **Dynamic Feature Induction: The Last Gist to the State-of-the-Art** [NAACL2016] [[paper link](https://aclanthology.org/N16-1031.pdf)] 

      提出动态特征归纳方法，通过自动从数据中归纳高维特征组合，减少对手工特征工程的依赖，提升模型性能。方法是在模型预测错误时，选择强区分性特征组合并动态扩展特征空间，结合正则化和结构化学习优化特征选择。【词性标注和命名实体识别】【词性标注：Penn Treebank III；命名实体识别：CoNLL’03英文数据集】【准确率、F1值】【动态特征归纳模型（结合RDA和LOLS）】

184. **Joint Learning Templates and Slots for Event Schema Induction** [NAACL2016] [[paper link](https://aclanthology.org/N16-1049.pdf)]  

      提出一种无监督方法，从文本中归纳事件模板和槽位（如“爆炸事件”包含哪些角色），通过联合建模实体关系和句子约束，解决传统方法误差传播问题。方法是用归一化割聚类实体，结合共现、语义相似性和句子内约束，同时学习模板和槽位。【自动事件模式归纳（AESI）】【MUC-4】【Precision、Recall、F1】【基于归一化割的联合实体驱动模型，加入句子约束（SC）】

185. **Deconfounded Lexicon Induction for Interpretable Social Science** [NAACL2018] [[paper link](https://aclanthology.org/N18-1146.pdf)]  

      提出解混淆词典归纳任务，从文本中自动提取既预测目标（如销售额）又不受混淆变量（如价格）干扰的词汇，提升社会现象分析的因果解释性。方法是通过深度残差化或对抗训练分离文本与混淆变量的影响，从中归纳高解释性词汇。【解混淆词典归纳】【CFPB消费者投诉、斯坦福大学课程描述、Rakuten电商商品描述】【预测性能提升、词典与混淆变量的相关性】【DR（深度残差化）+BOW（词频）/ATTN（注意力机制）、A（对抗选择器）+BOW/ATTN】

186. **Word Emotion Induction for Multiple Languages as a Deep Multi-Task Learning Problem** [NAACL2018] [[paper link](https://aclanthology.org/N18-1173.pdf)]  

      提出多任务学习框架，从小规模标注数据中归纳词级情感规律（如“sunshine”高Valence），联合预测VAD三维度，解决数据稀缺问题并提升跨语言泛化能力。方法是通过共享隐藏层的神经网络联合学习情感维度，利用词嵌入和正则化避免过拟合。【词级情感预测】【9种语言的11个情感词典】【Pearson相关系数衡量预测值与人工标注的一致性】【MTLNN（多任务学习神经网络）】

187. **Unsupervised Induction of Linguistic Categories with Records of Reading, Speaking, and Writing** [NAACL2018] [[paper link](https://aclanthology.org/N18-1184.pdf)] 

      利用人类阅读、说话、打字的多模态数据（如眼动、键盘停顿），通过表征融合增强无监督词性和句法分析，解决低资源语言标注稀缺问题，验证小词典下的有效性。方法：通过CCA/SVD-IS或拼接融合多模态特征，结合词典约束的二阶HMM模型归纳语法类别。【无监督词性标注和句法组块归纳】【眼动：Dundee Corpus、GECO Corpus；语音：CHILDES（Brent/Providence）；键盘日志：Killourhy & Maxion；文本：Ontonotes 5.0、Penn Treebank】【标注准确率】【多模态二阶隐马尔可夫模型】

188. **Verb Alternations and Their Impact on Frame Induction** [NAACL2018] [[paper link](https://aclanthology.org/N18-4003.pdf)] 

      旨在解决动词交替现象在自动构建语义框架时的挑战，提出了一种通过嵌入框架关联动词不同用法的方法，体现了从具体语料中归纳语义关系的思路。方法：提出了一种半监督的框架归纳方法，将动词的核心语义嵌入到更高层框架中以表示交替关系。【自动构建语义框架资源】【FrameNet和PropBank】【分布语义方法】【基于嵌入框架的半监督方法，结合预定义的交替规则和分布语义分析】

189. **Efficient Graph-based Word Sense Induction by Distributional Inclusion Vector Embeddings** [NAACL2018] [[paper link](https://aclanthology.org/W18-1706.pdf)] 

      旨在高效地从无标注文本中归纳多义词的不同词义，通过全局主题分组和局部图聚类，避免了传统方法的计算瓶颈。方法：使用DIVE（分布包含向量嵌入）将词汇分组为基础索引，构建目标词相关的图并聚类，最后转换为词义嵌入。【无监督词义归纳（WSI）】【WaCkypedia、TWSI、SemEval-2013 task 13】【Precision@1、F1/Tau】【基于DIVE的图聚类方法】

190. **Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing** [NAACL2019] [[paper link](https://aclanthology.org/N19-1084.pdf)]

      细粒度实体类型标注中，传统方法依赖预定义的类型层次结构，提出通过数据驱动的标签关系建模，捕捉类型间的隐含相关性。这种数据驱动的归纳方式使模型能够泛化到未见过的自由形式类型。【细粒度实体类型标注，预测句子中实体提及的语义类型】【Ultra-Fine、OntoNotes】【Ultra-Fine：MRR、精确率、召回率、F1值；OntoNotes：准确率、F1值】【Label_GCN】

191. **Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction** [NAACL2019] [[paper link](https://aclanthology.org/N19-1113.pdf)]

      旨在通过互信息最大化从无标注文本中归纳词性标签，提出变分下界目标，以简单架构实现高效无监督学习。方法是通过变分近似优化互信息下界，结合上下文窗口和词形LSTM编码，直接学习标签与上下文的统计依赖关系。【无监督词性标注】【45-tag Penn WSJ、12-tag Universal Treebank、CoNLL-X/2007】【多对一准确率、V-measure】【基于变分互信息下界的神经网络模型】

192. **Single Document Summarization as Tree Induction** [NAACL2019] [[paper link](https://aclanthology.org/N19-1173.pdf)]

      提出将摘要任务重新定义为树归纳问题，通过Sumo模型从文档中自动学习多根依赖树结构，同时生成高质量摘要。方法：结合结构化注意力和迭代优化算法，逐步细化文档的树结构，最终生成以摘要句为根、支撑句为子树的依赖树。【单文档抽取式摘要】【CNN/DailyMail、NYT】【ROUGE-1/2/L、人工评估】【Sumo（基于结构化注意力和迭代优化的多根依赖树模型）】

193. **Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders** [NAACL2019] [[paper link](https://aclanthology.org/N19-1116.pdf)]

      提出了一种无监督方法DIORA，通过归纳学习从文本中自动发现句法结构，解决了监督方法依赖标注数据的问题。方法：DIORA结合内部-外部动态规划和自编码目标，无监督地学习句法树和成分表示。【无监督句法分析】【WSJ、MultiNLI、WSJ-10、WSJ-40】【F1分数】【DIORA（Deep Inside-Outside Recursive Autoencoders）】

194. **Cross-lingual CCG Induction** [NAACL2019] [[paper link](https://aclanthology.org/N19-1160.pdf)]

      提出跨语言方法，利用英语解析器和平行语料库，自动为目标语言归纳CCG语法和解析模型，解决了无监督学习效果差的问题。方法：通过词对齐和推导投影，将源语言的CCG结构迁移到目标语言，训练目标语言解析器。【跨语言CCG语法归纳和解析】【PASCAL基准数据集、Tatoeba平行语料库】【无标记依存准确率（UAS）】【跨语言CCG归纳模型（基于词对齐和推导投影）】

195. **Inductive Topic Variational Graph Auto-Encoder for Text Classification** [NAACL2021] [[paper link](https://aclanthology.org/2021.naacl-main.333.pdf)]

      基于GCN文本分类需预定义包含所有文档的全局图，无法处理训练时未见的文档（Inductive Learning）。提出为每个文档单独构建文档-词关系图，支持新文档的归纳推理。
     任务一：【监督/半监督文本分类】【20NewsGroups、Ohsumed（医学摘要）、R8、R52（路透社新闻）、MR（电影评论）】【Micro Precision/Recall/F1、Accuracy】【GCN】
     任务二：【无监督主题建模：学习文档和词的主题分布】【20NewsGroups、Ohsumed、R8、R52、MR】【主题连贯性（Topic Coherence）、困惑度（Perplexity）】【GCN】

196. **Video-aided Unsupervised Grammar Induction** [NAACL2021] [[paper link](https://aclanthology.org/2021.naacl-main.119.pdf)]

      通过利用视频中的多模态动态信息（如动作、音频等），改进无监督语法归纳任务，让模型能自动从数据中学习更准确的句法结构，尤其是动词短语这类动态内容。方法：提出MMC-PCFG模型，用多模态Transformer整合视频特征，结合文本匹配损失优化语法规则归纳。【视频辅助的无监督语法归纳】【DiDeMo、YouCook2、MSRVTT】【Corpus-level F1 (C-F1) 和 Sentence-level F1 (S-F1)】【MMC-PCFG（多模态复合概率上下文无关文法模型）】

197. **Production vs Perception: The Role of Individuality in Usage-Based Grammar Induction** [NAACL2021] [[paper link](https://aclanthology.org/2021.cmcl-1.19.pdf)]

      通过对比个体生产与群体感知的语法归纳，发现个体数据会生成更多独特构式，揭示语言学习中个体差异的重要性。方法：基于计算构式语法，从多语域语料中提取语法和词汇表征，并通过增长曲线和相似性分析量化差异。【对比生产型和感知型语法归纳的差异】【学术文章（AC-IND、AC-AGG）、书籍（PG-IND、PG-AGG）、推特（TW-AGG）、网页（CC-AGG）、维基百科（WI-AGG）、新闻（NW-AGG）】【语法和词汇的增长曲线参数、Jaccard距离】【基于计算构式语法（Computational Construction Grammar）的语法归纳模型】

198. **Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs** [NAACL2022] [[paper link](https://aclanthology.org/2022.naacl-main.423.pdf)] 

      利用Transformer注意力机制中的归纳偏置，设计一种无需显式监督信号即可分离文本中的句法和语义信息。用两个潜在变量分别控制键和值，改进注意力，将键和值的生成解耦，迫使模型分别学习句法和语义表示。【句法与语义解纠缠、句法与语义迁移】【ParaNMT】【语义相似性：METEOR分数、基于ParaBART的余弦相似度；句法相似性：句法树编辑距离、模板匹配准确率】【提出的QKVAE：基于Transformer的VAE】

199. **StATIK: Structure and Text for Inductive Knowledge Graph Completion** [NAACL2022] [[paper link](https://aclanthology.org/2022.findings-naacl.46.pdf)]

      提出一种归纳的混合模型，结合图结构和文本描述，以完成知识图谱补全。提出StATIK模型，通过BERT提取实体和关系的文本描述，再聚合局部图结构信息，通过邻居采样和消息传递生成实体表示。【归纳式知识图谱补全】【WN18RR、FB15k-237、Wikidata-5M】【MRR、Hits@1/3/10】【StATIK】

200. **Unsupervised Slot Schema Induction for Task-oriented Dialog** [NAACL2022] [[paper link](https://aclanthology.org/2022.naacl-main.86.pdf)]

      通过无监督方法从对话数据中自动归纳槽位模式，避免了人工设计的成本，让系统能快速适应新领域。方法：结合语言模型的注意力机制和PCFG的结构约束提取候选短语，再通过多步聚类生成槽位模式。【无监督槽位模式归纳】【MultiWOZ、SGD】【精确率、召回率、F1值】【基于LM和PCFG的无监督槽位模式归纳模型】

201. **Event Schema Induction with Double Graph Autoencoders** [NAACL2022] [[paper link](https://aclanthology.org/2022.naacl-main.147.pdf)]

      提出通过捕捉事件图中的全局依赖关系，从具体事件实例中归纳出高质量且逻辑合理的事件模式。方法：使用双图自动编码器，分阶段学习事件骨架和实体关系，确保生成的模式具有全局一致性。【事件模式归纳】【General-IED、Car-IED、Suicide-IED】【事件类型匹配F1、事件序列匹配F1、节点/边类型分布的KL散度、最大公共子图】【Double Graph Autoencoders (DoubleGAE)】

202. **Minimally-Supervised Relation Induction from Pre-trained Language Model** [NAACL2022] [[paper link](https://aclanthology.org/2022.findings-naacl.135.pdf)]

      提出了一种极小监督下的关系归纳方法，通过BERT生成高质量模板和伪句子，从少量种子实体对中归纳出通用的语义关系。方法：结合BERT的掩码预测、注意力机制和积分梯度，迭代优化模板并训练分类器，实现高精度的关系归纳。【极小监督关系归纳】【Google Analogy Test Set (GATS)、Bigger Analogy Test Set (BATS)、DiffVec】【精确率、召回率、F1分数】【IST（Iteratively-Selected Templates from PLM）】

203. **Jointly Learning Guidance Induction and Faithful Summary Generation via Conditional Variational Autoencoders** [NAACL2022] [[paper link](https://aclanthology.org/2022.findings-naacl.180.pdf)] 

      提出了一种联合学习关键短语归纳和摘要生成的方法，通过归纳源文档的关键信息生成更忠实的事实性摘要。方法：基于条件变分自编码器（CVAE），联合训练关键短语预测和摘要生成，避免依赖外部工具。【抽象摘要】【XSUM、CNNDM】【ROUGE（R1、R2、RL）、BERTScore、事实一致性指标】【GISG（基于BART的CVAE框架）】

204. **Distilling Hypernymy Relations from Language Models: On the Effectiveness of Zero-Shot Taxonomy Induction** [NAACL2022] [[paper link](https://aclanthology.org/2022.starsem-1.13.pdf)]

      用语言模型“猜”出术语的上下位关系（比如“猫是动物的一种”），无需额外训练，直接通过提示和评分归纳出分类法。方法：用BERT/GPT-2等模型给提示句打分或填词，选最靠谱的答案当上下位关系。【分类法学习】【SemEval的TExEval-1和TExEval-2】【精确度、召回率、F1值】【基于LLMs的零样本方法】

205. **Improved Induction of Narrative Chains via Cross-Document Relations** [NAACL2022] [[paper link](https://aclanthology.org/2022.starsem-1.18.pdf)]

      通过案例间的引用关系，让法律叙事链（比如“犯罪-判刑”事件序列）的自动归纳更准确。方法：改进了PMI算法，加入跨文档统计（如被引用案例中的事件）。【叙事链归纳】【美国联邦法院案例】【Recall@1/5/50、MRR】【四种跨文档PMI变体】


206. **Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning**
     [NAACL2024] [[paper link](https://aclanthology.org/2024.findings-naacl.75.pdf)] 

      基于预训练语言模型的时序知识图谱推理，需要昂贵的训练，难以适应少样本归纳式设置。提出用对比学习拉近相关历史对的表示距离，在冻结的PLM中引入可训练的时间前缀令牌，通过prefix-tuning注入时间信息。【时序知识图谱推理（TKGR），包括转导式（预测已知实体间缺失事实）和少样本归纳式（预测新实体间关系）两种设置。】【转导式：ICEWS14、ICEWS18、ICEWS05-15、ICEWS14*、WIKI、YAGO。少样本归纳式：ICEWS14-OOG、ICEWS18-OOG、ICEWS0515-OOG。】【MRR、Hits@N】【ChapTER】

207. **A(More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection**
     [NAACL2024] [[paper link](https://aclanthology.org/2024.findings-naacl.30.pdf)] 

      恶意内容检测模型实际应用中效果不佳，提出了一种inductive的评估设置，以测试模型在动态社交图中的泛化能力。提出Few-shot子图采样，通过局部探索生成支持子图，模拟真实场景中有限的标注数据和局部上下文。【恶意内容检测】【用于预训练：GossipCop；用于测试任务：CoAID，TwitterHateSpeech】【F1-score、Matthews Correlation Coefficient (MCC)（综合评估分类性能）、Area Under the Precision-Recall curve (AUPR)（多阈值评估）】【提出一种基于梯度的元学习方法】

208. **Re-evaluating the Need for Multimodal Signals in Unsupervised Grammar Induction** [NAACL2024] [[paper link](https://aclanthology.org/2024.findings-naacl.70.pdf)]


      验证多模态信号（如图像、视频）是否对无监督语法归纳是必要的。先前研究认为多模态输入能提升性能，但仅与弱文本基线对比。本文质疑这一结论，提出在大量文本数据下，纯文本方法可能足够。方法是用预训练语言模型提取文本特征，结合概率文法模型（C-PCFG）归纳句法结构。【无监督语法归纳】【MSCOCO（图像描述）、DiDeMo、YouCook2、MSRVTT（视频描述）、HowTo100M（大规模视频文本）】【Corpus-F1、Sentence-F1、困惑度、平均分支因子】【LC-PCFG（纯文本，结合LLM嵌入）】

209. **How Lexicalis Bilingual Lexicon Induction?** [NAACL2024] [[paper link](https://aclanthology.org/2024.findings-naacl.273.pdf)]

      现有双语词典归纳方法忽略了词汇特征（如词频、词性）的作用，提出通过这些简单但有效的特征提升性能。方法是用预训练模型（XLM-R）初筛候选词，再用XGBoost结合词频、词性等特征重新排序，选出最佳翻译。【双语词典归纳】【XLING】【准确率】【LETOR（XLM-RoBERTa + XGBoost，结合词频、词性特征）】

210. **Improving Word Sense Induction through Adversarial Forgetting of Morphosyntactic Information** [NAACL2024] [[paper link](https://aclanthology.org/2024.starsem-1.19.pdf)]

      发现BERT的词表示中包含过多与词义无关的噪声信息（如形态和句法特征），这些信息干扰了无监督的词义归纳（WSI）。为了解决这个问题，作者提出了一种对抗训练方法，通过“遗忘门”移除这些噪声，生成更干净的词表示，从而提升聚类效果。【无监督词义归纳】【训练遗忘门：Brown Corpus；WSI评估：SemCor】【聚类性能：调整兰德指数、纯度、逆纯度、V-Measure；信息移除效果：词预测的困惑度和特征探测的准确率】【基于对抗训练的遗忘门（forget-gate），生成形态不变和句法不变表示】

211. **Generating Diverse Hypotheses for Inductive Reasoning** [NAACL2025] [[paper link](https://aclanthology.org/2025.naacl-long.429.pdf)]

      LLMs在归纳推理任务中生成多个假设时，经常产生语义冗余。提出Mixture of Concepts (MoC)的方法模拟人类归纳推理，先生成一系列语义非冗余的概念列表，然后基于每个概念生成对应的假设。【归纳推理】【List Functions、MiniARC、MBPP+、Playgol-str】【Accuracy、假设的多样性】【LLMs】

212. **ARISE: Iterative Rule Induction and Synthetic Data Generation for Text Classification** [NAACL2025] [[paper link](https://aclanthology.org/2025.findings-naacl.359.pdf)]

      通过归纳泛化从数据中提取规则，并结合合成数据生成，提升文本分类任务的性能，解决了合成数据方差不足的问题。方法：迭代生成规则和合成数据，通过规则过滤和自举优化模型。【文本分类】【Discovery、CDR、ANLI（全样本）；FewMany Benchmark（少样本）；MASSIVE（多语言）】【准确率】【ARISE（结合规则归纳和合成数据生成的框架）】

213. **Reordering Grammar Induction** [EMNLP2015] [[paper link](https://aclanthology.org/D15-1005.pdf)]

      通过归纳学习从平行语料中自动提取重排序规则和句法结构，提出了一种单步无监督的预排序方法，提升了机器翻译的调序能力。方法：基于排列树（PETs）初始化PCFG，通过非终端符号分裂和MBR解码优化重排序结果。【机器翻译中的预排序】【NTCIR-8 Patent Translation (PATMT) Task】【内部评估：Kendall τ（衡量重排序预测准确性）；外部评估：BLEU、METEOR、TER、RIBES（衡量翻译质量）】【Reordering PCFG】

214. **Visual Bilingual Lexicon Induction with Transferred ConvNet Features** [EMNLP2015] [[paper link](https://aclanthology.org/D15-1015.pdf)]

      提出用CNN提取图像特征，通过视觉相似性自动归纳双语词典，解决了平行语料稀缺的问题，并发现视觉方法在部分语言对上优于传统语言模型。方法：用预训练的CNN提取图像特征，计算跨语言单词的视觉相似度来匹配翻译对。【双语词典归纳】【Bergsma500和Vulic1000】【MRR、Precision at N】【CNN】

215. **Script Induction as Language Modeling** [EMNLP2015] [[paper link](https://aclanthology.org/D15-1195.pdf)]

      提出用判别式语言模型（LBL）解决叙事填空任务，发现其显著优于传统统计方法，表明脚本归纳可能更适合语言建模框架。方法：用Log-Bilinear模型建模事件序列的上下文关系，通过概率预测缺失事件。【叙事填空测试】【Concretely Annotated Gigaword（纽约时报语料）】【平均排名、MRR、Recall@10、Recall@50】【Log-Bilinear语言模型】

216. **Using Left-corner Parsing to Encode Universal Structural Constraints in Grammar Induction** [EMNLP2016] [[paper link](https://aclanthology.org/D16-1004.pdf)]

      提出通过左角解析量化中心嵌套程度，限制语法归纳模型的搜索空间，从而更有效地从无标注数据中归纳出合理的依存结构，验证了语言普遍性假设对无监督学习的价值。方法：用左角解析的栈深度约束模型搜索空间，排除高中心嵌套的语法树，优化依存语法模型（DMV）。【无监督依存语法归纳】【Universal Dependencies】【无标记依存正确率、未标记括号结构的精确率/召回率/F1】【基于左角解析的DMV，结合栈深度约束】

217. **Relation Schema Induction using Tensor Factorization with Side Information** [EMNLP2016] [[paper link](https://aclanthology.org/D16-1040.pdf)]

      提出SICTF模型，通过张量分解从文本中自动归纳关系模式，减少人工干预，验证了侧信息和非负约束对提升归纳效果的重要性。方法：联合分解OpenIE三元组张量和侧信息矩阵，学习非负潜在因子以表示关系模式。【关系模式归纳】【MEDLINE（医学文献）和StackOverflow（技术问答）】【人工标注的诱导模式准确率（RSI accuracy）】【SICTF（非负耦合张量分解模型）】

218. **NITE: ANeural Inductive Teaching Framework for Domain-Specific NER** [EMNLP2017] [[paper link](https://aclanthology.org/D17-1280.pdf)]

      领域特定命名实体识别（DNER）数据不足，提出NITE框架（Neural Inductive Teaching Framework），通过教师-学生模型和多示例学习实现知识迁移，通过迁移现有模型的知识，提升DNER中的性能。【疾病命名实体识别】【NCBI疾病语料库】【精确率、召回率、F1值】【教师模型：DNorm（基于特征工程的领域SOTA模型）；学生网络：CLC（CNN-BiLSTM-CRF）】

219. **Zero-Shot Activity Recognition with Verb Attribute Induction** [EMNLP2017] [[paper link](https://aclanthology.org/D17-1099.pdf)]

      通过从词典和词嵌入中归纳动词的通用属性（如“喝酒”涉及手臂和短时间），利用这些属性帮助计算机识别从未见过的动作（如“豪饮”）。方法：用双向GRU分析词典定义，结合词嵌入预测动词属性，再用于零样本分类。【零样本活动识别】【动作属性标注数据集；imSitu（图像数据集）；辅助数据：Wordnik词典、MPII电影描述】【属性预测准确率（micro/macro-averaged）；零样本分类的Top-1和Top-5准确率】【BGRU+GloVe联合编码词典和词嵌入，预测属性后用于零样本分类】

220. **Dependency Grammar Induction with Neural Lexicalization and Big Training Data** [EMNLP2017] [[paper link](https://aclanthology.org/D17-1176.pdf)]

      通过结合大数据和神经网络，让计算机从海量句子中归纳出词汇化的依存语法规则（比如“动词常支配名词”），即使某些词罕见也能准确预测句法结构。方法：用神经网络扩展传统语法模型，通过分批训练和智能初始化提升学习效率。【无监督依存语法归纳】【英语：WSJ10、BLLIP语料库；中文：CTB 6.0（中文树库）】【有向依存准确率】【L-NDMV（词汇化神经DMV）】

221. **Earth Mover’s Distance Minimization for Unsupervised Bilingual Lexicon Induction** [EMNLP2017] [[paper link](https://aclanthology.org/D17-1207.pdf)]

      研究在不依赖跨语言监督数据（如平行语料或种子词典）的情况下，实现无监督的双语词典归纳。传统方法需要跨语言对齐信息，而本文提出仅利用单语数据即可建立语言间的词向量映射。用WGAN和EMDOT交替优化，最小化双语词向量分布的推土机距离，实现无监督词典归纳。【无监督双语词典归纳】【单语词向量训练数据：Wikipedia可比语料；评测词典：LDC汉语-英语词典、Open Multilingual WordNet等】【F1分数（支持多翻译对匹配）】【WGAN + EMDOT（无监督）】

222. **Knowledge Distillation for Bilingual Dictionary Induction** [EMNLP2017] [[paper link](https://aclanthology.org/D17-1264.pdf)]

      针对双语词典归纳任务中低资源语言因种子词典不足导致映射函数性能低的问题，提出利用高资源语言的翻译路径作为“教师”，通过知识蒸馏提升低资源语言的映射函数性能，减少对大规模种子词典的依赖。【无监督/低资源双语词典归纳】【训练数据：多语言Wikipedia词向量、Google Translate生成的种子词典；测试数据：人工标注词典】【Top-1/5/10准确率】【知识蒸馏模型（THIS）】

223. **Cross-Lingual Induction and Transfer of Verb Classes  Based on Word Vector Space Specialisation** [EMNLP2017] [[paper link](https://aclanthology.org/D17-1270.pdf)]

      旨在利用英语VerbNet的丰富资源，通过归纳跨语言的动词分类模式，自动为资源稀缺的语言构建VerbNet风格的分类，减少对目标语言特征工程的依赖。方法：通过跨语言词向量空间专业化技术，将英语VerbNet的知识转移到目标语言，并用聚类算法完成分类。【跨语言动词分类】【英语：Polyglot Wikipedia；目标语言：法语（frWaC）等；外部资源：BabelNet、PanLex（用于跨语言约束）】【F-1分数】【初始词向量：SGNS、专业化模型：基于ParaGRAM框架的Attract-Repel方法、聚类算法：MINCut谱聚类】

224. **Cross-Lingual Word Representations: Induction and Evaluation** [EMNLP2017] [[paper link](https://aclanthology.org/D17-3007.pdf)]

      旨在介绍如何通过归纳跨语言数据中的模式，学习通用的词向量表示，支持多语言NLP任务，尤其是资源稀缺的语言。方法：总结了从多种数据源（如平行语料、词典等）诱导跨语言词向量的技术，并讨论了其评估和应用。【跨语言词表示学习及其评估与应用】【平行语料（如翻译对）、可比语料、非对齐单语数据、词典（如BabelNet）、图像、眼动数据】【包括内在评估（如词相似度、类比任务）和外在评估（如下游NLP任务性能）】【基于词典对齐的模型、基于句子/文档对齐的模型、结合单语和多语数据的模型】

225. **A Discriminative Latent-Variable Model for Bilingual Lexicon Induction** [EMNLP2018] [[paper link](https://aclanthology.org/D18-1042.pdf)]

      通过归纳单语词嵌入中的跨语言模式，结合二分图匹配先验，自动构建高精度双语词典，尤其适用于资源稀缺语言。方法：提出判别式隐变量模型，用Viterbi EM算法优化词嵌入对齐，强制1:1匹配以解决中心词问题。【双语词典诱导】【标准语言对：English–Italian（Dinu et al., 2015）低资源语言对：English–{Turkish, Bengali, Hindi}、Estonian–Finnish（使用fastText词向量和Wikipedia语料）】【Precision@1、Spearman相关系数】【基于二分图匹配先验的隐变量模型】

226. **Depth-bounding is effective: Improvements and evaluation of unsupervised PCFG induction** [EMNLP2018] [[paper link](https://aclanthology.org/D18-1292.pdf)]

      无监督PCFG诱导方法在搜索空间过大时容易陷入局部最优解，而递归深度限制被认为可以模拟人类认知记忆限制，提升文法诱导的准确性。本文验证深度限制的有效性，并开发一种高效的基于图表的贝叶斯PCFG诱导模型。【无监督句法分析】【英语：Penn Treebank；中文：Chinese Treebank 5.0；德语：NEGRA 2.0】【无标记PARSEVAL】【深度受限的贝叶斯PCFG诱导模型】

227. **Word Sense Induction with Neural biLM and Symmetric Patterns** [EMNLP2018] [[paper link](https://aclanthology.org/D18-1523.pdf)]

      基于n-gram语言模型的词义归纳（WSI）无法充分利用上下文和目标词本身的信息。神经双向语言模型（biLM）如何有效结合上下文和目标词信息仍需探索。提出用双向语言模型生成替代词，加上动态对称模式提升信息利用，最后聚类归纳出词义。【词义归纳】【SemEval 2013 Task 13数据集】【Fuzzy Normalized Mutual Information和 Fuzzy B-Cubed ，以及几何平均】【基于ELMo biLM和动态对称模式的WSI模型】

228. **Grammar Induction with Neural Language Models: AnUnusual Replication** [EMNLP2018] [[paper link](https://aclanthology.org/D18-1544.pdf)]

      探索神经语言模型PRPN是否能在无监督条件下从语言建模任务中归纳出有意义的句法结构，实验证明PRPN确实能学出合理的语法树。方法：用PRPN模型在语言建模任务上训练，通过F1分数和困惑度评估其语法归纳能力。【无监督语法归纳】【WSJ（Penn Treebank）、WSJ10（短句子集）、AliNLI（SNLI + MultiNLI合并）】【语言建模困惑度、无标注解析F1分数】【PRPN】

229. **Rule induction for global explanation of trained models** [EMNLP2018] [[paper link](https://aclanthology.org/W18-5411.pdf)]

      提出了一种归纳方法（规则归纳），通过分析模型的行为数据生成易懂的规则，从而全局解释模型的预测逻辑。方法：通过梯度加权特征并离散化，再用规则归纳算法（RIPPER-k）提取if-then-else规则。【4类文本分类】【20 newsgroups数据集】【宏平均F1分数】【规则归纳的解释模型（使用RIPPER-k算法）】

230. **Interpretable Structure Induction Via Sparse Attention** [EMNLP2018] [[paper link](https://aclanthology.org/W18-5450.pdf)]

      为了解决神经网络注意力机制难以解释的问题，文章提出通过归纳学习稀疏和结构化的注意力模式（如删减无关词、聚类相关词），使模型决策更透明。方法：用稀疏投影、语言学正则化和组合优化（如SparseMAP）改造注意力机制，生成简洁且可解释的权重分布。【通用NLP任务】【SNLI】【注意力权重的稀疏性和结构化程度】【注意力机制】

231. **Unsupervised Bilingual Lexicon Induction via Latent Variable Models** [EMNLP2018] [[paper link](https://aclanthology.org/D18-1062.pdf)]

      提出一种无监督方法，通过归纳学习跨语言词向量的潜在语义结构，仅用单语数据就能构建高质量双语词典。方法：用变分自编码器映射词向量到共享潜在空间，并通过对抗训练对齐分布。
     【无监督双语词典归纳】【LDC2002L27、MUSE】【Top-1准确率】【基于潜在变量和对抗训练的跨语言词对齐模型】

232. **Instance-based Inductive Deep Transfer Learning by Cross-Dataset  Querying with Locality Sensitive Hashing** [EMNLP2019] [[paper link](https://aclanthology.org/D19-6120.pdf)]

      解决标注数据问题。提出基于实例的归纳迁移学习方法，从外部数据集中检索相似实例并融合其表征。在目标模型训练中，引入约束目标实例与检索实例的表征相似性。【新闻分类】【20 Newsgroups（News20）、BBC、BBC Sports】【准确率、F1分数】【实例融合Bi-LSTM】

233. **Supervised and Nonlinear Alignment of Two Embedding Spaces for  Dictionary Induction in Low Resourced Languages** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1076.pdf)]

      针对低资源语言，提出一种非线性映射方法（LLMap），通过归纳学习词向量空间的局部结构差异，显著提升双语词典构建的准确性。方法：结合鲁棒Procrustes分析（RGP）和局部线性神经网络（LLNF），实现分段线性映射以适配不同语言区域的特征。【双语词典归纳】【MUSE】【Precision@1/5/10】【LLMap】

234. **Don’t Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1090.pdf)] 

     研究双语词典模型是否能像人类一样从常见词归纳出罕见词形的翻译，为此构建了新数据集并发现模型在低频形态上表现差，但加入形态约束后有所提升。方法：通过控制变量实验评估模型泛化能力，并简单添加形态对齐约束改进模型。【双语词典归纳】【自建的40个形态完整词典】【精确度@1】【BLI模型+形态约束】

235. **A Regularization-based Framework for Bilingual Grammar Induction** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1148.pdf)]

      提出无需外部资源的双语语法归纳方法，通过正则化让两种语言的解析器互相学习，利用语言间的相似性提升性能，体现了从已知语言到新语言的归纳能力。方法：在无监督解析器上添加参数、边得分或解析树的正则项，强制双语模型共享知识。【双语语法归纳】【Universal Dependencies】【有向依存准确率】【Convex-MST解析器，三种正则化方法】

236. **Cross-lingual Semantic Specialization via Lexical Relation Induction** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1226.pdf)]

      提出一种跨语言方法，利用英语的词汇关系为资源贫乏语言生成语义特化约束，通过翻译和过滤实现知识迁移，显著提升目标语言的词向量质量。方法：先翻译源语言约束到目标语言，再用关系预测模型过滤噪声，最后单语特化目标词向量。【跨语言语义特化】【多语言词相似度数据集】【Spearman、准确率】【本文CLSRI】

237. **Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1328.pdf)]

      揭露MUSE数据集在BDI评估中的缺陷（专有名词干扰和目标词缺失），通过人工分析和实验证明这些缺陷扭曲模型比较，呼吁改进评估方法。方法：标注数据词性、过滤噪声后重评估模型性能，并手动验证预测结果。【双语词典归纳的评估可靠性分析】【MUSE数据集】【精确率@1】【RCSLS、VecMap（VM-S）、MUSE等】

238. **Induction Networks for Few-Shot Text Classification** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1403.pdf)]

      为了解决少样本文本分类中样本多样性干扰的问题，提出了一种能够从少量样本中归纳出类别通用表示的方法（Induction Networks），从而提升模型对未见类别的分类能力。方法总结：通过动态路由算法将样本特征归纳为类别表示，并结合编码和关系模块完成分类任务。【少样本文本分类】【ARSC 、ODIC】【准确率】【本文Induction Networks】

239. **Dialog Intent Induction with Deep Multi-View Clustering** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1413.pdf)]

      为了解决对话系统中意图难以预先定义的问题，提出了一种从对话数据中自动归纳用户意图的方法（Av-Kmeans），通过联合利用用户查询和对话上下文的多视角信息，提升意图发现的准确性。方法总结：通过交替优化多视角表示和聚类，将用户查询与对话上下文的语义信息协同归纳为意图类别。【对话意图归纳】【Twitter航空公司客户支持数据集和AskUbuntu问题意图聚类数据集】【精确率、召回率、F1分数、无监督聚类准确率】【交替视角k均值聚类（Av-Kmeans）】

240. **Multilingual Grammar Induction with Continuous Language Identification** [EMNLP2019] [[paper link](https://aclanthology.org/D19-1576.pdf)]

      为解决多语言语法归纳中语言相似性难以定义的问题，提出了一种通过语言嵌入自动学习句法共性的方法（G/I模型），无需依赖语言谱系知识即可实现语法参数的跨语言共享。方法总结：用语言嵌入表示语言身份，通过神经网络预测语法参数，并联合语言识别任务优化嵌入，从而归纳出跨语言的通用句法模式。【多语言无监督依存语法归纳】【Universal Dependency Treebank 1.4】【无标记依存准确率】【基于语言嵌入的多语言语法模型（G）】

241. **Automatic Taxonomy Induction and Expansion** [EMNLP2019] [[paper link](https://aclanthology.org/D19-3005.pdf)]

      为解决知识图谱构建中分类体系人工标注成本高的问题，提出了一种混合自动归纳与人机协作的系统（KGIS），通过多策略提取“is-a”关系并支持人工优化，实现了高效的知识归纳。方法总结：结合语言学规则、语义网和神经网络（SPON）自动提取分类关系，并通过交互式表格（SSS）嵌入人工反馈，形成闭环的知识归纳流程。【自动分类体系归纳与扩展】【SemEval 2018 Hypernym Detection英文领域语料、NeurIPS会议论文语料】【人工验证的准确率】【KGIS系统】

242. **Latent semantic network induction in the context of linked example senses** [EMNLP2019] [[paper link](https://aclanthology.org/D19-5523.pdf)]

      利用Wiktionary的开放数据，通过归纳词条间的关系自动构建语义网络，填补传统专家标注资源的不足，并验证其语义结构的有效性。方法：基于关系消歧和集合层次归纳，从具体词条定义中提取抽象语义关系，形成有向无环图。【关系消歧和语义网络构建】【Wiktionary的英语部分】【宏/微召回率、精确率、F0.1分数；与WordNet的相似性对比】【基于集合交集的层次归纳算法】

243. **Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.345.pdf)]

      提出神经符号归纳学习模型RuleNN，学习可解释的一阶逻辑规则的神经网络。先构建逻辑谓词、然后用谓词生成模块和子句生成模块学习组合谓词。【句子分类】【Contracts、TREC】【AUC-PR、F1分数】【RuleNN】

244. **Be More with Less: Hypergraph Attention Networks for Inductive Text Classification** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.399.pdf)]

      GNN文本分类任务，提出文档级超图避免构建全局大图，支持归纳式学习，可直接处理新文档。通过超图建模高阶单词交互，避免二元关系的语义损失。【文本分类】【20NG（20-Newsgroups）、R8/R52（Reuters）、Ohsumed（医学文献）、MR（Movie Review）】【准确率】【提出的HyperGAT】

245. **“A Little Birdie Told Me ... ”- Inductive Biases for Rumour Stance Detection on Social Media** [EMNLP2020] [[paper link](https://aclanthology.org/2020.wnut-1.31.pdf)]

      现有方法未考虑社交媒体特有的用户行为和语言风格，通过引入归纳偏置来改进立场检测任务。【立场分类】【Macro F1F1分数】【准确率】【基于BERT的模型，结合Late Fusion、TF-IDF特征】


246. **Accurate Word Alignment Induction from Neural Machine Translation** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.42.pdf)]

      旨在改进神经机器翻译中的词对齐问题，提出一种归纳方法，通过调整对齐步骤和引入监督学习，显著提升了对齐效果。方法：Shift-Att通过调整对齐步骤从注意力权重中提取对齐信息，而Shift-AET通过增加对齐模块并利用监督学习进一步优化对齐准确性。【词对齐】【German-English】【Alignment Error Rate 和 BLEU】【Shift-Att 和 Shift-AET】

247. **Connecting the Dots: Event Graph Schema Induction with Path Language Modeling** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.50.pdf)]

      提出了一种归纳方法（PathLM），从事件实例中学习高频且连贯的路径模式，构建事件图模式，以更好地理解事件间的复杂关联，并提升信息抽取效果。【事件图模式归纳】【ACE 2005】【实例覆盖率、实例连贯性、信息抽取任务（F1分数）】【PathLM】

248. **Semi-supervised New Event Type Induction and Event Detection** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.53.pdf)]

      提出一种半监督归纳方法（SS-VQ-VAE），利用少量标注数据自动发现新事件类型，同时提升事件检测性能，减少对人工标注的依赖。方法总结：通过向量量化和变分自编码器联合学习事件类型的离散表示，结合半监督目标函数实现新类型的归纳和检测。【半监督新事件类型归纳和事件检测】【ACE 2005】【事件检测：精确率、召回率、F1值；新类型归纳：标准化互信息、Fowlkes Mallows指数、同质性】【SS-VQ-VAE】

249. **Improving Bilingual Lexicon Induction for Low Frequency Words** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.100.pdf)]

      分析低频词翻译的瓶颈（边际和中心性），提出两种归纳方法（铰链损失和HNN），显著提升双语词典归纳中低频词的性能。方法总结：通过优化变换矩阵的边际约束和均匀化目标词检索概率，解决低频词翻译的核心问题。【双语词典归纳和单语词典归纳】【FastText的Wiki和Crawl词向量、MUSE库中的英语-法语和英语-芬兰语数据】【翻译准确率、边际值、中心性度量】【结合铰链损失和HNN的改进方法】

250. **Analogous Process Structure Induction for Sub-event Sequence Prediction** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.119.pdf)]

      现有的NLP研究多关注于“横向”事件预测（如预测下一个事件），而忽略了事件的“纵向”关系（如子事件序列的抽象和类比）。因此，提出通过类比已知过程的子事件结构，来预测新过程的子事件序列。【预测新过程的子事件序列】【从WikiHow网站收集的过程图】【事件级别的ROUGE】【APSI】

251. **Structured Attention for Unsupervised Dialogue Structure Induction** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.148.pdf)]

      对话结构（如语义结构和交互结构）自动归纳的现有方法多依赖人工标注或忽略结构化依赖关系。通过无监督方式学习对话的潜在结构，提升模型的解释性和泛化能力。提出“结构化注意力变分循环神经网络”（SVRNN），通过结合结构化注意力机制与VRNN，无监督地学习对话的语义结构和交互结构。【无监督对话结构学习】【SimDial（模拟对话）和Ubuntu Chat Corpus】【语义结构：结构欧氏距离和结构交叉熵；交互结构：BLEU、说话者/受话者识别准确率】【SVRNN】

252. **LNMAP: Departures from Isomorphic Assumption in Bilingual Lexicon  Induction Through Non-Linear Mapping in Latent Space** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.215.pdf)] 

      基于线性映射的BLI方法假设不同语言的词嵌入空间具有相似的几何结构，提出不依赖同构假设的半监督方法，用非线性映射在潜在空间中学习更灵活的词嵌入对齐。提出LNMap，通过两个独立的自编码器将源语言和目标语言的词嵌入映射到各自的潜在空间，然后利用少量种子词典学习潜在空间之间的非线性映射。【双语词典归纳】【MUSE和VecMap】【P@1】【LNMap】

253. **Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction** [EMNLP2020] [[paper link](https://aclanthology.org/2020.emnlp-main.238.pdf)]

      通过设计双向交互机制（POT和BLU）提升半监督双语词典归纳的性能，利用有限标注数据归纳先验知识，并结合未标注数据的结构信息扩展学习能力。方法：提出基于消息传递的循环（CSS）和并行（PSS）半监督框架，通过监督与无监督模块的交互优化词向量对齐。【双语词典归纳】【MUSE和VecMap】【P@1】【基于POT和BLU的CSS和PSS框架】

254. **AnEvaluation Method for Diachronic Word Sense Induction** [EMNLP2020] [[paper link](https://aclanthology.org/2020.findings-emnlp.284.pdf)]

      提出基于生物医学标注数据的评估框架，直接量化历时词义归纳模型（如NEO和SCAN）在词义聚类和动态演变检测中的性能，揭示模型从数据中归纳词义规律的优劣。方法：通过构建大规模时间标注数据集并设计多维度指标（如聚类分类、时序距离），系统评估生成式概率模型对词义演变的建模能力。【历时词义归纳】【基于Medline和UMLS构建的生物医学多义词历时数据集】【聚类分类（F1、MAE）、涌现检测（精确率、召回率）、时序距离（DTW、欧氏距离）】【词义归纳模型】

255. **Improved Latent Tree Induction with Distant Supervision via Span Constraints** [EMNLP2021] [[paper link](https://aclanthology.org/2021.emnlp-main.395.pdf)]

      提出利用低成本span约束（如实体边界）提升无监督句法分析性能，通过PS-SVM从部分标注中归纳句法结构规律，在新闻和生物医学领域显著缩小与监督模型的差距。方法：在DIORA中注入span约束并设计PS-SVM损失，以远程监督优化无监督句法树的生成。【无监督成分句法分析】【WSJ Penn Treebank、CRAFT、Ontonotes】【成分匹配准确率、约束覆盖度】【DIORA + PS-SVM】

256. **The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction** [EMNLP2021] [[paper link](https://aclanthology.org/2021.emnlp-main.422.pdf)] 

      提出了一种基于图建模的时序复杂事件模式归纳方法，从实例中抽象出概率化模式以支持事件预测。通过自回归图生成模型（结合GNN和复制机制）构建事件图，捕捉事件间的时间与参数依赖关系。【时序复杂事件模式归纳】【General Schema Learning Corpus、IED Schema Learning Corpus】【 事件匹配F1、事件序列匹配F1、参数连接匹配F1、实例图困惑度、模式引导事件预测的MRR】【时序事件图模型】

257. **Corpus-based Open-Domain Event Type Induction** [EMNLP2021] [[paper link](https://aclanthology.org/2021.emnlp-main.441.pdf)]

      提出了一种无监督的事件类型归纳方法ETypeClus，通过从语料中自动聚类谓词-宾语对生成事件类型，显著降低了人工标注需求，体现了从数据中归纳知识的核心思想。方法: 结合谓词消歧和潜在空间聚类，将P-O对嵌入球形空间并生成事件类型。【开放域事件类型归纳】【ACE 2005、ERE、Pandemic Dataset】【人工评估和聚类质量指标】【ETypeClus】

258. **An Analysis of Euclidean vs. Graph-Based Framing for Bilingual Lexicon Induction from Word Embedding Spaces** [EMNLP2021] [[paper link](https://aclanthology.org/2021.findings-emnlp.64.pdf)] 

      对比了欧几里得与图结构方法在双语词典归纳中的表现，提出结合两者的框架，通过从种子数据中归纳映射关系，显著提升了低资源场景的性能。方法: 结合正交Procrustes（线性变换）与种子图匹配（邻域结构对齐），通过迭代和随机性优化映射质量。【双语词典归纳】【MUSE】【精确率、召回率】【迭代Procrustes、迭代SGM】

259. **Unsupervised Chunking as Syntactic Structure Induction with a Knowledge-Transfer Approach** [EMNLP2021] [[paper link](https://aclanthology.org/2021.findings-emnlp.307.pdf)]

      通过归纳无监督解析模型中的分块模式，设计HRNN模型以提升无监督分块的性能，从而解决低资源语言的语言结构发现问题。方法：提出了一种基于知识迁移和分层循环神经网络的方法，通过启发式规则诱导标签并利用HRNN学习分块模式。【无监督分块】【CoNLL-2000（英语）、CoNLL-2003（德语）、English Web Treebank（评论领域）】【短语F1分数和标签准确率】【分层循环神经网络（HRNN）】

260. **Character-based PCFG Induction for Modeling the Syntactic Acquisition of Morphologically Rich Languages** [EMNLP2021] [[paper link](https://aclanthology.org/2021.findings-emnlp.371.pdf)] 

      通过引入基于字符的PCFG模型，验证子词信息对句法结构归纳的作用，尤其是在形态丰富的语言中，从而支持分布式语言习得理论。方法：提出了一种基于字符序列的神经PCFG模型，与单词模型对比，证明其优越性。【无监督PCFG归纳】【儿童导向语料库、多语言树库】【Recall-Homogeneity（结合召回率和同质性）】【NeuralChar（基于字符的PCFG）和NeuralWord（基于单词的PCFG）】

261. **Dependency Induction Through the Lens of Visual Perception** [EMNLP2021] [[paper link](https://aclanthology.org/2021.conll-1.2.pdf)]

      通过结合词语具体性和视觉信息，从多模态数据中归纳更准确的句法结构，解决纯文本模型的局限性。方法：扩展神经L-PCFG，引入词语具体性评分和视觉语义角色标签作为先验，联合优化依存和成分分析。
     【无监督依存结构和成分结构的联合语法归纳】【MSCOCO】【依存分析：定向附着分数和无向附着分数；成分分析：F1分数（语料库级和句子级）】【Concrete L-PCFG（词语具体性）、Coupling（视觉启发式）】

262. **Inductive Relation Prediction with Logical Reasoning Using Contrastive Representations** [EMNLP2022] [[paper link](https://aclanthology.org/2022.emnlp-main.286.pdf)]   

      提升知识图谱关系预测方法利用隐含逻辑规则进行归纳的能力，通过关系路径建模逻辑规则中的关系语义，利用对比学习缓解逻辑规则监督不足的问题。【归纳式关系预测】【WN18RR、FB15K-237、NELL-995】【AUC-PR、Hits@10】【提出的LogCo】

263. **An Adaptive Logical Rule Embedding Model for Inductive Reasoning over Temporal Knowledge Graphs** [EMNLP2022] [[paper link](https://aclanthology.org/2022.emnlp-main.493.pdf)]  

      提出自适应逻辑规则嵌入模型（ALRE-IR），旨在结合嵌入和逻辑规则两种方法的优点，自适应地提取和评估逻辑规则，实现高效且可解释的时间知识图谱归纳推理。【时间知识图谱的外推推理】【ICEWS0515、ICEWS14、ICEWS18】【MRR、Hits@1、Hits@3、Hits@10】【ALRE-IR】

264. **Simplified Graph Learning for Inductive Short Text Classification** [EMNLP2022] [[paper link](https://aclanthology.org/2022.emnlp-main.735.pdf)]  

      提出一种基于层次图学习的归纳式短文本分类方法SimpleSTC。从WikiText中提取高频词，基于点间互信息构建词图，用GNN学习词的嵌入，将短文本表示为词嵌入的聚合，用余弦相似度动态构建文本图，传播标签进行分类。【归纳式短文本分类】【Twitter、MR（电影评论）、Snippets（搜索片段）、TagMyNews（新闻标题）】【Micro-Accuracy（ACC）和Macro-F1】【SimpleSTC】

265. **Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation** [EMNLP2022] [[paper link](https://aclanthology.org/2022.findings-emnlp.270.pdf)] 

      大型预训练语言模型容易生成有毒或带有偏见的内容。通过逆向生成方法，构造高诱导性（highly inductive）的数据集BAD+，暴露预训练对话模型（如Blender、DialoGPT）的安全缺陷。【对话模型的安全性检测与去毒】【BAD+】【诱导成功率、毒性分数、多样性指标】【Blender、DialoGPT、Plato2】

266. **Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport** [EMNLP2022] [[paper link](https://aclanthology.org/2022.emnlp-main.164.pdf)]

      提出GOAT方法，通过最优传输改进双语词典归纳任务，解决低资源和非同构嵌入空间下的泛化（归纳）问题。方法总结：基于图匹配和Sinkhorn算法的平滑优化，实现更鲁棒的双语词对齐。【双语词典归纳】【MUSE双语词典、fastText词嵌入】【Precision@1】【GOAT】

267. **Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation** [EMNLP2022] [[paper link](https://aclanthology.org/2022.emnlp-main.181.pdf)]

      通过全局剪枝策略改进基于图的模型R2D2模型，解决其局部最优和推理速度问题，并在无监督语法归纳任务中验证了方法的有效性。方法：提出Fast-R2D2，通过自上而下解析器联合训练优化剪枝过程，提升性能和效率。【无监督语法归纳和文本表示】【英文使用WikiText103和Penn Treebank (PTB)，中文使用中文维基百科和Chinese Penn Treebank (CTB)】【句子级别的无标记F1分数、成分召回率】【Fast-R2D2】

268. **Finding Dataset Shortcuts with Grammar Induction** [EMNLP2022] [[paper link](https://aclanthology.org/2022.emnlp-main.293.pdf)]

      通过语法归纳自动发现NLP数据集中的捷径模式，揭示模型对虚假相关的依赖，并提出缓解方法。方法：提出基于PCFG/SCFG的语法归纳框架，通过互信息筛选高频子树作为捷径特征。【自动检测NLP数据集中的捷径】【IMDb（情感分析）、SUBJ（主观性分类）、SNLI（自然语言推理）、QQP（复述检测）】【互信息】【基于PCFG/SCFG的语法归纳方法】

269. **RAPO:AnAdaptive Ranking Paradigm for Bilingual Lexicon Induction** [EMNLP2022] [[paper link](https://aclanthology.org/2022.emnlp-main.606.pdf)]

      通过个性化适配和严格正交映射提升双语词典归纳任务的性能，解决全局映射和排序能力不足的问题。方法：RAPO结合个性化偏移、Householder投影和排序损失，显著提升了双语词典的生成准确率。【双语词典归纳】【MUSE】【Precision@1】【RAPO】

270. **Automatic Rule Induction for Efficient Semi-Supervised Learning** [EMNLP2022] [[paper link](https://aclanthology.org/2022.findings-emnlp.3.pdf)]

      通过自动规则归纳增强预训练模型在小样本任务中的性能和可解释性，解决了黑箱模型和人工规则设计的局限性。方法：ARI通过低容量模型生成符号化规则，并利用注意力机制将其与预训练模型结合，实现了规则与神经网络的协同优化。【序列分类和关系抽取任务】【AGNews、CDR、ChemProt、IMDB、SciCite、SemEval、SMS、TREC、Youtube】【二元分类任务使用F1分数，多分类任务使用宏平均F1分数】【ARI】

271. **Improving Bilingual Lexicon Induction with Cross-Encoder Reranking** [EMNLP2022] [[paper link](https://aclanthology.org/2022.findings-emnlp.302.pdf)]

      通过归纳CLWE空间中的词汇知识，结合交叉编码器改进双语词典归纳任务，显著提升跨语言词对齐的准确性。方法：基于极化分数微调mPLM作为交叉编码器，重排序CLWE的相似度结果。【双语词典归纳】【XLING和PanLex-BLI】【Precision@1】【BLICEr（基于XLM-R或mBERT的交叉编码器）】

272. **The Effects of Corpus Choice and Morphosyntax on Multilingual Space Induction** [EMNLP2022] [[paper link](https://aclanthology.org/2022.findings-emnlp.304.pdf)]

      通过扰动语料和统计分析，归纳语言模型在多语言空间构建中的偏差，发现其能力主要源于词汇分布统计而非语法结构。方法：轻量BERT在扰动语料上训练，通过词翻译和句子检索评估跨语言能力，结合聚类与相关性分析。【多语言空间构建，通过词翻译和句子检索评估】【Wikipedia和Common Craw】【词翻译准确率、句子检索准确率】【轻量级BERT变体】

273. **Multilingual SubEvent Relation Extraction:  A Novel Dataset and Structure Induction Method** [EMNLP2022] [[paper link](https://aclanthology.org/2022.findings-emnlp.407.pdf)]

      通过归纳文本中的关键上下文结构（图表示）和构建多语言数据集，提升子事件关系抽取的泛化能力。方法：基于最优传输对齐依赖路径与非依赖路径词，构建图结构并用GCN学习表示。【子事件关系抽取】【HiEve、IC（英语）和自建多语言数据集mSubEvent】【Parent-Child（PC）、Child-Parent（CP）及其平均F1分数】【OT-SRE（基于RoBERTa/GCN+OT）】

274. **Combining Noisy Semantic Signals with Orthographic Cues: Cognate Induction for the Indic Dialect Continuum** [EMNLP2022] [[paper link](https://aclanthology.org/2022.conll-1.9.pdf)]

      为解决低资源Indic方言的同源词识别问题，本文提出结合噪声语义嵌入与拼写规则的无监督归纳方法，构建首个大规模方言语料库HinDialect并验证方法的有效性。方法：通过双语嵌入筛选候选词，再优化拼写距离（JW或EM学习）以识别同源词。【无监督同源词识别】【单语语料：自爬取的HinDialect语料；评估词典：从Languages Home网站构建的20种语言与Hindi的合成双语词典】【精确率、跨语言嵌入整合度、Recall@K】【SEM_JW、SEM_EMT】

275. **FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional Reasoning in GPT-4** [EMNLP2023] [[paper link](https://aclanthology.org/2023.findings-emnlp.245.pdf)]  

      通过利用微调模型中已验证的归纳偏差，提升GPT-4在组合推理任务中的性能，同时减少人工设计提示的负担。提出了FinePrompt方法，通过以下三种策略将微调模型的归纳偏差转化为提示：Attribute-Infused Prompt：注入任务相关的属性（如算术规则或不等式定义）作为提示的一部分。Pipeline-Infused Prompt：将复杂任务分解为子问题序列，模仿微调模型中的流水线方法。Graph-Infused Prompt：将图结构中的连接信息（如实体或句子间的关系）嵌入到提示中。【多跳问答和文本数值推理】【MuSiQue（多跳问答）和DROP（数值推理）】【DROP：答案精确匹配和F1分数；MuSiQue：答案F1和支持段落F1】【GPT-4】

276. **Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information** [EMNLP2023] [[paper link](https://aclanthology.org/2023.findings-emnlp.431.pdf)] 

      知识图谱中，基于结构的归纳推理方法（如GraIL）在处理新实体时缺乏足够的邻居信息，并且忽略知识图谱中的本体信息。提出通过融合本体信息来增强知识图谱的归纳推理能力。利用本体类型信息增强节点初始化，通过类型约束捕获缺失的本体信息。【知识图谱的归纳链接预测】【YAGO21K-610和DB45K-165】【MRR、Hits@1、Hits@10】【基于GNN的子图推理框架】

277. **IAG: Induction-Augmented Generation Framework for Answering  Reasoning Questions** [EMNLP2023] [[paper link](https://aclanthology.org/2023.emnlp-main.1.pdf)]

      解决开放域问答中隐式推理问题，提出归纳增强生成（IAG）框架，结合检索和归纳推理。两种实现：IAG-GPT：直接使用GPT-3生成的归纳知识辅助生成答案。IAG-Student：通过知识蒸馏和TailBack优化训练学生模型，替代GPT-3以减少推理时的依赖。【开放域问答】【CSQA2.0（常识问答）和StrategyQA（策略推理问答）】【准确率】【IAG-GPT、IAG-Student】

278. **On Bilingual Lexicon Induction with Large Language Models** [EMNLP2023] [[paper link](https://aclanthology.org/2023.emnlp-main.595.pdf)]

      探索多语言大模型在双语词典归纳任务中的潜力，通过零样本和少样本提示方法，结合归纳推理提升翻译准确性。方法：基于提示和微调的BLI方法，通过检索近邻词对作为上下文示例优化模型输出。【双语词典归纳】【XLING和PanLex-BLI】【Precision@1】【LLMs】

279. **CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs** [EMNLP2023] [[paper link](https://aclanthology.org/2023.emnlp-main.717.pdf)]

      通过归纳原子任务的组合规则，自动生成复杂对话任务数据，以提升公开模型在组合任务上的性能。方法：CESAR框架通过模块化任务指令，实现程序化的任务组合生成。【多轮对话任务】【InstructDial++】【原子任务使用准确率、Rouge-L等】【CESAR】

280. **Adaptive Structure Induction for Aspect-based Sentiment Analysis with Spectral Perspective** [EMNLP2023] [[paper link](https://aclanthology.org/2023.findings-emnlp.79.pdf)]

      通过频谱方法从预训练语言模型中归纳适应性结构，以提升基于方面的情感分析性能。方法：提出基于频域过滤和自动频率选择的图结构学习方法，优化多尺度语言表示的结构归纳能力。【基于方面的情感分析】【SemEval 2014、Twitter】【Accuracy和Macro-F1】【基于PLMs（BERT、RoBERTa）的GSL模块，结合FLT和AFS】

281. **Complex Event Schema Induction with Knowledge-Enriched  Diffusion Model** [EMNLP2023] [[paper link](https://aclanthology.org/2023.findings-emnlp.319.pdf)]

      通过LLMs的知识增强和离散扩散模型，从实例图中归纳复杂事件模式，解决知识覆盖不足和错误传播问题。方法：提出结合Python风格提示的实例图扩展、非自回归离散扩散生成和实体关系预测的三阶段框架。【复杂事件模式归纳】【IED Schema Learning Corpus】【事件类型匹配F1、事件序列匹配F1、节点/边类型分布的KL散度、实体关系连接匹配】【KDM】

282. **A Structure-Aware Generative Adversarial Network for Bilingual Lexicon Induction** [EMNLP2023] [[paper link](https://aclanthology.org/2023.findings-emnlp.721.pdf)]

      通过显式建模词嵌入的拓扑结构和学习灵活的映射函数，提升双语词典归纳的准确性，尤其是在非同构语言对上。方法：SA-GAN结合GCN、对抗训练和局部映射，显式捕捉结构信息并减少对同构假设的依赖。
     【双语词典归纳】【MUSE数据集】【Precision@1】【SA-GAN】

283. **Grammar induction pretraining for language modeling in low resource contexts** [EMNLP2023] [[paper link](https://aclanthology.org/2023.conll-babylm.5.pdf)]

      通过无监督语法归纳提取语法信息融入语言模型，提升低资源下的泛化能力。实验发现性能提升可能来自分词器而非语法嵌入本身。方法：用compound-PCFG模型归纳语法结构并初始化语言模型，对比随机嵌入和基线模型。【低资源语言模型预训练及评估】【BabyLM严格小数据集】【BLiMP（语言学最小对基准）】【基于语法归纳嵌入初始化的OPT-125M-like模型】

284. **Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations** [EMNLP2024] [[paper link](https://aclanthology.org/2024.emnlp-main.645.pdf)]  

      提出方法STEP，通过在预训练阶段让Transformer学习执行基于依赖树的句法转换任务来增强其结构化归纳偏置让其更好地处理句法相关的任务。预训练阶段模型接收一个描述转换的前缀和输入句子，预测转换后的依赖树。【句法转换任务、分块任务、语义解析任务】【预训练数据：C4语料库；下游任务数据集：StylePTB（句法转换）、CoNLL-2000（分块）、SLOG（语义解析）、ATIS（航班查询语义解析）】【精确匹配准确率、BLEU、TER】【STEP】

285. **Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues** [EMNLP2024] [[paper link](https://aclanthology.org/2024.emnlp-main.964.pdf)]  

      让两个LLM交互自动生成对话（一个模拟用户提问，另一个作为系统回答），可以生成多轮指令对话数据。提出了一种归纳-演绎策略复用方法（IDEAS），提升用户模拟器的能力。归纳阶段从真实对话中提取策略，并通过聚类和抽象生成高层策略。演绎阶段用户模拟器选择合适的策略，并通过质量控制模块确保指令质量。【指令生成、回答生成】【ShareGPT和UltraChat】【指令评估：适当性、连贯性、深度、洞察力、多样性；模型评估：AlpacaEval、MT-Bench、MT-Bench++、MT-Eval】【LLMs】

286. **Explicit Inductive Inference using Large Language Models** [EMNLP2024] [[paper link](https://aclanthology.org/2024.findings-emnlp.926.pdf)]  

      LLMs在归纳推理中存在“认证偏差”，导致模型在判断前提P是否蕴含假设H时，过度依赖假设H本身的真实性，而忽略了前提P与H之间的逻辑关系。提出EIDI，为P中实体分配类型标签，消除谓词歧义。用LLM生成新的前提P'，并推导出对应的假设H'，对新的P'→H'推理问题进行预测。【判断两个二元谓词之间的蕴含关系】【Levy/Holt数据集】【AUCnorm】【GPT-3.5-Turbo和Llama3-70B-instruct】

287. **INDUCT-LEARN: Short Phrase Prompting with Instruction Induction** [EMNLP2024] [[paper link](https://aclanthology.org/2024.emnlp-main.297.pdf)]

      针对低资源场景，提出Induct-Learn框架，通过LLMs的归纳能力从少量示例生成伪指令和推理链，提升模型任务表现并降低依赖人工成本。方法：三阶段框架（归纳指令→生成伪CoT→推理），结合语义归纳与拼写优化。【指令归纳与少样本推理】【BBH-Induct、Evals-Induct】【精确匹配准确率、跨语言嵌入整合度、Fill Rate】【Induct-Learn】

288. **SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions** [EMNLP2024] [[paper link](https://aclanthology.org/2024.emnlp-industry.24.pdf)]

      提出 SHIELD，让 LLM 与专家协作“归纳”出供应链事件图谱，再用该图谱在新闻中预测断供风险，显著提升可解释性与准确率。方法：用 LLM 归纳层次化事件图谱 → 结合 GCN/逻辑约束在新闻上做可解释断供预测。【EV 电池供应链断供事件检测与风险预测】【Schema Learning Dataset、EV Supply Chain News Dataset】【Precision、Recall、F-score、专家主观评分】【GPT-4o、Llama3-3b/70b（schema 归纳），RoBERTa-large（事件抽取），GCN+逻辑约束+指代消解（预测）】

289. **Text2Model: Text-based Model Induction for Zero-shot Image Classification** [EMNLP2024] [[paper link](https://aclanthology.org/2024.findings-emnlp.8.pdf)]

      提出一种动态生成任务特定分类模型的方法（T2M-HN），解决了传统零样本学习中固定表示空间的局限性，并通过归纳能力支持复杂文本描述和多模态数据分类。方法总结：基于超网络的框架，输入文本描述生成分类模型，利用等变性和不变性设计提升泛化性能。【零样本图像分类、3D点云分类、动作识别】【AwA（动物图像）、SUN（场景图像）、CUB（鸟类图像）、ModelNet-40（3D点云）、BABEL 120（3D骨骼动作数据）】【准确率、调和平均数、AUC】【T2M-HN】

290. **Leveraging Grammar Induction for Language Understanding and Generation** [EMNLP2024] [[paper link](https://aclanthology.org/2024.findings-emnlp.259.pdf)]

      提出了一种无监督语法归纳方法，通过自诱导语法结构增强Transformer模型的语言理解和生成能力，解决了传统方法依赖外部解析器的问题，并验证了其在归纳学习中的有效性。方法总结：通过语法解析器生成语法掩码指导注意力机制，结合BPE嵌入和混合损失函数优化语法归纳过程。【机器翻译（MT）和自然语言理解】【机器翻译：IWSLT14-De/En、NC11-De/En、ASPEC-Zh/Ja；自然语言理解：GLUE（包含CoLA、SST-2、MNLI等子任务）】【BLEU、准确率/F1/Matthews相关系数】【基于语法掩码的语法感知Transformer】

291. **Unsupervised Does Not Mean Uninterpretable: The Case for Word Sense Induction and Disambiguation** [EACL 2017] [[paper link](https://aclanthology.org/2024.eacl-long.74.pdf)]
 
     本文探讨无监督词义归纳与消歧（WSI/WSD）任务的可解释性问题，提出将神经嵌入聚类结果映射到人类可理解的语义标签的方法。作者基于多语言语料构建聚类，并通过上下文特征与原型词表映射生成解释标签，验证无监督方法并非“黑箱”。在 SemEval 和 WiC 等数据集上评估，结果显示该方法可在保持高性能的同时，提供可解释的词义聚类，帮助理解模型决策过程。【词义归纳/消歧】【SemEval WSI/WSD, WiC】【聚类性能与可解释性评估指标】【基于神经嵌入聚类+可解释语义标签映射】

293. **From Segmentation to Analyses: a Probabilistic Model for Unsupervised Morphology Induction** [EACL 2017] [[paper link](https://aclanthology.org/2024.eacl-long.41.pdf)] 

     本文提出一种从词形分割到形态分析的统一概率模型，用于无监督形态归纳任务。模型在贝叶斯框架下同时学习词的形态切分及其语义分析，利用词典先验和词频信息，通过吉布斯采样推断词缀与词根。实验证明，该方法在多种形态复杂语言（芬兰语、土耳其语等）的分割和分析任务上优于现有无监督方法，提供了更丰富且结构化的形态分析结果。【无监督形态归纳】【芬兰语、土耳其语等形态复杂语料】【分割F1、分析准确率】【贝叶斯概率模型（吉布斯采样）】

295. **The ContrastMedium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links** [EACL 2017] [[paper link](https://aclanthology.org/2024.eacl-long.19.pdf)]  

     本文提出 ContrastMedium 算法，旨在在仅有极少超/下位边且知识图谱噪声较大的条件下进行分类体系（taxonomy）归纳。该方法通过模拟“流体扩散”过程，将少量已知的层级信息（如超类-子类关系）传播到整个图中，从而形成潜在层次结构；随后结合分布式嵌入和图正则化，对节点进行重新排序以增强鲁棒性。实验证明，该算法在 WordNet 子图及其他噪声知识图谱上显著优于基线方法，尤其在稀疏和噪声环境下保持高性能。【分类体系归纳】【WordNet 子图、噪声知识图谱】【F1、层级一致性指标】【ContrastMedium 扩散算法 + 图正则化】

296. **Bilingual Lexicon Induction by Learning to Combine Word-Level and Character-Level Representations** [EACL 2017] [[paper link](https://aclanthology.org/2024.eacl-long.82.pdf)]  

     本文提出一种结合词级与字符级表示的双语词典归纳方法，旨在提升跨语言词汇对齐的准确性，尤其在低频词和形态丰富语言场景中。模型联合学习词向量和字符级表示（基于CNN/LSTM），并通过可训练的加权机制自适应融合两种表示，然后在无监督对齐框架下（如 MUSE）进行映射。实验结果表明，该方法在多组语言对上的 Precision@1 显著优于仅用词级表示的基线，对形态变化和拼写差异更具鲁棒性。【双语词典归纳】【多语言对词典数据】【Precision@1】【词级+字符级表示融合模型（CNN/LSTM + 加权融合）】

297. **Unsupervised Dialogue Act Induction using Gaussian Mixtures** [EACL 2017] [[paper link](https://aclanthology.org/2024.eacl-long.46.pdf)]  

     本文提出一种基于高斯混合模型（GMM）的无监督对话行为归纳方法，将对话中的话语表示映射到潜在语义空间，通过高斯混合聚类自动发现对话行为类别。该方法不依赖人工标注，利用句向量及上下文信息建模话语的语义分布。实验在 Switchboard 和 AMI 等多语料对话数据集上进行，结果显示该方法在 Purity、V-measure 等聚类指标上优于多种无监督基线，对对话行为模式具有良好的自动归纳能力。【对话行为归纳】【Switchboard, AMI】【Purity, V-measure】【高斯混合模型聚类】

298. **Bootstrapping Unsupervised Bilingual Lexicon Induction** [EACL 2017] [[paper link](https://aclanthology.org/2024.eacl-long.83.pdf)]  

     本文提出一种自举式无监督双语词典归纳方法，通过迭代扩展高置信度词对来不断改进跨语言词向量映射。方法在初始无监督对齐的基础上，选择置信度最高的词对加入训练集，重新估计映射矩阵，循环迭代直到收敛，从而显著提升低资源和形态丰富语言的词典诱导精度。实验证明，该方法在多组语言对上的 Precision@1 较传统无监督方法有显著提高，且无需任何平行语料。【无监督双语词典归纳】【多语言对词典数据】【Precision@1】【自举迭代映射方法】

299. **Semantic Frame Induction with Deep Metric Learning** [EACL 2023] [[paper link](https://aclanthology.org/2024.eacl-long.40.pdf)]  

     本文提出一种结合深度度量学习的语义框架归纳方法，旨在自动从文本中发现语义框架及其角色结构。方法利用预训练语言模型获取谓词-论元上下文表示，通过深度度量学习将同一框架实例映射到相邻空间，从而实现无监督聚类。实验在 FrameNet 及其他语义角色标注数据上进行，结果显示该方法在框架归纳和角色识别上均优于传统聚类和嵌入方法，表现出较强的自动归纳能力。【语义框架归纳】【FrameNet 等 SRL 数据】【聚类质量指标（F1、AMI 等）】【深度度量学习 + 预训练语言模型】

300. **Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention** [EACL 2023] [[paper link](https://aclanthology.org/2024.eacl-long.42.pdf)]  

     本文提出一种半监督的新事件类型归纳与描述方法，利用对比损失约束的批量注意力机制（Batch Attention with Contrastive Loss）提升少标注场景下的新事件类型识别能力。方法通过已知事件和未标注事件的联合训练，将语义相似的实例聚在一起，并利用注意力机制对每个批次的实例进行全局对比优化，同时自动生成新事件类型的描述。实验证明，该方法在多个事件抽取基准数据集上显著优于传统聚类和纯监督方法，在少量标注样本条件下也能准确发现和描述新类型事件。【事件类型归纳】【ACE、KAIROS 等事件数据】【聚类与描述质量指标（F1、NMI、描述生成BLEU）】【批量注意力+对比损失的半监督模型】

301. **Zero-Shot On-the-Fly Event Schema Induction** [EACL 2023] [[paper link](https://aclanthology.org/2024.findings-eacl.103.pdf)]  

     本文提出一种零样本的即时事件模式（schema）归纳方法，无需预先定义事件类型即可在推理时动态生成事件模式。方法利用大型预训练语言模型，从上下文抽取事件触发词和论元，并通过提示（prompt）引导模型生成事件类型及其论元角色，实现“即取即用”的事件模式构建。实验证明，该方法在多个开放域事件抽取数据集上表现优异，能够准确发现新事件类型及其结构，对低资源和开放域场景具有很强适用性。【事件模式归纳（零样本）】【开放域事件抽取数据】【模式发现与论元识别指标】【预训练语言模型+提示生成】

302. **CEO: Corpus-based Open-Domain Event Ontology Induction** [EACL 2024] [[paper link](https://aclanthology.org/2024.findings-eacl.102.pdf)]  

     本文提出 CEO 框架，用于从大规模语料中自动构建开放域事件本体。CEO 通过从文本中抽取事件触发词、论元及其语义角色，利用聚类与模式发现方法自动归纳事件类型及其层次关系，并构建事件类型之间的本体网络。实验在开放域新闻与维基数据集上验证，CEO 能生成结构合理、层级清晰的事件本体，显著优于现有方法，支持开放域知识抽取与推理。【事件本体归纳】【新闻、维基开放域语料】【本体层次质量指标（层级一致性、聚类质量）】【触发词/论元抽取+聚类+层级模式发现】

303. **Relation Induction in Word Embeddings Revisited** [COLING 2018] [[paper link](https://aclanthology.org/C18-1138.pdf)]

     本文针对“关系归纳”任务——即根据已有的词对关系样本，预测新的符合该关系的词对，分析了传统基于向量平移的方法易受错误正例影响的弱点，提出两种更鲁棒的概率模型：一种利用高斯分布在平移向量上建模其变异性并为源词与目标词施加软约束，另一种采用贝叶斯线性回归假设源词与目标词之间存在线性映射但更弱的结构假设，两者在控制错误预测与泛化能力之间取得更优平衡。【关系归纳】【BATS 数据集（关系实例）】【预测准确性（降低错误率）】【概率关系模型：Gaussian translation + Bayesian linear regression】

304. **Cross-Lingual Emotion Lexicon Induction using Representation Alignment in Low-Resource Settings** [COLING 2020] [[paper link](https://aclanthology.org/2020.coling-main.517.pdf)]  

     本文探讨在资源极度匮乏的语言环境下，如何从资源丰富语言的情感词典迁移构建目标语言的情感词典；方法先利用极小规模的单语语料（如少量圣经节）训练 fastText 嵌入并通过三种无监督对齐算法（Wasserstein-Procrustes、Unsupervised Orthogonal Refinement、Neural LM alignment）构建跨语言向量空间，再用 CSLS 基于词向量检索源语言的近邻，将其情感强度平均投射到目标词，最终评估包括翻译精度（precision@k）、与已有情感词典的皮尔逊相关性，以及下游句子情感预测性能。【情感词典归纳】【极小圣经语料 + NRC English Emotion Intensity Lexicon】【translation precision@3, Pearson correlation, sentence-level emotion prediction accuracy】【fastText + cross-lingual alignment (Wasserstein-Procrustes, Orthogonal Refinement, Neural LM variants) + CSLS 投射】  


305. **A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction** [COLING 2020] [[paper link](https://aclanthology.org/2020.coling-main.526.pdf)]  

     本文提出一种简单且鲁棒的无监督双语词典归纳方法，针对现有方法对噪声和初始化敏感的问题，改进了跨语言词嵌入的对齐与映射策略。通过稳健的初始映射、迭代精炼和噪声过滤机制，使得在不同语料和语言对上表现稳定且无需平行语料。实验在 MUSE 和 VecMap 等多语言数据集上进行，结果在多种语言对上均显著优于现有无监督方法，提升 P@1、P@5、P@10 等指标。【无监督双语词典归纳】【MUSE, VecMap】【P@1, P@5, P@10】【改进的稳健词嵌入对齐算法】


306. **Data Selection for Bilingual Lexicon Induction from Specialized Comparable Corpora** [COLING 2020] [[paper link](https://aclanthology.org/2020.coling-main.527.pdf)]  

      本文探讨在专门领域的小规模可比语料中如何有效进行双语词典归纳。由于领域内平行或可比语料稀缺，直接训练往往效果有限。作者提出利用外域可比语料，通过数据选择挑选对目标领域最有帮助的句对，从而提升词典归纳的准确性。论文系统比较了三种数据选择方法：基于词频的 TF-IDF 评分、基于语言模型的交叉熵选择、以及利用 BERT 句向量计算相似度的方法。在多语言多领域实验中，交叉熵方法表现最佳，相比不做数据选择的基线，平均精准率（MAP）提升约 4 个百分点，同时计算效率是 BERT 方法的十倍，适用于实际低资源场景。【双语词典归纳】【专用可比语料 + 外域补充】【MAP（平均精准率）】【数据选择方法：TF-IDF、交叉熵、BERT 模型】


307. **Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction** [COLING 2020] [[paper link](https://aclanthology.org/2020.coling-main.531.pdf)]  

      本文提出在双语词典归纳任务中结合语义词嵌入和双语正字法（拼写）嵌入，以提升特别是低频词和命名实体的翻译效果。作者训练无监督序列到序列转写模型（seq2seqTr）来生成双语正字法嵌入（BOEs），并与语义词嵌入（BWEs）联合，通过分类模型在两者之间自适应选择最佳翻译来源。实验在 English–Russian BUCC 2020 数据集上按词频分层评估，方法在高、中、低频区间均显著提升 acc@n 指标，并在 NEWS 2010 转写任务上也表现优异，证明跨脚本音译能力强。【双语词典归纳】【BUCC 2020 (En–Ru)、NEWS 2010】【acc@n】【BWEs+BOEs 多模态融合模型（seq2seqTr + 分类器）】


308. **Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction** [COLING 2020] [[paper link](https://aclanthology.org/2020.coling-main.531.pdf)]

     本文研究如何在双语词典归纳（BDI）任务中结合语义信息和正字法（拼写）信息以提升低频词或命名实体的翻译准确性。提出通过训练无监督的序列到序列转写模型（seq2seqTr）构建 Bilingual Orthography Embeddings（BOEs），使源语言和目标语言中音译对在向量空间中靠近，然后设计一个分类模型在BWEs（语义嵌入）与 BOEs（正字法嵌入）之间智能选择最合适的翻译来源。通过在 English–Russian 的 BUCC 2020 数据集上依频率（高频/中频/低频）细分测试，分类模型相比基线方法在所有频率区间均显著提升 acc@n 指标；此外，在 NEWS 2010 转写挖掘任务上，BOEs 的表现也优越，表明其跨脚本音译能力强。【双语词典归纳】【BUCC 2020 (En–Ru) + NEWS 2010 转写挖掘任务】【acc@n】【BWEs, BOEs, seq2seqTr-based classification model】

309. **Lexical Induction of Morphological and Orthographic Forms for Low-Resourced Languages** [COLING Workshop MSR 2020] [[paper link](https://aclanthology.org/2020.msr-1.5.pdf)]

     本文针对资源极端匮乏的低资源语言（尤其是沒有标准正字法的方言性语言，如 Arabizi）中的高词汇稀疏问题，提出一种结合规则推导与词嵌入的词形与正字法变体归纳方法：通过规则映射生成潜在变体候选，并基于词向量相似性筛选与扩展。案例应用于拉丁化阿拉伯方言（Arabizi），构建包含超过 171 000 条词条的变体词汇词典，并在情感分析任务中显著提升表现。【词形与正字法归纳】【Arabizi 方言拉丁化语料 + 规则映射 + 词嵌入】【情感分析任务性能提升】【规则基 + 词嵌入融合方法】

310. **BOS at SemEval-2020 Task 1: Word Sense Induction via Lexical Substitution for Lexical Semantic Change Detection** [SemEval 2020] [[paper link](https://aclanthology.org/2020.semeval-1.20.pdf)]

      本文提出一种基于上下文替换（lexical substitution）的无监督词义归纳方法，应用于语义变化检测任务。作者首先使用神经语言模型为目标词在上下文中生成替代词，然后通过聚类这些替换词以识别不同语义倾向，从而实现词义归纳；方法特别适合检测词义随时间变化。实验在 SemEval-2020 Task 1 提供的多语言语料（英文、德语、拉丁语、瑞典语）上运行，评估指标包括词义数识别准确性、sense clustering 质量等，结果表明该方法在无监督语义变化检测中具有竞争力，且具备良好可解释性。【词义归纳 / 语义变化检测】【SemEval-2020 Task 1 多语言语料（English, German, Latin, Swedish）】【聚类质量 / 词义识别准确性】【基于上下文替换的聚类方法 + 神经语言模型】

311. **Bilingual Lexicon Induction across Orthographically-distinct Under-Resourced Dravidian Languages** [VarDial 2020] [[paper link](https://www.aclweb.org/anthology/2020.vardial-1.6/)]

     本文针对使用不同书写系统的资源匮乏 Dravidian 语言（如 Tamil、Telugu、Kannada、Malayalam），提出一种方法：首先将这些语言统一转写为拉丁字母，通过最大公共子序列（LCS）替代 Levenshtein 编辑距离来检测同源词，再用于双语词典归纳以提升嵌入对齐的准确性。实验显示，该方法大幅提升了多种语言对上的词典归纳准确率，使得词典自动构建在这些低资源、书写系统多样的语言组合中变得可行。【双语词典归纳】【Dravidian 语言 Wikipedia 单语语料 + IndoWordNet 种子词典】【翻译准确率】【转写处理 + LCS 驱动的嵌入对齐方法】


312. **Team Rouges at SemEval-2020 Task 12: Cross-lingual Inductive Transfer to Detect Offensive Language** [SemEval 2020] [[paper link](https://aclanthology.org/2020.semeval-1.290.pdf)]

     本文提出一种跨语言归纳迁移方法，用于在多语言环境中识别社交媒体中的冒犯性语言。作者基于上下文化字嵌入模型 XLM-RoBERTa（XLM-R），在五种语言的 OffensEval 2020 多语言冒犯语识别数据集上进行训练与评估——在英语子任务中取得 F1=0.919 的第四名，在土耳其语子任务中取得 F1=0.781 的第八名。进一步实验显示该模型在零样本学习场景下依然具有竞争力，具备良好的跨语言扩展能力。【冒犯语言识别】【OffensEval 2020 (五种语言 Twitter 语料)】【F1-score】【XLM-R】


313. **Resource Constrained Dialog Policy Learning via Differentiable Inductive Logic Programming** [COLING 2020] [[paper link](https://aclanthology.org/2020.coling-main.597.pdf)]  

     本文提出一种面向资源受限场景的对话策略学习方法——通过可微归纳逻辑（DILOG），将归纳逻辑编程与神经网络结合于一体，旨在提升在样本极少时的学习效率与跨领域迁移能力。DILOG 在 SimDial 数据集上的单样本训练中实现了99%以上的领域内测试准确率，并可零样本迁移到其他领域保持99%以上迁移准确率；在 MultiWoZ 上，同样展示了超过90%的 Inform 和 Success指标，同时提出引入 Action F1 分数，以衡量误报问题。与最先进神经模型相比，DILOG 在 MultiWoZ 上达到了约 100 倍的数据效率，且性能相当【对话策略学习（资源受限）】【SimDial, MultiWoZ】【域内准确率, Inform, Success, Action F1】【DILOG（可微归纳逻辑模型）】


314. **ConTextING: Granting Document-Wise Contextual Embeddings to Graph Neural Networks for Inductive Text Classification** [COLING 2022] [[paper link](https://aclanthology.org/2022.coling-1.100/)]

     本文提出 ConTextING 模型，将文档级上下文嵌入引入到图神经网络中，以提升归纳式文本分类能力。作者针对先前 GNN 方法缺乏上下文感知、难以处理未见词与新文档的问题，设计了机制融合每个文档自己的语言模型编码（例如 BERT）与文档内部的图结构，通过图节点接入文档级嵌入，增强每份文档图的表示能力并支持归纳泛化能力【文本分类（归纳式）】【各文档 BERT 级上下文嵌入 + 文档级图结构】【分类准确率（如 Acc, F1）】【ConTextING（GNN + 文档上下文融合模型）】

315. **ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering** [COLING 2022] [[paper link](https://aclanthology.org/2022.coling-1.148.pdf)]

     本文针对知识库问答（KBQA）中的搜索空间大与模式链接歧义两个挑战，提出 ArcaneQA —— 一种生成式模型框架，集成动态程序归纳（dynamic program induction）以灵活生成逻辑程序查询 与 动态上下文化编码（dynamic contextualized encoding）以改进 schema linking，两者相互促进。实验表明，在多个 KBQA 基准数据集上，ArcaneQA 在效果与效率上均具备高度竞争力。【知识库问答（KBQA）】【多种流行 KBQA 基准数据集】【效果与效率表现优越】【ArcaneQA（生成式模型 + 动态程序归纳 + 动态上下文化编码）】


316. **Cross-lingual Feature Extraction from Monolingual Corpora for Low-resource Unsupervised Bilingual Lexicon Induction** [COLING 2022] [[paper link](https://aclanthology.org/2022.coling-1.469/)]  

     本文针对远距离、资源稀缺语言对中无监督双语词典归纳（UBLI）方法初始化不足的问题，提出从单语语料中学习跨语言的语义特征（cross-lingual features, CFE），用于增强词与其翻译词在嵌入空间中的对齐。具体通过利用词在其上下文中的语境相关性构造语言无关的特征向量，并通过Embedding Combination (ECB) 与 Similarity Combination (SCB) 两种策略将 CFE 与预训练词嵌入融合，用于对齐初始化。实验在 EN-VI、EN-TH、EN-ZH、EN-JA 等低资源语言对上显著优于最先进的无监督方法。消融研究进一步证明 CFE 对提升表示能力与模型鲁棒性具有显著贡献。【无监督双语词典归纳】【单语语料 + CFE 特征】【Precision@k 提升】【CFE + 预训练嵌入融合模型（ECB, SCB）】

317. **Prior Relational Schema Assists Effective Contrastive Learning for Inductive Knowledge Graph Completion** [COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.1139/)]

     本文研究在归纳式知识图谱补全任务中，如何利用关系的先验模式（relational schema）辅助对比学习以提升模型的泛化能力。作者通过引入实体类型构成的关系 schema 作为先验约束，设计了一种基于该先验的对比学习机制，使得模型在无监督或少监督下仍能学得更加区分性的实体关系表达，从而显著提升对新实体/新关系的补全能力。【归纳式知识图谱补全】【实体类型关系 schema 先验】【MRR, Hits@1, Hits@3, Hits@10 等补全指标】【Contrastive Learning + Relational Schema 先验融合模型】

318. **Prompt-fused Framework for Inductive Logical Query Answering** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.1152/)]

     本文提出 Pro-QE，即一种 查询感知的提示融合框架，用于归纳式逻辑查询应答，特别解决知识图谱中“新实体出现”与“整体查询理解”的挑战。模型通过聚合实体的局部邻居与关系域范围信息生成归纳式实体嵌入，再结合由符号查询编码构建的全局 query prompt，实现整体语义驱动的推理；引入两个基准测试评估未见实体场景下性能并通过消融验证聚合器与提示组件的有效性。【归纳式逻辑查询应答】【两个新构建的归纳式逻辑查询基准】【查询应答准确率（如MRR, Hits@k）】【Pro-QE 框架（局部+全局聚合 + query prompt）】

319. **Categorial Grammar Induction with Stochastic Category Selection** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.258.pdf)]

     本文提出一种用于范畴文法（categorial grammar）归纳的新型模型，能够从原始未标注文本学习语法结构，且无需依赖词性标注或偏向性目标函数。其关键在于设计出一种随机过程，用于动态选择可用的句法范畴，从而移除先前模型中对分支结构行为的“捷径”依赖。在针对儿童语言输入语料（child-directed speech）的实验中，该模型实现了 recall-homogeneity 指标为 0.48，相比之前的范畴文法归纳器取得了显著提升。【语法归纳】【English child-directed speech 语料】【recall-homogeneity = 0.48】【带有随机范畴选择机制的范畴文法归纳模型】

320. **Linguistic Rule Induction Improves Adversarial and OOD Robustness in Large Language Models** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.924/)]  

     本文研究“语言学规则归纳”在提升大型语言模型（LLMs）抗击对抗性攻击（adversarial）及分布外（OOD）鲁棒性方面的作用。作者构建了一个“语言规则指令集”，用于 fine-tune LLaMA-13B，使其学习从因果关系等语言学规则出发进行逐步推理（LingR-based chain-of-thoughts）。实验证明，经过这种处理的 LLaMA-13B（LingR-LLaMA）在各种 adversarial 和 OOD 评估任务中，其性能可与 GPT-3.5 和 GPT-4 相媲美，表明规则归纳是提高模型稳健性的关键因素。【鲁棒性提升 / 对抗 & OOD】【语言规则指令集 + LingR-CoT 推理】【对抗 & OOD 鲁棒性 comparable to GPT-3.5 & GPT-4】【LingR-LLaMA-13B (LLaMA-13B + linguistic rule induction)】

321. **Linking Adaptive Structure Induction and Neuron Filtering: A Spectral Perspective for Aspect-based Sentiment Analysis** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.926.pdf)]  

     提出一种面向方面情感分析的自适应图结构归纳方法与频域神经元过滤（NeuLT），先用图结构学习构建邻接关系并结合GNN完成预测，再在词表示的频谱上进行过滤以提升结构质量与分类性能；在三个公开数据集上，方法显著缩短方面与情感的距离（AsD），Accuracy 与 Macro-F1 达到或接近 SOTA，表明频域视角与神经元过滤有助于稳健的结构归纳与性能提升。【文本分类→情感分析→基于方面的情感分析】【三个公共 ABSA 数据集】【Accuracy, Macro-F1, AsD】【GSL 自适应图结构 + GNN + NeuLT（频域神经元过滤）】

323. **Multilingual Substitution-based Word Sense Induction** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.1035.pdf)]

     本文提出一种基于替换法（lexical substitution）的多语言词义归纳（WSI）方法，在支持的 100 种语言之间几乎无需任何特别适配即可应用。作者利用 XLM-R 多语言掩码语言模型，通过设计多种替换生成策略（包括 Concat 和 Word Continuation Masking, WCM）结合目标注入（如 SDP）及 FastText 跨语言重新排序方法构建词义表示；然后对每个实例基于生成替换词构建 TF-IDF 向量并聚类，实现硬聚类的词义归纳。实验表明，该方法在常见的英文 WSI 数据集上表现与最先进的单语言方法相当，且在低资源语言上更具通用性与实用性。【词义归纳】【覆盖 100 种语言的单语语料 + XLM-R, FastText】【WSI 聚类性能（与单语言方法对比）】

324. **NutFrame: Frame-based Conceptual Structure Induction with LLMs** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.1079/)]  

     提出基于 FrameNet 的概念结构归纳基准 NutFrame，包含 Frame Induction、Frame Element Induction 和 Frame Relation Induction 三个子任务；利用提示设计引导大语言模型自动发现框架及其成分和关系，评估其在不同子任务上的表现，展示 LLM 在语义结构学习和框架构建方面的潜力。【概念结构归纳】【NutFrame 基准（三个子任务）】【—】【LLM + prompt 引导】

326. **PRIMO: Progressive Induction for Multi-hop Open Rule Generation** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.1137.pdf)]

     本文提出 PRIMO，一种渐进式多阶段开放规则生成方法，用于推导多跳规则，从而捕捉更复杂的推理链。该方法引入本体（ontology）信息以减少生成模糊性与逻辑不一致性，并设计三个依次协作的模块：生成（generation）、提取（extraction）和排序（ranking），并在每一步将生成的假设原子融入下一轮前提，实现多跳推理。在此基础上，PRIMO 还引入人类反馈的强化学习（RLHF）进一步提升规则的质量和多样性。实验使用构建的多跳开放规则基准（含 495 个前提原子，共 2851 个样本），在 BLEU-1, BLEU-4, ROUGE-L 等指标上显著优于 Prompt、COMET、Orion 等基线模型，并在 Self-BLEU-2 上反映出更高规则多样性，同时明显降低语义重复率 (Self-BLEU-2 更低，即多样性更高)。PRIMO 在参数量远小于大型语言模型（如 Vicuna-13B）的情况下，实现了接近 LLM 的性能。实验结果支持其在多跳开放规则生成任务中的优异与鲁棒性。【开放规则生成（多跳）】【Freebase 构建的多跳开放规则数据集（495 前提原子，2851 样本）】【BLEU-1, BLEU-4, ROUGE-L, Self-BLEU-2 多样性指标】【PRIMO（生成 + 提取 + 排序 多阶段模型）+ 本体约束 + RLHF 强化学习】


327. **When Your Cousin Has the Right Connections: Unsupervised Bilingual Lexicon Induction for Related Data-Imbalanced Languages** [LREC-COLING 2024] [[paper link](https://aclanthology.org/2024.lrec-main.1526/)]

     本文针对资源极端匮乏、与高资源语言（HRL）密切相关但数据严重不平衡的低资源语言（LRLs），提出一种无监督双语词典归纳（BLI）新方法。作者首先证实现有对齐或嵌入方法在这种偏极端的数据不平衡设置下几乎失效，然后设计：仅使用 HRL 的遮蔽语言模型（MLM）对 LRL 句子中的未知词进行预测引导；通过迭代替换已知词使句子更“HRL可理解”，不断扩展词典。实验以 Bhojpuri 和 Magahi（单语语料 < 5M token）对 Hindi 为目标高资源语言展开，结果远超传统方法，并在 Marathi 和 Nepali 中获得对比参考，同时公开发布五种 LR Indic 语言（Bhojpuri, Magahi, Awadhi, Braj, Maithili）—Hindi 的生成词典资源。【双语词典归纳（无监督）】【LRL 单语语料 + HRL MLM（Hindi-BERT）、Indic LRL 句子】【Precision@2（P@2）、非同形预测准确率（NIA）】【迭代遮蔽预测 + HRL MLM 推断 + 正字法重排（Rulebook variant）方法】

328. **TaxoCritic: Exploring Credit Assignment in Taxonomy Induction with Multi-Critic Reinforcement Learning** [DLnLD (LREC-COLING Workshop) 2024] [[paper link](https://aclanthology.org/2024.dlnld-1.2/)]  

     提出 TaxoCritic 方法，用于自动分类体系构建，通过多 Critic 强化学习细粒度评估生成的分类边子操作，改进归因分配并提高边识别的准确性与稳健性；实验证明该方法在正确性和鲁棒性方面优于现有模型。【分类体系归纳】【DLnLD 2024 Workshop 分类语料】【分类边准确率和鲁棒性】【多 Critic 强化学习模型（credit assignment）】

330. **Pre-trained Semantic Interaction based Inductive Graph Neural Networks for Text Classification** [COLING 2025] [[paper link](https://aclanthology.org/2025.coling-main.54.pdf)]

     本文提出 PaSIG，一种结合预训练语义交互与归纳式图神经网络的文本分类框架。构建文本–词语异构图，并通过设计非对称拓扑结构确保信息仅从词节点传递到测试文本节点，实现训练与测试的解耦；使用 fine-tuned 语言模型（如 BERT）为词语与文本节点生成包含分类语义的信息嵌入；引入gated fusion机制，自适应融合中心节点与邻居信息；为提升推理效率，还设计了子图采样与中间状态保留策略。实验覆盖五个标杆文本分类数据集，PaSIG 平均提升准确率约 2.7%，在推断速度及资源消耗方面也优于最先进方法。【文本分类（归纳式）】【MR, Ohsumed, 20NG, R8, R52 五个文本分类数据集】【分类准确率, macro-F1】【PaSIG（预训练语义交互 + 非对称图结构 + gated fusion + 子图采样）】

331. **Commonsense Subgraph for Inductive Relation Reasoning with Meta-learning** [COLING 2025] [[paper link](https://aclanthology.org/2025.coling-main.150/)]

     本文提出 CSML（Commonsense Subgraph Meta-Learning），一种将常识元信息纳入元学习框架的新方法，用于解决归纳式关系推理中的少样本问题。通过提取实体对应的概念构建“常识子图”（commonsense subgraph），CSML 利用这些高层语义作为元信息，帮助模型在极少或零样本关系下迅速适应与泛化。【（少样本/零样本）归纳式关系推理】【概念抽取构建的常识子图 + few-shot 关系推理任务】【MRR, Hits@1/3/10 等指标】【CSML（元学习 + 常识子图辅助模型）】

332. **Inductive Link Prediction in N-ary Knowledge Graphs** [COLING 2025] [[paper link](https://aclanthology.org/2025.coling-main.595/)]  

     本文提出归纳式链接预测的新任务 ILPN（Inductive Link Prediction in N-ary Knowledge Graphs），旨在预测包含未见实体或角色的 n-ary 关系中的缺失元素。为解决传统三元组方法难以处理 n-ary 结构的问题，作者构建 n-ary 语义超图表示，并提出 NS-HART 模型（基于角色感知 Transformer 的子图推理）以捕捉实体无关的多跳语义模式。实验在“带实体特征/无实体特征的迁移推理”以及“对子图成对推理”等多种归纳任务中验证，NS-HART 在 MRR、Hits@10 等指标上显著优于三元组模型与现有超图 GNN 方法，展现了卓越的归纳能力。【知识图谱推理→链接预测→归纳式链接预测（n-ary KGs）】【ILPN 任务 + n-ary 语义超图】【MRR, Hits@10, AUC-PR】【NS-HART（role-aware Transformer 子图聚合网络）】


## Others

1. **Inductive Representation Learning on Large Graphs** [NIPS2017] [[paper link](https://arxiv.org/pdf/1706.02216)]

   GraphSAGE是一个通用的归纳式框架，它利用节点的特征信息，可以高效地为以前未见过的数据生成节点embedding表示。

2. **Inductive Quantum Embedding** [NIPS2020] [[paper link](https://proceedings.neurips.cc/paper_files/paper/2020/file/b87039703fe79778e9f140b78621d7fb-Paper.pdf)]

   原始的 Quantum Embedding (QE)方法只能用于transductive设置，现在在QE损失中引入inductive特征映射项。

3. **Grounding inductive biases in natural images: invariance stems from variations in data** [NIPS2021] [[paper link](https://arxiv.org/pdf/2106.05121)]

   传统观点认为，CNN中的空间平移不变性来自网络结构本身的归纳偏置（例如卷积和池化操作），但这篇论文质疑了这个观点，提出了一个新的假设：不变性并不主要来自模型结构中的归纳偏置，而是来自训练数据中天然存在的变换多样性（variations in natural images）。

4. **The Inductive Bias of Quantum Kernels** [NIPS2021] [[paper link](https://arxiv.org/pdf/2106.03747)]

   揭示quantum kernel中inductive bias与泛化的关系，没有归纳偏置的指数空间”会导致泛化困难，而偏置引入又伴随测量成本的指数级增长。

5. **A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases** [NIPS2022] [[paper link](https://papers.nips.cc/paper_files/paper/2022/file/184c1e18d00d7752805324da48ad25be-Paper-Conference.pdf)]

   借助动力系统理论分析优化器动态，从中提炼出有助于稳定与泛化的归纳偏置，以优化训练。

6. **SHINE: SubHypergraph Inductive Neural nEtwork** [NIPS2022] [[paper link](https://papers.nips.cc/paper_files/paper/2022/file/7721f1fea280e9ffae528dc78c732576-Paper-Conference.pdf)]

   有关超图预测与类比。

7. **Maximum Class Separation as Inductive Bias in One Matrix** [NIPS2022] [[paper link](https://arxiv.org/abs/2206.08704)]

   最大类别间隔（maximum class separation）是传统机器学习（例如 SVM）中的重要归纳偏置。本文试图将将“最大类别间隔”以结构级别的归纳偏置嵌入网络架构中。

8. **The Inductive Bias of Flatness Regularization for Deep Matrix Factorization** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/5927edd18c5dd83aa8936a4610c72029-Paper-Conference.pdf)]

   核心问题：深度神经网络在过参数化（参数远多于样本数）情况下仍能良好泛化，这与传统统计学习理论相矛盾。研究空白：平坦正则化（如L2正则化、SGD隐式偏好）如何影响深度矩阵分解（DMF）模型的优化动态？深度（层数）如何改变模型对低秩解的偏好？

9. **CAT-Walk: Inductive Hypergraph Learning via Set Walks** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/6739d8df16b5bce3587ca5f18662a6aa-Paper-Conference.pdf)]

   同样与超图有关。

10. **PAC-Bayes Generalization Certificates for Learned Inductive Conformal Prediction** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/9235c376df778f1aaf486a882afb7471-Paper-Conference.pdf)]

    在Inductive Conformal Prediction (ICP)：一种后验式、不要求数据交换性假设的保序预测方法，属于归纳学习范式中给出泛化界的inductive bias。

11. **PID‑Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks** [NIPS2023] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/ba1c5356d9164bb64c446a4b690226b0-Paper-Conference.pdf)]

    以PID控制为启发，设计一种既能有效编码历史信息又具备强鲁棒性的inductive bias。

12. **MOTIVE: A Drug-Target Interaction Graph For Inductive Link Prediction** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2024/file/fdb3fa770c2e0ecbb4b7dc7083ef5be9-Paper-Datasets_and_Benchmarks_Track.pdf)]

    靶向药。

13. **Theoretical Analysis of the Inductive Biases in Deep Convolutional Networks** [NIPS2024] [[paper link](https://papers.nips.cc/paper_files/paper/2023/file/eb1bad7a84ef68a64f1afd6577725d45-Paper-Conference.pdf)]

    系统性地解析这些结构形成的归纳偏置，以在理论上解释CNN的高效学习能力。

14. **Inductive-bias-driven Reinforcement Learning for Efficient Schedules in Heterogeneous Clusters** [ICML2020] [[paper link](https://proceedings.mlr.press/v119/banerjee20a/banerjee20a.pdf)]

    通过系统拓扑与性能计数器定义的架构依赖，提供inductive bias，引导RL agent（异构集群调度）学习有效状态表示。

15. **Provable Guarantees for Decision Tree Induction: The Agnostic Setting** [ICML2020] [[paper link](https://proceedings.mlr.press/v119/blanc20a/blanc20a.pdf)]

    与决策树。

16. **ModLaNets: Learning Generalisable Dynamics via Modularity and Physical Inductive Bias** [ICML2022] [[paper link](https://proceedings.mlr.press/v162/lu22c/lu22c.pdf)]

    与动力单元。

17. **Inductive Gradient Adjustment for Spectral Bias in Implicit Neural Representations** [ICML2025] [[paper link](https://openreview.net/pdf?id=pYMZQtkp3F)]

    从训练动力学视角出发，设计不依赖模型架构变化的 梯度调整策略，使MLP能有效学习高频分量。

18. **Map Induction: Compositional spatial submap learning for efficient exploration in novel environments** [ICLR2022] [[paper link](https://arxiv.org/pdf/2110.12301)]

    论文假设人类在探索未知环境时，并非一片空白地随机探索，而是会根据此前看到过的相似结构来归纳新环境的未观测区域，从而更加高效地规划路线和搜寻目标。这一认知机制可以用分层贝叶斯与程序归纳模型来模拟。

19. **Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems** [ICLR2023] [[paper link](https://arxiv.org/pdf/2209.10740)]

    引入 Graph Neural ODE（GNODE）：对粒子系统建模时，利用 GNN 构造连续时间动力学方程，作为 NODE 执行积分预测。相比传统输入位置 + 速度，GNODE 仅将 位置作为输入 且对加速度进行预测，为 second-order bias 做准备。inductive bias 通过将物理约束与系统特性（energy conservation、Newton’s law）编码为模型架构的一部分，被强制注入 GNODE。

20. **SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases** [ICLR2024] [[paper link](https://arxiv.org/pdf/2308.13212)]

    虽然等变图神经网络（Equivariant GNNs）在模拟多对象物理系统中广泛使用，但其泛化能力仍受限，因为未充分融入关键的物理归纳偏好。

21. **Interpretable Vision‑Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology** [ICLR2025] [[paper link](https://arxiv.org/pdf/2409.09369)]

    在计算病理学中，对全切片影像（WSI）进行预后生存分析（Survival Analysis，SA）通常面临两大挑战：病患数据稀缺（通常少于千人）和仅有粗粒度（病人级别）的监督标签。多实例学习（MIL）框架下，模型难以从海量切片图像中学习有效的预后表示。作者提出利用 Vision‑Language Foundation Models（如病理 VL 模型） 引入语言中的预后先验知识，以补强弱监督信号，提高数据效率与可解释性。
